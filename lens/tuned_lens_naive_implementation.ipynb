{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (2.7.0)\n",
      "Collecting transformer-lens\n",
      "  Using cached transformer_lens-2.16.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting datasets\n",
      "  Using cached datasets-4.0.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: filelock in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from torch) (4.13.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from torch) (2025.3.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from torch) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from torch) (9.5.1.17)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from torch) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from torch) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from torch) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from torch) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from torch) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from torch) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from torch) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from torch) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from torch) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.3.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from torch) (3.3.0)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from triton==3.3.0->torch) (75.8.0)\n",
      "Collecting accelerate>=0.23.0 (from transformer-lens)\n",
      "  Using cached accelerate-1.9.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting beartype<0.15.0,>=0.14.1 (from transformer-lens)\n",
      "  Using cached beartype-0.14.1-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting better-abc<0.0.4,>=0.0.3 (from transformer-lens)\n",
      "  Using cached better_abc-0.0.3-py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting einops>=0.6.0 (from transformer-lens)\n",
      "  Using cached einops-0.8.1-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting fancy-einsum>=0.0.3 (from transformer-lens)\n",
      "  Using cached fancy_einsum-0.0.3-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting jaxtyping>=0.2.11 (from transformer-lens)\n",
      "  Using cached jaxtyping-0.3.2-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting numpy<2,>=1.24 (from transformer-lens)\n",
      "  Using cached numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "Requirement already satisfied: pandas>=1.1.5 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from transformer-lens) (2.2.3)\n",
      "Requirement already satisfied: rich>=12.6.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from transformer-lens) (14.0.0)\n",
      "Collecting sentencepiece (from transformer-lens)\n",
      "  Using cached sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: tqdm>=4.64.1 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from transformer-lens) (4.67.1)\n",
      "Requirement already satisfied: transformers>=4.51 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from transformer-lens) (4.51.3)\n",
      "Collecting transformers-stream-generator<0.0.6,>=0.0.5 (from transformer-lens)\n",
      "  Using cached transformers_stream_generator-0.0.5-py3-none-any.whl\n",
      "Collecting typeguard<5.0,>=4.2 (from transformer-lens)\n",
      "  Using cached typeguard-4.4.4-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting wandb>=0.13.5 (from transformer-lens)\n",
      "  Using cached wandb-0.21.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Collecting typing-extensions>=4.10.0 (from torch)\n",
      "  Using cached typing_extensions-4.14.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting pyarrow>=15.0.0 (from datasets)\n",
      "  Using cached pyarrow-21.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Using cached dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: requests>=2.32.2 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from datasets) (2.32.3)\n",
      "Collecting xxhash (from datasets)\n",
      "  Using cached xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Using cached multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec (from torch)\n",
      "  Using cached fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from datasets) (0.31.2)\n",
      "Requirement already satisfied: packaging in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from datasets) (6.0.2)\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Using cached aiohttp-3.12.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: psutil in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from accelerate>=0.23.0->transformer-lens) (7.0.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from accelerate>=0.23.0->transformer-lens) (0.5.3)\n",
      "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.4.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Using cached aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting async-timeout<6.0,>=4.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Using cached async_timeout-5.0.1-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Using cached frozenlist-1.7.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Using cached multidict-6.6.3-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (5.3 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Using cached propcache-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Using cached yarl-1.20.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (73 kB)\n",
      "Requirement already satisfied: idna>=2.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.10)\n",
      "Collecting wadler-lindig>=0.1.3 (from jaxtyping>=0.2.11->transformer-lens)\n",
      "  Using cached wadler_lindig-0.1.7-py3-none-any.whl.metadata (17 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from pandas>=1.1.5->transformer-lens) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from pandas>=1.1.5->transformer-lens) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from pandas>=1.1.5->transformer-lens) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas>=1.1.5->transformer-lens) (1.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from rich>=12.6.0->transformer-lens) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from rich>=12.6.0->transformer-lens) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=12.6.0->transformer-lens) (0.1.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from transformers>=4.51->transformer-lens) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from transformers>=4.51->transformer-lens) (0.21.1)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from wandb>=0.13.5->transformer-lens) (8.1.8)\n",
      "Collecting gitpython!=3.1.29,>=1.0.0 (from wandb>=0.13.5->transformer-lens)\n",
      "  Using cached gitpython-3.1.45-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: platformdirs in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from wandb>=0.13.5->transformer-lens) (4.3.7)\n",
      "Collecting protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 (from wandb>=0.13.5->transformer-lens)\n",
      "  Using cached protobuf-6.31.1-cp39-abi3-manylinux2014_x86_64.whl.metadata (593 bytes)\n",
      "Requirement already satisfied: pydantic<3 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from wandb>=0.13.5->transformer-lens) (2.11.4)\n",
      "Collecting sentry-sdk>=2.0.0 (from wandb>=0.13.5->transformer-lens)\n",
      "  Using cached sentry_sdk-2.34.1-py2.py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from pydantic<3->wandb>=0.13.5->transformer-lens) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from pydantic<3->wandb>=0.13.5->transformer-lens) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from pydantic<3->wandb>=0.13.5->transformer-lens) (0.4.0)\n",
      "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.29,>=1.0.0->wandb>=0.13.5->transformer-lens)\n",
      "  Using cached gitdb-4.0.12-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb>=0.13.5->transformer-lens)\n",
      "  Using cached smmap-5.0.2-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from jinja2->torch) (3.0.2)\n",
      "Using cached transformer_lens-2.16.1-py3-none-any.whl (192 kB)\n",
      "Using cached beartype-0.14.1-py3-none-any.whl (739 kB)\n",
      "Using cached better_abc-0.0.3-py3-none-any.whl (3.5 kB)\n",
      "Using cached numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
      "Using cached typeguard-4.4.4-py3-none-any.whl (34 kB)\n",
      "Using cached datasets-4.0.0-py3-none-any.whl (494 kB)\n",
      "Using cached dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Using cached fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
      "Using cached multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "Using cached accelerate-1.9.0-py3-none-any.whl (367 kB)\n",
      "Using cached aiohttp-3.12.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
      "Using cached async_timeout-5.0.1-py3-none-any.whl (6.2 kB)\n",
      "Using cached multidict-6.6.3-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (241 kB)\n",
      "Using cached yarl-1.20.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (326 kB)\n",
      "Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Using cached aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
      "Using cached einops-0.8.1-py3-none-any.whl (64 kB)\n",
      "Using cached fancy_einsum-0.0.3-py3-none-any.whl (6.2 kB)\n",
      "Using cached frozenlist-1.7.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (222 kB)\n",
      "Using cached jaxtyping-0.3.2-py3-none-any.whl (55 kB)\n",
      "Using cached propcache-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (198 kB)\n",
      "Using cached pyarrow-21.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (42.7 MB)\n",
      "Using cached typing_extensions-4.14.1-py3-none-any.whl (43 kB)\n",
      "Using cached wadler_lindig-0.1.7-py3-none-any.whl (20 kB)\n",
      "Using cached wandb-0.21.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (22.2 MB)\n",
      "Using cached protobuf-6.31.1-cp39-abi3-manylinux2014_x86_64.whl (321 kB)\n",
      "Using cached gitpython-3.1.45-py3-none-any.whl (208 kB)\n",
      "Using cached gitdb-4.0.12-py3-none-any.whl (62 kB)\n",
      "Using cached smmap-5.0.2-py3-none-any.whl (24 kB)\n",
      "Using cached sentry_sdk-2.34.1-py2.py3-none-any.whl (357 kB)\n",
      "Using cached sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "Using cached xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "Installing collected packages: sentencepiece, better-abc, xxhash, wadler-lindig, typing-extensions, smmap, sentry-sdk, pyarrow, protobuf, propcache, numpy, fsspec, frozenlist, fancy-einsum, einops, dill, beartype, async-timeout, aiohappyeyeballs, typeguard, multiprocess, multidict, jaxtyping, gitdb, aiosignal, yarl, gitpython, wandb, aiohttp, accelerate, transformers-stream-generator, datasets, transformer-lens\n",
      "\u001b[2K  Attempting uninstall: typing-extensions\n",
      "\u001b[2K    Found existing installation: typing_extensions 4.13.2\n",
      "\u001b[2K    Uninstalling typing_extensions-4.13.2:\n",
      "\u001b[2K      Successfully uninstalled typing_extensions-4.13.2━━━━━━━━━━━\u001b[0m \u001b[32m 4/33\u001b[0m [typing-extensions]\n",
      "\u001b[2K  Attempting uninstall: numpy[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 9/33\u001b[0m [propcache]]ns]\n",
      "\u001b[2K    Found existing installation: numpy 2.2.5━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 9/33\u001b[0m [propcache]\n",
      "\u001b[2K    Uninstalling numpy-2.2.5:[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 9/33\u001b[0m [propcache]\n",
      "\u001b[2K      Successfully uninstalled numpy-2.2.5━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10/33\u001b[0m [numpy]\n",
      "\u001b[2K  Attempting uninstall: fsspec\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10/33\u001b[0m [numpy]\n",
      "\u001b[2K    Found existing installation: fsspec 2025.3.2━━━━━━━━━━━━━━\u001b[0m \u001b[32m10/33\u001b[0m [numpy]\n",
      "\u001b[2K    Uninstalling fsspec-2025.3.2:0m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10/33\u001b[0m [numpy]\n",
      "\u001b[2K      Successfully uninstalled fsspec-2025.3.2━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10/33\u001b[0m [numpy]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33/33\u001b[0m [transformer-lens][transformer-lens]\n",
      "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed accelerate-1.9.0 aiohappyeyeballs-2.6.1 aiohttp-3.12.15 aiosignal-1.4.0 async-timeout-5.0.1 beartype-0.14.1 better-abc-0.0.3 datasets-4.0.0 dill-0.3.8 einops-0.8.1 fancy-einsum-0.0.3 frozenlist-1.7.0 fsspec-2025.3.0 gitdb-4.0.12 gitpython-3.1.45 jaxtyping-0.3.2 multidict-6.6.3 multiprocess-0.70.16 numpy-1.26.4 propcache-0.3.2 protobuf-6.31.1 pyarrow-21.0.0 sentencepiece-0.2.0 sentry-sdk-2.34.1 smmap-5.0.2 transformer-lens-2.16.1 transformers-stream-generator-0.0.5 typeguard-4.4.4 typing-extensions-4.14.1 wadler-lindig-0.1.7 wandb-0.21.0 xxhash-3.5.0 yarl-1.20.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install torch transformer-lens datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import json\n",
    "\n",
    "import os\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "from transformer_lens import HookedTransformer\n",
    "from huggingface_hub import login\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_token = \"...\"\n",
    "login(token=hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bc44002d4264a79bc2f34e9a9424845",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:With reduced precision, it is advised to use `from_pretrained_no_processing` instead of `from_pretrained`.\n",
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model google/gemma-2-2b-it into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"google/gemma-2-2b-it\"  # Or any supported model; replace with your desired checkpoint\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "NUM_EPOCHS = 2\n",
    "CHECKPOINT_EVERY = 1  # Save checkpoint every N epochs\n",
    "\n",
    "# Load model and tokenizer\n",
    "model = HookedTransformer.from_pretrained(MODEL_NAME, device=DEVICE, torch_dtype=torch.float16, center_unembed=False)\n",
    "\n",
    "# Freeze model parameters & set to eval mode\n",
    "model.eval()\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "tokenizer = model.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_layers = model.cfg.n_layers\n",
    "d_model = model.cfg.d_model\n",
    "d_vocab = model.cfg.d_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa219b0059ba4267ac4c25b65108dd30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/1024 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "629b7cef786b4366822c5c4bd39536dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/1024 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 39470 clean text samples\n",
      "Token sequence length: 18330996 tokens\n"
     ]
    }
   ],
   "source": [
    "NUM_EXAMPLES = 39470\n",
    "dataset = load_dataset(\"allenai/c4\", name=\"en\", split=\"train\", streaming=True)\n",
    "text_samples = []\n",
    "for example in dataset:\n",
    "    text = example.get(\"text\", \"\").strip()\n",
    "    if text:\n",
    "        text_samples.append(text)\n",
    "    if len(text_samples) >= NUM_EXAMPLES:\n",
    "        break\n",
    "print(f\"Loaded {len(text_samples)} clean text samples\")\n",
    "\n",
    "# Join all text into one long sequence of tokens\n",
    "joined_text = \"\\n\\n\".join(text_samples) \n",
    "all_token_ids = tokenizer.encode(joined_text, return_tensors='pt')[0]  # shape: (total_tokens,)\n",
    "print(f\"Token sequence length: {len(all_token_ids)} tokens\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prefix tokens count: 3\n",
      "Suffix tokens count: 1\n"
     ]
    }
   ],
   "source": [
    "# Let's define your special strings exactly:\n",
    "prefix_text = \"<start_of_turn>user\\n\"\n",
    "suffix_text = \"<end_of_turn>\"\n",
    "\n",
    "# Encode prefix and suffix tokens with the tokenizer:\n",
    "prefix_tokens = tokenizer.encode(prefix_text, add_special_tokens=False)  # e.g. [....]\n",
    "suffix_tokens = tokenizer.encode(suffix_text, add_special_tokens=False)  # e.g. [....]\n",
    "\n",
    "print(f\"Prefix tokens count: {len(prefix_tokens)}\")\n",
    "print(f\"Suffix tokens count: {len(suffix_tokens)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "SEQ_LEN = 128\n",
    "SUBSET_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset_from_tokens(token_ids, seq_len):\n",
    "    examples = []\n",
    "    total_tokens = len(token_ids)\n",
    "    seq_len=seq_len-4\n",
    "    for i in range(0, total_tokens - seq_len, seq_len):\n",
    "        input_ids = token_ids[i : i + seq_len]\n",
    "        wrapped_seq=prefix_tokens+input_ids.tolist()+suffix_tokens\n",
    "        wrapped_tensor=torch.tensor(wrapped_seq, dtype=torch.long)\n",
    "        examples.append(wrapped_tensor)\n",
    "    input_ids_tensor = torch.stack(examples)\n",
    "    return input_ids_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sequences: 130351\n",
      "1018\n"
     ]
    }
   ],
   "source": [
    "TARGET_NUM_SEQUENCES = 130351\n",
    "input_ids = create_dataset_from_tokens(all_token_ids, seq_len=SEQ_LEN)\n",
    "input_ids = input_ids[:TARGET_NUM_SEQUENCES]\n",
    "print(f\"Total sequences: {len(input_ids)}\")\n",
    "N = len(input_ids)\n",
    "num_subsets = N // SUBSET_SIZE\n",
    "print(num_subsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_subsets=1018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stable_kl_loss(probe_logits, final_logits, top_k=100, eps=1e-8):\n",
    "    B, S, V = final_logits.size()\n",
    "\n",
    "    _, topk_final = torch.topk(final_logits, top_k, dim=-1)\n",
    "    _, topk_probe = torch.topk(probe_logits, top_k, dim=-1)\n",
    "\n",
    "    combined = torch.cat([topk_final, topk_probe], dim=-1).view(-1, 2 * top_k)  # shape: (B*S, 2*k)\n",
    "\n",
    "    max_len = combined.size(-1)\n",
    "    unique_list = []\n",
    "    mask_list = []\n",
    "    for idxs in combined:\n",
    "        unique = torch.unique(idxs, sorted=True)\n",
    "        # Pad with the last valid index if shorter than max_len\n",
    "        if unique.size(0) < max_len:\n",
    "            padded = torch.cat([unique, unique[-1].repeat(max_len - unique.size(0))])\n",
    "        else:\n",
    "            padded = unique[:max_len]\n",
    "        unique_list.append(padded)\n",
    "        # Create mask to mark valid indices\n",
    "        mask_list.append(torch.arange(max_len, device=unique.device) < unique.size(0))\n",
    "\n",
    "    union_indices = torch.stack(unique_list).view(B, S, max_len)\n",
    "    valid_mask = torch.stack(mask_list).view(B, S, max_len)  \n",
    "\n",
    "\n",
    "    probe_subset_logits = torch.gather(probe_logits, 2, union_indices).float()\n",
    "    final_subset_logits = torch.gather(final_logits, 2, union_indices).float()\n",
    "\n",
    "    target_probs = F.softmax(final_subset_logits, dim=-1).clamp(min=eps)\n",
    "    target_log_probs = torch.log(target_probs)\n",
    "    pred_log_probs = F.log_softmax(probe_subset_logits, dim=-1)\n",
    "\n",
    "    # Compute KL divergence elements and mask out padded positions\n",
    "    kl_elements = target_probs * (target_log_probs - pred_log_probs) * valid_mask.float()\n",
    "\n",
    "    # Sum over vocabulary subset dimension\n",
    "    kl_per_token = kl_elements.sum(dim=-1)\n",
    "\n",
    "    # Compute mean only over valid tokens (positions where any valid vocab exists)\n",
    "    # This prevents division by zero and excludes padded positions properly\n",
    "    token_mask = (valid_mask.any(dim=-1)).float()  # shape (B, S)\n",
    "    loss = kl_per_token.sum() / (token_mask.sum() + eps)\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activations(batch_input):\n",
    "    with torch.no_grad():\n",
    "        hooks_to_cache = [f'blocks.{i}.hook_resid_post' for i in range(n_layers)] \n",
    "\n",
    "        logits, cache = model.run_with_cache(batch_input, names_filter=hooks_to_cache)\n",
    "        #print(cache[f'blocks.0.hook_resid_post'].dtype)\n",
    "        resid_post_outs = [cache[f'blocks.{i}.hook_resid_post'] for i in range(n_layers)]\n",
    "\n",
    "    return resid_post_outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_logits_from_resid(resid, soft_cap=30.0):\n",
    "\n",
    "    normed = ln_final(resid)\n",
    "\n",
    "    W_U_casted = W_U.to(normed.device).type_as(normed)\n",
    "    b_U_casted = b_U.to(normed.device).type_as(normed)\n",
    "    logits = torch.einsum('bsd,dk->bsk', normed, W_U_casted) + b_U_casted\n",
    "\n",
    "    logits = soft_cap * torch.tanh(logits / soft_cap)#done in gemma 2\n",
    "    return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Probe(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(d_model, d_model, bias=True)\n",
    "        torch.nn.init.xavier_normal_(self.linear.weight)\n",
    "        torch.nn.init.zeros_(self.linear.bias)            \n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[GPU USAGE]\n",
      "Thu Jul 31 15:53:17 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 570.124.06             Driver Version: 570.124.06     CUDA Version: 12.8     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA RTX A6000               On  |   00000000:81:00.0 Off |                  Off |\n",
      "| 30%   33C    P8             16W /  300W |    8271MiB /  49140MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A             361      C   ...conda3/envs/py3.10/bin/python       8262MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "\n",
      "\n",
      "[GPU USAGE]\n",
      "Thu Jul 31 15:53:33 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 570.124.06             Driver Version: 570.124.06     CUDA Version: 12.8     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA RTX A6000               On  |   00000000:81:00.0 Off |                  Off |\n",
      "| 30%   32C    P8             16W /  300W |    8271MiB /  49140MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A             361      C   ...conda3/envs/py3.10/bin/python       8262MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "\n",
      "\n",
      "[GPU USAGE]\n",
      "Thu Jul 31 15:53:48 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 570.124.06             Driver Version: 570.124.06     CUDA Version: 12.8     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA RTX A6000               On  |   00000000:81:00.0 Off |                  Off |\n",
      "| 30%   36C    P2             67W /  300W |    1413MiB /  49140MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A             361      C   ...conda3/envs/py3.10/bin/python       1404MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Start GPU monitoring in background\n",
    "import subprocess\n",
    "import threading\n",
    "import time\n",
    "\n",
    "def monitor_gpu(interval=10):\n",
    "    while True:\n",
    "        try:\n",
    "            output = subprocess.check_output(['nvidia-smi'], encoding='utf-8')\n",
    "            print(\"\\n[GPU USAGE]\")\n",
    "            print(output)\n",
    "        except Exception as e:\n",
    "            print(\"[nvidia-smi error]:\", e)\n",
    "        time.sleep(interval)\n",
    "\n",
    "monitor_thread = threading.Thread(target=monitor_gpu, args=(15,), daemon=True)\n",
    "monitor_thread.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ignore this, just to clear memory if i want to run loop again\n",
    "for var_name in ['post_outs', 'final_logits', 'post_logits', 'post_probes','new_probes', 'resid_post_outs', 'subset_input', 'optimizers']:\n",
    "    if var_name in globals():\n",
    "        del globals()[var_name]\n",
    "    elif var_name in locals():\n",
    "        del locals()[var_name]\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "soft_cap_value=30.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Processing subset 1 / 1018 -- Sequences 0 to 127\n",
      "\n",
      "--- Processing subset 2 / 1018 -- Sequences 128 to 255\n",
      "\n",
      "--- Processing subset 3 / 1018 -- Sequences 256 to 383\n",
      "\n",
      "--- Processing subset 4 / 1018 -- Sequences 384 to 511\n"
     ]
    }
   ],
   "source": [
    "for subset_idx in range(num_subsets):    \n",
    "    start = subset_idx * SUBSET_SIZE\n",
    "    end = start + SUBSET_SIZE\n",
    "    print(f\"\\n--- Processing subset {subset_idx+1} / {num_subsets} -- Sequences {start} to {end-1}\")\n",
    "\n",
    "    subset_input = input_ids[start:end].to(DEVICE)\n",
    "    # Extract activations for this subset\n",
    "    resid_post_outs = get_activations(subset_input)\n",
    "    # Move activations to CPU to save GPU memory (keep as FP16 for storage efficiency)\n",
    "    torch.save(resid_post_outs, f'activations/activations_{subset_idx + 1}.pth')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_U = model.state_dict()['unembed.W_U'].to(DEVICE).clone()\n",
    "b_U = model.state_dict()['unembed.b_U'].to(DEVICE).clone()\n",
    "\n",
    "import copy\n",
    "ln_final = copy.deepcopy(model.ln_final).to(DEVICE)\n",
    "\n",
    "del model\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_MULTIPLIER = 2 # Set to 2 for 2 subsets at once, or 3 for 3, etc.\n",
    "num_combined_batches = (num_subsets + BATCH_MULTIPLIER - 1) // BATCH_MULTIPLIER\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/2\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 7.81 GiB. GPU 0 has a total capacity of 47.41 GiB of which 4.80 GiB is free. Including non-PyTorch memory, this process has 42.60 GiB memory in use. Of the allocated memory 42.28 GiB is allocated by PyTorch, and 16.11 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 46\u001b[0m\n\u001b[1;32m     43\u001b[0m loss \u001b[38;5;241m=\u001b[39m stable_kl_loss(probe_logits, final_logits_batch)\n\u001b[1;32m     45\u001b[0m opt\u001b[38;5;241m.\u001b[39mzero_grad(set_to_none\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 46\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(probe\u001b[38;5;241m.\u001b[39mparameters(), max_norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m)\n\u001b[1;32m     48\u001b[0m opt\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/_tensor.py:648\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    638\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    639\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    640\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    641\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    646\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    647\u001b[0m     )\n\u001b[0;32m--> 648\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    650\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/autograd/__init__.py:353\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    348\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    350\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 353\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/autograd/graph.py:824\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    822\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    823\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 824\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    825\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    826\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    827\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    828\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 7.81 GiB. GPU 0 has a total capacity of 47.41 GiB of which 4.80 GiB is free. Including non-PyTorch memory, this process has 42.60 GiB memory in use. Of the allocated memory 42.28 GiB is allocated by PyTorch, and 16.11 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "# Initialize one probe per layer (FP32 on DEVICE)\n",
    "probes = [Probe(d_model).to(DEVICE).to(torch.float32) for _ in range(n_layers)]\n",
    "optimizers = [torch.optim.AdamW(probe.parameters(), lr=1e-5) for probe in probes]\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f\"\\nEpoch {epoch+1}/{NUM_EPOCHS}\")\n",
    "    epoch_loss_per_layer = [0.0 for _ in range(n_layers)]\n",
    "\n",
    "    for layer in range(n_layers):\n",
    "        probe = probes[layer]\n",
    "        opt = optimizers[layer]\n",
    "\n",
    "        for combined_idx in range(num_combined_batches):\n",
    "            start_subset = combined_idx * BATCH_MULTIPLIER + 1\n",
    "            end_subset = min(start_subset + BATCH_MULTIPLIER - 1, num_subsets)\n",
    "\n",
    "            # Gather and concatenate only THIS layer's activations for all needed subsets\n",
    "            activ_list = []\n",
    "            final_layer_activ_list = []\n",
    "            for subset_idx in range(start_subset, end_subset + 1):\n",
    "                acts = torch.load(f'activations/activations_{subset_idx}.pth', map_location='cpu')\n",
    "                activ_list.append(acts[layer])\n",
    "                # Use the last layer’s activations as the reference for logits\n",
    "                final_layer_activ_list.append(acts[-1])\n",
    "\n",
    "            activ_batch = torch.cat(activ_list, dim=0).to(DEVICE).to(torch.float32)\n",
    "            final_layer_activ_batch = torch.cat(final_layer_activ_list, dim=0).to(DEVICE).to(torch.float32)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                final_logits_batch = compute_logits_from_resid(final_layer_activ_batch, soft_cap=30.0)\n",
    "\n",
    "            out = probe(activ_batch)\n",
    "            normed_out = ln_final(out)\n",
    "            B, S, d_model_ = normed_out.shape\n",
    "            normed_out_flat = normed_out.view(-1, d_model_)\n",
    "\n",
    "            W_U_casted = W_U.to(normed_out_flat.dtype)\n",
    "            b_U_casted = b_U.to(normed_out_flat.dtype)\n",
    "            logits_flat = torch.nn.functional.linear(normed_out_flat, W_U_casted.t().contiguous(), b_U_casted)\n",
    "            probe_logits = logits_flat.view(B, S, -1)\n",
    "            probe_logits = soft_cap_value * torch.tanh(probe_logits / soft_cap_value)\n",
    "\n",
    "            loss = stable_kl_loss(probe_logits, final_logits_batch)\n",
    "\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(probe.parameters(), max_norm=1.0)\n",
    "            opt.step()\n",
    "\n",
    "            epoch_loss_per_layer[layer] += loss.item()\n",
    "\n",
    "            # Cleanup\n",
    "            del activ_batch, final_layer_activ_batch, out, normed_out, normed_out_flat\n",
    "            del logits_flat, probe_logits, loss, final_logits_batch\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    print(\"Average loss per layer:\")\n",
    "    for layer in range(n_layers):\n",
    "        avg_loss = epoch_loss_per_layer[layer] / num_batches\n",
    "        print(f\"  Layer {layer}: {avg_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/2\n",
      "Shapes of all tensors in last layer before concatenation:\n",
      "  Tensor 0: torch.Size([128, 128, 2304])\n",
      "torch.Size([128, 128, 2304])\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 7.81 GiB. GPU 0 has a total capacity of 47.41 GiB of which 4.80 GiB is free. Including non-PyTorch memory, this process has 42.60 GiB memory in use. Of the allocated memory 42.28 GiB is allocated by PyTorch, and 16.11 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 58\u001b[0m\n\u001b[1;32m     55\u001b[0m loss \u001b[38;5;241m=\u001b[39m stable_kl_loss(probe_logits, final_logits_batch)\n\u001b[1;32m     57\u001b[0m opt\u001b[38;5;241m.\u001b[39mzero_grad(set_to_none\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 58\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(probe\u001b[38;5;241m.\u001b[39mparameters(), max_norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m)\n\u001b[1;32m     60\u001b[0m opt\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/_tensor.py:648\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    638\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    639\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    640\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    641\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    646\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    647\u001b[0m     )\n\u001b[0;32m--> 648\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    650\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/autograd/__init__.py:353\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    348\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    350\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 353\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/autograd/graph.py:824\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    822\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    823\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 824\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    825\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    826\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    827\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    828\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 7.81 GiB. GPU 0 has a total capacity of 47.41 GiB of which 4.80 GiB is free. Including non-PyTorch memory, this process has 42.60 GiB memory in use. Of the allocated memory 42.28 GiB is allocated by PyTorch, and 16.11 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "# Initialize one probe per layer (FP32 on DEVICE)\n",
    "probes = [Probe(d_model).to(DEVICE).to(torch.float32) for _ in range(n_layers)]\n",
    "optimizers = [torch.optim.AdamW(probe.parameters(), lr=1e-5) for probe in probes]\n",
    "\n",
    "num_batches = num_subsets\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f\"\\nEpoch {epoch+1}/{NUM_EPOCHS}\")\n",
    "    epoch_loss_per_layer = [0.0 for _ in range(n_layers)]\n",
    "    \n",
    "    for combined_idx in range(num_combined_batches):\n",
    "        start_subset = combined_idx * BATCH_MULTIPLIER + 1\n",
    "        end_subset = min(start_subset + BATCH_MULTIPLIER - 1, num_subsets)\n",
    "    \n",
    "        # For each layer (and final layer), build list of tensors to concatenate\n",
    "        batch_resid_post_outs = [[] for _ in range(n_layers)]\n",
    "\n",
    "        # Load each subset activation and append per-layer activations\n",
    "        for subset_idx in range(start_subset, end_subset + 1):\n",
    "            acts = torch.load(f'activations/activations_{subset_idx}.pth', map_location='cpu')\n",
    "            for layer_idx in range(n_layers):\n",
    "                batch_resid_post_outs[layer_idx].append(acts[layer_idx])\n",
    "                \n",
    "        print(\"Shapes of all tensors in last layer before concatenation:\")\n",
    "        for i, t in enumerate(batch_resid_post_outs[-1]):\n",
    "            print(f\"  Tensor {i}: {t.shape}\")\n",
    "\n",
    "        batch_resid_post_outs = [\n",
    "            torch.cat(layer_acts_list, dim=0) for layer_acts_list in batch_resid_post_outs\n",
    "        ]\n",
    "\n",
    "        for layer in range(n_layers):\n",
    "            probe = probes[layer]\n",
    "            opt = optimizers[layer]\n",
    "\n",
    "            activ_batch = batch_resid_post_outs[layer].to(DEVICE).to(torch.float32)     # activations for this layer\n",
    "            final_layer_activ_batch = batch_resid_post_outs[-1].to(DEVICE).to(torch.float32)  # final layer activations\n",
    "\n",
    "            with torch.no_grad():\n",
    "                print(final_layer_activ_batch.shape)\n",
    "                final_logits_batch = compute_logits_from_resid(final_layer_activ_batch, soft_cap=30.0)\n",
    "\n",
    "            out = probe(activ_batch)\n",
    "            normed_out = ln_final(out)#using the stored ln_final\n",
    "            B, S, d_model_ = normed_out.shape\n",
    "            normed_out_flat = normed_out.view(-1, d_model_)\n",
    "    \n",
    "            # Before using W_U and b_U in the loop:\n",
    "            W_U_casted = W_U.to(normed_out_flat.dtype)\n",
    "            b_U_casted = b_U.to(normed_out_flat.dtype)\n",
    "            logits_flat = torch.nn.functional.linear(normed_out_flat, W_U_casted.t().contiguous(), b_U_casted)\n",
    "            probe_logits = logits_flat.view(B, S, -1)\n",
    "            probe_logits = soft_cap_value * torch.tanh(probe_logits / soft_cap_value)\n",
    "        \n",
    "            loss = stable_kl_loss(probe_logits, final_logits_batch)\n",
    "    \n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(probe.parameters(), max_norm=1.0)\n",
    "            opt.step()\n",
    "    \n",
    "            epoch_loss_per_layer[layer] += loss.item()\n",
    "    \n",
    "                # Cleanup\n",
    "            del activ_batch, final_layer_activ_batch, out, normed_out, normed_out_flat\n",
    "            del logits_flat, probe_logits, loss, final_logits_batch\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    print(\"Average loss per layer:\")\n",
    "    for layer in range(n_layers):\n",
    "        avg_loss = epoch_loss_per_layer[layer] / num_batches\n",
    "        print(f\"  Layer {layer}: {avg_loss:.4f}\")\n",
    "\n",
    "# Save final trained probes (one per layer) after all epochs\n",
    "print(\"\\nSaving final trained probes per layer...\")\n",
    "for layer in range(n_layers):\n",
    "    save_dir='probes'\n",
    "    save_path = f'{save_dir}/probe_{layer}.pt'\n",
    "    torch.save(probes[layer].state_dict(), save_path)\n",
    "    print(f\"Saved layer {layer} final probe to {save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
