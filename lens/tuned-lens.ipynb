{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install torch transformer-lens datasets accelerate wandb","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true,"execution":{"iopub.status.busy":"2025-08-07T08:07:05.734413Z","iopub.execute_input":"2025-08-07T08:07:05.734675Z","iopub.status.idle":"2025-08-07T08:08:33.469571Z","shell.execute_reply.started":"2025-08-07T08:07:05.734654Z","shell.execute_reply":"2025-08-07T08:08:33.468635Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nfrom datasets import load_dataset\nimport json\n\nimport os\nos.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\nos.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom tqdm.auto import tqdm \nimport gc\nfrom transformer_lens import HookedTransformer\nfrom huggingface_hub import login\nfrom torch.utils.data import DataLoader, TensorDataset\n\nfrom accelerate import Accelerator\nimport wandb\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T08:08:33.471048Z","iopub.execute_input":"2025-08-07T08:08:33.471280Z","iopub.status.idle":"2025-08-07T08:09:11.241735Z","shell.execute_reply.started":"2025-08-07T08:08:33.471258Z","shell.execute_reply":"2025-08-07T08:09:11.241173Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nfrom kaggle_secrets import UserSecretsClient\nimport wandb\n\nuser_secrets = UserSecretsClient()\n\nwandb_api_key = user_secrets.get_secret(\"WANDB_API_KEY\")\n\nwandb.login(key=wandb_api_key)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T08:09:11.242404Z","iopub.execute_input":"2025-08-07T08:09:11.242591Z","iopub.status.idle":"2025-08-07T08:09:17.868618Z","shell.execute_reply.started":"2025-08-07T08:09:11.242576Z","shell.execute_reply":"2025-08-07T08:09:17.867921Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nACCUMULATION_STEPS = 16 # Increase this number if you still face OOM errors. It trades speed for memory.\n\naccelerator = Accelerator(\n    gradient_accumulation_steps=ACCUMULATION_STEPS,\n    log_with=\"wandb\"\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T08:09:17.870601Z","iopub.execute_input":"2025-08-07T08:09:17.871362Z","iopub.status.idle":"2025-08-07T08:09:17.912852Z","shell.execute_reply.started":"2025-08-07T08:09:17.871330Z","shell.execute_reply":"2025-08-07T08:09:17.911838Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"config = {\n    \"model_name\": \"google/gemma-2-2b-it\",\n    \"num_epochs\": 2,\n    \"seq_len\": 128,\n    \"subset_size\": 8, # Number of sequences to process at a time\n    \"learning_rate\": 1e-4,\n    \"accumulation_steps\": ACCUMULATION_STEPS,\n    \"torch_dtype\": torch.float16,\n    \"soft_cap\": 30.0,\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T08:09:17.913779Z","iopub.execute_input":"2025-08-07T08:09:17.914050Z","iopub.status.idle":"2025-08-07T08:09:17.918011Z","shell.execute_reply.started":"2025-08-07T08:09:17.914027Z","shell.execute_reply":"2025-08-07T08:09:17.917419Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"accelerator.init_trackers(\n    project_name=\"Tuned-Lens-Gemma-2B\", \n    config=config\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T08:09:17.918533Z","iopub.execute_input":"2025-08-07T08:09:17.918796Z","iopub.status.idle":"2025-08-07T08:09:24.629404Z","shell.execute_reply.started":"2025-08-07T08:09:17.918777Z","shell.execute_reply":"2025-08-07T08:09:24.628846Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nACTIVATION_DIR = \"/kaggle/working/activations\"\nPROBE_DIR = \"/kaggle/working/probes\"\nos.makedirs(ACTIVATION_DIR, exist_ok=True)\nos.makedirs(PROBE_DIR, exist_ok=True)\nDEVICE = accelerator.device","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T08:09:24.630084Z","iopub.execute_input":"2025-08-07T08:09:24.630360Z","iopub.status.idle":"2025-08-07T08:09:24.635583Z","shell.execute_reply.started":"2025-08-07T08:09:24.630332Z","shell.execute_reply":"2025-08-07T08:09:24.634924Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"hf_token = user_secrets.get_secret(\"HF_TOKEN\")\nlogin(token=hf_token)\n\n\nmodel = HookedTransformer.from_pretrained(\n    config[\"model_name\"], \n    device=\"cpu\", \n    torch_dtype=config[\"torch_dtype\"],\n    center_unembed=False\n)\n\nfor block in model.blocks:\n    block.gradient_checkpointing = True\n\n# Freeze parameters\nmodel.eval()\nfor param in model.parameters():\n    param.requires_grad = False\n\ntokenizer = model.tokenizer\n\n# Move the model to the correct device\nmodel = model.to(DEVICE)\n\nprint(f\"Model '{config['model_name']}' loaded onto device: {model.cfg.device}\")\nprint(\"Activation Checkpointing: ENABLED (manual block method)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T08:09:24.636302Z","iopub.execute_input":"2025-08-07T08:09:24.636527Z","iopub.status.idle":"2025-08-07T08:10:45.985070Z","shell.execute_reply.started":"2025-08-07T08:09:24.636511Z","shell.execute_reply":"2025-08-07T08:10:45.984218Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"n_layers = model.cfg.n_layers\nd_model = model.cfg.d_model\nd_vocab = model.cfg.d_vocab","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T08:10:45.985938Z","iopub.execute_input":"2025-08-07T08:10:45.986231Z","iopub.status.idle":"2025-08-07T08:10:45.990637Z","shell.execute_reply.started":"2025-08-07T08:10:45.986208Z","shell.execute_reply":"2025-08-07T08:10:45.989840Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nBATCH_SIZE = 128\nSEQ_LEN = 128\nSUBSET_SIZE = 8 ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T08:10:45.993103Z","iopub.execute_input":"2025-08-07T08:10:45.993280Z","iopub.status.idle":"2025-08-07T08:10:46.015033Z","shell.execute_reply.started":"2025-08-07T08:10:45.993266Z","shell.execute_reply":"2025-08-07T08:10:46.014274Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"NUM_EXAMPLES = 39470\ndataset = load_dataset(\"allenai/c4\", name=\"en\", split=\"train\", streaming=True)\ntext_samples = []\nfor example in dataset:\n    text = example.get(\"text\", \"\").strip()\n    if text:\n        text_samples.append(text)\n    if len(text_samples) >= NUM_EXAMPLES:\n        break\nprint(f\"Loaded {len(text_samples)} clean text samples\")\n\n# Join all text into one long sequence of tokens\njoined_text = \"\\n\\n\".join(text_samples) \nall_token_ids = tokenizer.encode(joined_text, return_tensors='pt')[0]  # shape: (total_tokens,)\nprint(f\"Token sequence length: {len(all_token_ids)} tokens\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T08:10:46.015706Z","iopub.execute_input":"2025-08-07T08:10:46.015931Z","iopub.status.idle":"2025-08-07T08:12:21.791391Z","shell.execute_reply.started":"2025-08-07T08:10:46.015914Z","shell.execute_reply":"2025-08-07T08:12:21.790618Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Let's define your special strings exactly:\nprefix_text = \"<start_of_turn>user\\n\"\nsuffix_text = \"<end_of_turn>\"\n\n# Encode prefix and suffix tokens with the tokenizer:\nprefix_tokens = tokenizer.encode(prefix_text, add_special_tokens=False)  # e.g. [....]\nsuffix_tokens = tokenizer.encode(suffix_text, add_special_tokens=False)  # e.g. [....]\n\nprint(f\"Prefix tokens count: {len(prefix_tokens)}\")\nprint(f\"Suffix tokens count: {len(suffix_tokens)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T08:12:21.838951Z","iopub.execute_input":"2025-08-07T08:12:21.839158Z","iopub.status.idle":"2025-08-07T08:12:21.846523Z","shell.execute_reply.started":"2025-08-07T08:12:21.839143Z","shell.execute_reply":"2025-08-07T08:12:21.845902Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def create_dataset_from_tokens(token_ids, seq_len):\n    examples = []\n    total_tokens = len(token_ids)\n    seq_len=seq_len-4\n    for i in range(0, total_tokens - seq_len, seq_len):\n        input_ids = token_ids[i : i + seq_len]\n        wrapped_seq=prefix_tokens+input_ids.tolist()+suffix_tokens\n        wrapped_tensor=torch.tensor(wrapped_seq, dtype=torch.long)\n        examples.append(wrapped_tensor)\n    input_ids_tensor = torch.stack(examples)\n    return input_ids_tensor","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T08:12:21.847357Z","iopub.execute_input":"2025-08-07T08:12:21.847583Z","iopub.status.idle":"2025-08-07T08:12:21.874054Z","shell.execute_reply.started":"2025-08-07T08:12:21.847551Z","shell.execute_reply":"2025-08-07T08:12:21.873561Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"TARGET_NUM_SEQUENCES = 130351\ninput_ids = create_dataset_from_tokens(all_token_ids, seq_len=config[\"seq_len\"])\ninput_ids = input_ids[:TARGET_NUM_SEQUENCES]\nprint(f\"Total sequences: {len(input_ids)}\")\nN = len(input_ids)\nnum_subsets = N // config[\"subset_size\"]\nprint(num_subsets)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T08:12:21.874771Z","iopub.execute_input":"2025-08-07T08:12:21.875077Z","iopub.status.idle":"2025-08-07T08:12:25.329888Z","shell.execute_reply.started":"2025-08-07T08:12:21.875059Z","shell.execute_reply":"2025-08-07T08:12:25.329280Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def stable_kl_loss(probe_logits, final_logits, top_k=100, eps=1e-8):\n    B, S, V = final_logits.size()\n\n    _, topk_final = torch.topk(final_logits, top_k, dim=-1)\n    _, topk_probe = torch.topk(probe_logits, top_k, dim=-1)\n\n    combined = torch.cat([topk_final, topk_probe], dim=-1).view(-1, 2 * top_k)  # shape: (B*S, 2*k)\n\n    max_len = combined.size(-1)\n    unique_list = []\n    mask_list = []\n    for idxs in combined:\n        unique = torch.unique(idxs, sorted=True)\n        # Pad with the last valid index if shorter than max_len\n        if unique.size(0) < max_len:\n            padded = torch.cat([unique, unique[-1].repeat(max_len - unique.size(0))])\n        else:\n            padded = unique[:max_len]\n        unique_list.append(padded)\n        # Create mask to mark valid indices\n        mask_list.append(torch.arange(max_len, device=unique.device) < unique.size(0))\n\n    union_indices = torch.stack(unique_list).view(B, S, max_len)\n    valid_mask = torch.stack(mask_list).view(B, S, max_len)  \n\n\n    probe_subset_logits = torch.gather(probe_logits, 2, union_indices).float()\n    final_subset_logits = torch.gather(final_logits, 2, union_indices).float()\n\n    target_probs = F.softmax(final_subset_logits, dim=-1).clamp(min=eps)\n    target_log_probs = torch.log(target_probs)\n    pred_log_probs = F.log_softmax(probe_subset_logits, dim=-1)\n\n    # Compute KL divergence elements and mask out padded positions\n    kl_elements = target_probs * (target_log_probs - pred_log_probs) * valid_mask.float()\n\n    # Sum over vocabulary subset dimension\n    kl_per_token = kl_elements.sum(dim=-1)\n\n    # Compute mean only over valid tokens (positions where any valid vocab exists)\n    # This prevents division by zero and excludes padded positions properly\n    token_mask = (valid_mask.any(dim=-1)).float()  # shape (B, S)\n    loss = kl_per_token.sum() / (token_mask.sum() + eps)\n    return loss\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T08:12:25.330614Z","iopub.execute_input":"2025-08-07T08:12:25.330851Z","iopub.status.idle":"2025-08-07T08:12:25.339334Z","shell.execute_reply.started":"2025-08-07T08:12:25.330833Z","shell.execute_reply":"2025-08-07T08:12:25.338785Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\ndef get_activations(model, batch_input):  # Added 'model' as the first argument\n    with torch.no_grad():\n        hooks_to_cache = [f'blocks.{i}.hook_resid_post' for i in range(n_layers)] \n\n        # Now, it uses the specific 'model' object that we pass to it\n        logits, cache = model.run_with_cache(batch_input, names_filter=hooks_to_cache)\n        \n        resid_post_outs = [cache[f'blocks.{i}.hook_resid_post'] for i in range(n_layers)]\n\n    return resid_post_outs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T08:17:56.370410Z","iopub.execute_input":"2025-08-07T08:17:56.371043Z","iopub.status.idle":"2025-08-07T08:17:56.377126Z","shell.execute_reply.started":"2025-08-07T08:17:56.371018Z","shell.execute_reply":"2025-08-07T08:17:56.376614Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def compute_logits_from_resid(resid, soft_cap=30.0):\n\n    normed = ln_final(resid)\n\n    W_U_casted = W_U.to(normed.device).type_as(normed)\n    b_U_casted = b_U.to(normed.device).type_as(normed)\n    logits = torch.einsum('bsd,dk->bsk', normed, W_U_casted) + b_U_casted\n\n    logits = soft_cap * torch.tanh(logits / soft_cap)#done in gemma 2\n    return logits\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T08:12:25.364055Z","iopub.execute_input":"2025-08-07T08:12:25.364302Z","iopub.status.idle":"2025-08-07T08:12:25.384309Z","shell.execute_reply.started":"2025-08-07T08:12:25.364264Z","shell.execute_reply":"2025-08-07T08:12:25.383596Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class Probe(nn.Module):\n    def __init__(self, d_model):\n        super().__init__()\n        self.linear = nn.Linear(d_model, d_model, bias=True)\n        torch.nn.init.xavier_normal_(self.linear.weight)\n        torch.nn.init.zeros_(self.linear.bias)            \n\n    def forward(self, x):\n        return self.linear(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T08:12:25.385009Z","iopub.execute_input":"2025-08-07T08:12:25.385171Z","iopub.status.idle":"2025-08-07T08:12:25.402063Z","shell.execute_reply.started":"2025-08-07T08:12:25.385158Z","shell.execute_reply":"2025-08-07T08:12:25.401402Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"W_U = model.state_dict()['unembed.W_U'].to(DEVICE).clone()\nb_U = model.state_dict()['unembed.b_U'].to(DEVICE).clone()\n\nimport copy\nln_final = copy.deepcopy(model.ln_final).to(DEVICE)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T08:12:25.402734Z","iopub.execute_input":"2025-08-07T08:12:25.402895Z","iopub.status.idle":"2025-08-07T08:12:25.443957Z","shell.execute_reply.started":"2025-08-07T08:12:25.402882Z","shell.execute_reply":"2025-08-07T08:12:25.443454Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\nimport os\nimport torch\n\n\ncheckpoint_dir = \"/kaggle/working/checkpoints\"\nos.makedirs(checkpoint_dir, exist_ok=True)\n\n\nresume_epoch = 0\nresume_chunk = 0\n\nresume_from_checkpoint = os.path.exists(checkpoint_dir) and len(os.listdir(checkpoint_dir)) > 0\n\nif resume_from_checkpoint:\n    print(\"Resuming from checkpoint...\")\n    accelerator.load_state(checkpoint_dir)\n    \n    progress_tracker = torch.load(os.path.join(checkpoint_dir, \"progress_tracker.pt\"))\n    resume_epoch = progress_tracker[\"epoch\"]\n    resume_chunk = progress_tracker[\"chunk\"] + 1 # We start from the *next* chunk\n    print(f\"Resuming from Epoch {resume_epoch}, Chunk {resume_chunk}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import time\n\nCHUNK_SIZE = 32\nnum_chunks = (num_subsets + CHUNK_SIZE - 1) // CHUNK_SIZE\n\nprobes = nn.ModuleList([Probe(d_model) for _ in range(n_layers)])\noptimizers = [torch.optim.AdamW(probe.parameters(), lr=config[\"learning_rate\"]) for probe in probes]\nprobes, optimizers = accelerator.prepare(probes, optimizers)\n\nW_U = W_U.to(DEVICE)\nb_U = b_U.to(DEVICE)\nln_final = ln_final.to(DEVICE)\n\n\nfor epoch in range(resume_epoch, config[\"num_epochs\"]):\n    print(f\"--- Starting Epoch {epoch + 1}/{config['num_epochs']} ---\")\n    \n    for chunk_idx in tqdm(range(resume_chunk, num_chunks), initial=resume_chunk, total=num_chunks, desc=\"Chunk Progress\"):\n        start_subset = chunk_idx * CHUNK_SIZE\n        end_subset = min(start_subset + CHUNK_SIZE, num_subsets)\n        print(f\"\\n-- Processing Chunk {chunk_idx + 1}/{num_chunks} (Subsets {start_subset} to {end_subset-1}) --\")\n        caching_start_time = time.time()\n\n        # === STEP 1: CACHE Activations for the current chunk ===\n        print(\"Caching activations for this chunk...\")\n        # Prepare the model for inference on the accelerator\n        model_for_caching = accelerator.prepare_model(model)\n        \n        for subset_idx in tqdm(range(start_subset, end_subset), desc=\"Caching\"):\n            start = subset_idx * config[\"subset_size\"]\n            end = start + config[\"subset_size\"]\n            subset_input = input_ids[start:end].to(DEVICE)\n            with torch.no_grad():\n                resid_post_outs = get_activations(model_for_caching, subset_input)\n            \n            cpu_resid_post_outs = [act.to(\"cpu\") for act in resid_post_outs]\n            torch.save(cpu_resid_post_outs, f'{ACTIVATION_DIR}/activations_{subset_idx + 1}.pth')\n\n            del subset_input, resid_post_outs, cpu_resid_post_outs\n            gc.collect()\n            torch.cuda.empty_cache()\n\n        caching_duration = time.time() - caching_start_time\n        training_start_time = time.time()\n\n\n        # === STEP 2: TRAIN Probes on this chunk's activations ===\n        print(\"Training probes on this chunk's activations...\")\n        for layer in range(n_layers):\n            probe = probes[layer]\n            opt = optimizers[layer]\n            probe.train()\n            \n            for subset_idx in tqdm(range(start_subset, end_subset), desc=f\"Layer {layer} Training\"):\n                with accelerator.accumulate(probe):\n                    acts = torch.load(f'{ACTIVATION_DIR}/activations_{subset_idx + 1}.pth', map_location='cpu')\n                    activ_batch = acts[layer].to(DEVICE).to(torch.float32)\n                    final_layer_activ_batch = acts[-1].to(DEVICE).to(torch.float32)\n                    \n                    with torch.no_grad():\n                        final_logits_batch = compute_logits_from_resid(final_layer_activ_batch)\n                    \n                    with accelerator.autocast():\n                        out = probe(activ_batch)\n                        normed_out = ln_final(out)\n                        W_U_casted = W_U.to(normed_out.dtype)\n                        b_U_casted = b_U.to(normed_out.dtype)\n                        probe_logits = torch.einsum('bsd,dk->bsk', normed_out, W_U_casted) + b_U_casted\n                        probe_logits = config[\"soft_cap\"] * torch.tanh(probe_logits / config[\"soft_cap\"])\n                        loss = stable_kl_loss(probe_logits, final_logits_batch)\n\n                    accelerator.backward(loss)\n                    opt.step()\n                    opt.zero_grad()\n                    accelerator.log({\"loss\": loss.item() * config[\"accumulation_steps\"]})\n\n        training_duration = time.time() - training_start_time\n\n        accelerator.log({\n            \"caching_duration_sec\": caching_duration,\n            \"training_duration_sec\": training_duration\n        })\n\n        # === STEP 3: CLEAN UP the cached files for this chunk ===\n        print(\"Cleaning up cached activation files for this chunk...\")\n        for subset_idx in range(start_subset, end_subset):\n            file_path = f'{ACTIVATION_DIR}/activations_{subset_idx + 1}.pth'\n            if os.path.exists(file_path):\n                os.remove(file_path)\n\n        print(f\"Chunk {chunk_idx} complete. Saving checkpoint...\")\n        accelerator.save_state(checkpoint_dir)\n        torch.save({\"epoch\": epoch, \"chunk\": chunk_idx}, os.path.join(checkpoint_dir, \"progress_tracker.pt\"))\n        # === END OF CHUNK ===\n\n    resume_chunk = 0\n\n# --- Save Final Probes ---\naccelerator.wait_for_everyone()\nprint(\"\\nSaving final trained probes...\")\nfor i, probe in enumerate(probes):\n    unwrapped_probe = accelerator.unwrap_model(probe)\n    accelerator.save(unwrapped_probe.state_dict(), f'{PROBE_DIR}/probe_{i}.pt')\n\naccelerator.end_training()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T08:23:01.752774Z","iopub.execute_input":"2025-08-07T08:23:01.753075Z","execution_failed":"2025-08-07T08:35:14.960Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Training finished. Deleting the base model from VRAM...\")\ndel model\ngc.collect()\nif torch.cuda.is_available():\n    torch.cuda.empty_cache()\n\nprint(\"Cleanup complete.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T08:12:27.257023Z","iopub.status.idle":"2025-08-07T08:12:27.257369Z","shell.execute_reply.started":"2025-08-07T08:12:27.257182Z","shell.execute_reply":"2025-08-07T08:12:27.257197Z"}},"outputs":[],"execution_count":null}]}