{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.11"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12336928,"sourceType":"datasetVersion","datasetId":7777053},{"sourceId":12340085,"sourceType":"datasetVersion","datasetId":7779271},{"sourceId":247648806,"sourceType":"kernelVersion"}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!git clone https://github.com/vsriramv/Truth_is_Universal.git","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\nfrom huggingface_hub import login\n\nHF_TOKEN = \"<add yours>\"\n\nlogin(token=HF_TOKEN)\n\nmodel_name = \"google/gemma-2-2b-it\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_name, token=HF_TOKEN)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=torch.bfloat16,\n    device_map=\"auto\",\n    token=HF_TOKEN\n)\n\ndef format_prompt(user_message: str) -> str:\n    return f\"<start_of_turn>user\\n{user_message}<end_of_turn>\\n<start_of_turn>model\\n\"\n\npipe = pipeline(\n    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"prompt = format_prompt(\"Who am i?\")\noutputs = pipe(prompt, max_new_tokens=256, do_sample=True, temperature=0.7\n              )\nresponse = outputs[0][\"generated_text\"].split(\"<start_of_turn>model\\n\")[-1].strip()\nprint(response)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install -q huggingface_hub git-lfs\n\nfrom huggingface_hub import snapshot_download\n\nsnapshot_download(\n    repo_id=\"google/gemma-2-2b-it\",\n    local_dir=\"./gemma-2-2b-it\",\n    token=\"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\",  \n    local_dir_use_symlinks=False\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%cd /kaggle/working/Truth_is_Universal","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"with open(\"/kaggle/working/Truth_is_Universal/config.ini\",\"w\") as f:\n    f.write(\"\"\"[Gemma2]\nweights_directory = /kaggle/working/\n2B_chat_subdir = gemma-2-2b-it\"\"\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%cat config.ini","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"'''#ran this\n\nfile_path = \"/kaggle/working/Truth_is_Universal/generate_acts.py\"\n\n# Read the file\nwith open(file_path, \"r\") as f:\n    lines = f.readlines()\n\n# Replace the problematic line\nfor i, line in enumerate(lines):\n    if \"self.out, _ = module_outputs\" in line:\n        lines[i] = \"        self.out = module_outputs\\n\"\n        print(f\"ðŸ”§ Patched line {i+1}: self.out = module_outputs\")\n\n# Write it back\nwith open(file_path, \"w\") as f:\n    f.writelines(lines)\n\nprint(\"File patchedÂ successfully.\")'''","metadata":{"execution":{"iopub.status.busy":"2025-07-03T06:11:40.39918Z","iopub.execute_input":"2025-07-03T06:11:40.399494Z","iopub.status.idle":"2025-07-03T06:11:40.406402Z","shell.execute_reply.started":"2025-07-03T06:11:40.399467Z","shell.execute_reply":"2025-07-03T06:11:40.405793Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#!grep -A 5 \"class Hook\" /kaggle/working/Truth_is_Universal/utils.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T10:42:06.425771Z","iopub.execute_input":"2025-07-02T10:42:06.426077Z","iopub.status.idle":"2025-07-02T10:42:06.588171Z","shell.execute_reply.started":"2025-07-02T10:42:06.426049Z","shell.execute_reply":"2025-07-02T10:42:06.587228Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"'''file_path = \"/kaggle/working/Truth_is_Universal/utils.py\"\n\n# Read the file\nwith open(file_path, \"r\") as f:\n    lines = f.readlines()\n\n# Replace the Hook class definition\nnew_hook_class = [\n    \"class Hook:\\n\",\n    \"    def __init__(self, name=None):\\n\",\n    \"        self.name = name\\n\",\n    \"        self.out = None\\n\\n\",\n    \"    def __call__(self, module, module_input, module_output):\\n\",\n    \"        self.out = module_output\\n\"\n]\n\n# Find and replace the Hook class\ninside_hook = False\nnew_lines = []\nfor line in lines:\n    if line.strip().startswith(\"class Hook\"):\n        inside_hook = True\n        new_lines.extend(new_hook_class)\n        print(\"ðŸ”§ Patched: Hook class replaced.\")\n        continue\n    if inside_hook:\n        # Skip lines until we reach the next class or function\n        if line.strip().startswith(\"class \") or line.strip().startswith(\"def \"):\n            inside_hook = False\n            new_lines.append(line)\n        continue\n    new_lines.append(line)\n\n# Save back\nwith open(file_path, \"w\") as f:\n    f.writelines(new_lines)\n\nprint(\"âœ… utils.py patched and ready.\")\n'''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T10:38:45.336433Z","iopub.execute_input":"2025-07-02T10:38:45.337242Z","iopub.status.idle":"2025-07-02T10:38:45.344777Z","shell.execute_reply.started":"2025-07-02T10:38:45.33721Z","shell.execute_reply":"2025-07-02T10:38:45.344192Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"'''#ran this\n\nfile_path = \"/kaggle/working/Truth_is_Universal/generate_acts.py\"\n\n# Read the file\nwith open(file_path, \"r\") as f:\n    lines = f.readlines()\n\n# Patch the indexing line\nfor i, line in enumerate(lines):\n    if \"acts[layer].append(hook.out[0, -1])\" in line:\n        lines[i] = \"        acts[layer].append(hook.out[0][-1])\\n\"\n        print(f\"ðŸ”§ Patched line {i+1}: hook.out[0][-1]\")\n\n# Save back\nwith open(file_path, \"w\") as f:\n    f.writelines(lines)\n\nprint(\"âœ… generate_acts.py patchedÂ andÂ ready.\")'''","metadata":{"execution":{"iopub.status.busy":"2025-07-03T06:12:54.364024Z","iopub.execute_input":"2025-07-03T06:12:54.364341Z","iopub.status.idle":"2025-07-03T06:12:54.371512Z","shell.execute_reply.started":"2025-07-03T06:12:54.364313Z","shell.execute_reply":"2025-07-03T06:12:54.370814Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"'''file_path = \"/kaggle/working/Truth_is_Universal/generate_acts.py\"\n\n# Read the file\nwith open(file_path, \"r\") as f:\n    lines = f.readlines()\n\n# Fix the incorrect indexing with proper indentation\nfor i, line in enumerate(lines):\n    if \"acts[layer].append(hook.out[0, -1])\" in line:\n        lines[i] = \"    acts[layer].append(hook.out[0][-1])\\n\"  # 4 spaces\n        print(f\"ðŸ”§ Patched line {i+1} with correct indentation and indexing.\")\n\n# Save the updated file\nwith open(file_path, \"w\") as f:\n    f.writelines(lines)\n\nprint(\"âœ… Fixed indentation and indexing in generate_acts.py.\")'''","metadata":{"execution":{"iopub.status.busy":"2025-07-01T16:21:28.531674Z","iopub.execute_input":"2025-07-01T16:21:28.53192Z","iopub.status.idle":"2025-07-01T16:21:28.549761Z","shell.execute_reply.started":"2025-07-01T16:21:28.531882Z","shell.execute_reply":"2025-07-01T16:21:28.549167Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"'''file_path = \"/kaggle/working/Truth_is_Universal/generate_acts.py\"\n\n# Read the file\nwith open(file_path, \"r\") as f:\n    lines = f.readlines()\n\n# Patch the indexing line with 8 spaces\nfor i, line in enumerate(lines):\n    if \"acts[layer].append(hook.out[0, -1])\" in line:\n        lines[i] = \"        acts[layer].append(hook.out[0][-1])\\n\"  # 8 spaces\n        print(f\"ðŸ”§ Patched line {i+1} with correct indexing and indentation.\")\n\n# Write back to file\nwith open(file_path, \"w\") as f:\n    f.writelines(lines)\n\nprint(\"âœ… generate_acts.py is now correctlyÂ patched.\")'''","metadata":{"execution":{"iopub.status.busy":"2025-07-01T16:21:28.550614Z","iopub.execute_input":"2025-07-01T16:21:28.550867Z","iopub.status.idle":"2025-07-01T16:21:28.56155Z","shell.execute_reply.started":"2025-07-01T16:21:28.550846Z","shell.execute_reply":"2025-07-01T16:21:28.561022Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"'''file_path = \"/kaggle/working/Truth_is_Universal/generate_acts.py\"\n\nwith open(file_path, \"r\") as f:\n    lines = f.readlines()\n\nfor i, line in enumerate(lines):\n    if \"acts[layer].append(hook.out[0, -1])\" in line:\n        leading_spaces = len(line) - len(line.lstrip())  # detect original indent\n        lines[i] = \" \" * leading_spaces + \"acts[layer].append(hook.out[0][-1])\\n\"\n        print(f\"ðŸ”§ Patched line {i+1} with {leading_spaces} spaces of indent.\")\n\nwith open(file_path, \"w\") as f:\n    f.writelines(lines)\n\nprint(\"âœ… Patched generate_acts.py correctly with originalÂ indentation.\")'''","metadata":{"execution":{"iopub.status.busy":"2025-07-01T16:21:28.562271Z","iopub.execute_input":"2025-07-01T16:21:28.562823Z","iopub.status.idle":"2025-07-01T16:21:28.573351Z","shell.execute_reply.started":"2025-07-01T16:21:28.562802Z","shell.execute_reply":"2025-07-01T16:21:28.57277Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"'''file_path = \"/kaggle/working/Truth_is_Universal/generate_acts.py\"\n\n# Read all lines\nwith open(file_path, \"r\") as f:\n    lines = f.readlines()\n\n# Locate and patch the specific line\nfor i, line in enumerate(lines):\n    if \"acts[layer].append(hook.out[0, -1])\" in line:\n        lines[i] = \"        acts[layer].append(hook.out[0][-1])\\n\"  # EXACT indent\n        print(f\"âœ… Patched line {i+1}: acts[layer].append(hook.out[0][-1])\")\n\n# Write back the corrected file\nwith open(file_path, \"w\") as f:\n    f.writelines(lines)\n\nprint(\"âœ… generate_acts.py is now fixed with correctÂ indentation.\")'''","metadata":{"execution":{"iopub.status.busy":"2025-07-01T16:21:30.490921Z","iopub.execute_input":"2025-07-01T16:21:30.491618Z","iopub.status.idle":"2025-07-01T16:21:30.497568Z","shell.execute_reply.started":"2025-07-01T16:21:30.491593Z","shell.execute_reply":"2025-07-01T16:21:30.496769Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"'''#ran this\n\nfile_path = \"/kaggle/working/Truth_is_Universal/generate_acts.py\"\n\nwith open(file_path, \"r\") as f:\n    for i, line in enumerate(f):\n        if 55 <= i <= 80:\n            print(f\"{i+1}: {line.rstrip()}\")'''","metadata":{"execution":{"iopub.status.busy":"2025-07-03T06:13:34.364992Z","iopub.execute_input":"2025-07-03T06:13:34.365555Z","iopub.status.idle":"2025-07-03T06:13:34.371209Z","shell.execute_reply.started":"2025-07-03T06:13:34.365528Z","shell.execute_reply":"2025-07-03T06:13:34.370537Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"'''#ran this\n\nfile_path = \"/kaggle/working/Truth_is_Universal/generate_acts.py\"\n\nwith open(file_path, \"r\") as f:\n    lines = f.readlines()\n\n# Add 4 more spaces to existing indentation of line 67 (index 66)\ntarget_line = 66\nlines[target_line] = \"            \" + lines[target_line].lstrip()  # 12 spaces\n\nwith open(file_path, \"w\") as f:\n    f.writelines(lines)\n\nprint(\"âœ… Line 67 now has 12 spaces ofÂ indentation.\")'''","metadata":{"execution":{"iopub.status.busy":"2025-07-03T06:13:58.37573Z","iopub.execute_input":"2025-07-03T06:13:58.376046Z","iopub.status.idle":"2025-07-03T06:13:58.382165Z","shell.execute_reply.started":"2025-07-03T06:13:58.376026Z","shell.execute_reply":"2025-07-03T06:13:58.381438Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"'''file_path = \"/kaggle/working/Truth_is_Universal/generate_acts.py\"\n\nwith open(file_path, \"r\") as f:\n    lines = f.readlines()\n\n# Add 4 more spaces to existing indentation of line 82 (index 81)\ntarget_line = 81\nlines[target_line] = \"            \" + lines[target_line].lstrip()  # 12 spaces\n\nwith open(file_path, \"w\") as f:\n    f.writelines(lines)\n\nprint(\"âœ… Line 82 now has 12 spaces ofÂ indentation.\")'''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T16:21:38.248017Z","iopub.execute_input":"2025-07-01T16:21:38.248279Z","iopub.status.idle":"2025-07-01T16:21:38.254102Z","shell.execute_reply.started":"2025-07-01T16:21:38.248259Z","shell.execute_reply":"2025-07-01T16:21:38.253369Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"'''#ran this\n\nfile_path = \"/kaggle/working/Truth_is_Universal/generate_acts.py\"\n\nwith open(file_path, \"r\") as f:\n    lines = f.readlines()\n\n# Replace the stacking line\nfor i, line in enumerate(lines):\n    if \"acts[layer] = t.stack(\" in line:\n        lines[i] = \"        acts[layer] = t.stack([x[-1] for x in act]).float()\\n\"\n        print(f\"âœ… Patched line {i+1} to stack only the last token from each activation.\")\n\nwith open(file_path, \"w\") as f:\n    f.writelines(lines)\n\nprint(\"generate_acts.py is now collecting only last-tokenÂ activations.\")'''","metadata":{"execution":{"iopub.status.busy":"2025-07-03T06:15:04.988412Z","iopub.execute_input":"2025-07-03T06:15:04.988746Z","iopub.status.idle":"2025-07-03T06:15:04.995593Z","shell.execute_reply.started":"2025-07-03T06:15:04.98872Z","shell.execute_reply":"2025-07-03T06:15:04.994974Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%cd /kaggle/working/Truth_is_Universal","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!python generate_attn_acts.py --model_family Gemma2 --model_size 2B --model_type chat --layers -1 --datasets all_topic_specific --device cuda:0\n","metadata":{"execution":{"iopub.status.busy":"2025-07-03T06:32:07.466144Z","iopub.execute_input":"2025-07-03T06:32:07.466515Z","iopub.status.idle":"2025-07-03T06:34:24.944037Z","shell.execute_reply.started":"2025-07-03T06:32:07.46649Z","shell.execute_reply":"2025-07-03T06:34:24.94302Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!zip -r /kaggle/working/activations_52_layer_new_prompt.zip /kaggle/working/Truth_is_Universal/acts","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%cp -r /kaggle/input/activations-zip/kaggle/working/Truth_is_Universal/acts /kaggle/working/Truth_is_Universal/acts","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T06:02:54.525715Z","iopub.execute_input":"2025-07-02T06:02:54.526007Z","iopub.status.idle":"2025-07-02T06:06:28.092867Z","shell.execute_reply.started":"2025-07-02T06:02:54.525965Z","shell.execute_reply":"2025-07-02T06:06:28.092086Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#%cp -r /kaggle/input/activations-new-pf/kaggle/working/Truth_is_Universal/acts /kaggle/working/Truth_is_Universal/acts","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T17:39:50.323531Z","iopub.execute_input":"2025-07-01T17:39:50.324133Z","iopub.status.idle":"2025-07-01T17:42:42.590496Z","shell.execute_reply.started":"2025-07-01T17:39:50.32411Z","shell.execute_reply":"2025-07-01T17:42:42.589652Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#%rm -rf /kaggle/working/Truth_is_Universal/acts","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T11:37:10.757299Z","iopub.execute_input":"2025-07-01T11:37:10.758004Z","iopub.status.idle":"2025-07-01T11:37:12.199113Z","shell.execute_reply.started":"2025-07-01T11:37:10.757975Z","shell.execute_reply":"2025-07-01T11:37:12.198048Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"'''import os\n\n# Paste your actual username and key here:\nos.environ[\"KAGGLE_USERNAME\"] = \"sharankeshavs\"\nos.environ[\"KAGGLE_KEY\"] = \"22e318c6eb8d6363c54d6a874d089340\"'''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T06:52:57.115798Z","iopub.execute_input":"2025-07-01T06:52:57.116689Z","iopub.status.idle":"2025-07-01T06:52:57.121099Z","shell.execute_reply.started":"2025-07-01T06:52:57.116658Z","shell.execute_reply":"2025-07-01T06:52:57.120121Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#!kaggle kernels output sharankeshavs/viveka-truth-is-universal-trial2/ -p /kaggle/working/","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-27T10:07:12.92902Z","iopub.execute_input":"2025-06-27T10:07:12.929935Z","iopub.status.idle":"2025-06-27T10:07:50.946418Z","shell.execute_reply.started":"2025-06-27T10:07:12.929898Z","shell.execute_reply":"2025-06-27T10:07:50.945547Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#%ls","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-27T18:59:20.585301Z","iopub.execute_input":"2025-06-27T18:59:20.586113Z","iopub.status.idle":"2025-06-27T18:59:20.706441Z","shell.execute_reply.started":"2025-06-27T18:59:20.586079Z","shell.execute_reply":"2025-06-27T18:59:20.705306Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch \nimport numpy as np\nfrom utils import DataManager, dataset_sizes, collect_training_data, compute_statistics\nfrom probes import learn_truth_directions\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import LinearSegmentedColormap\nfrom sklearn.linear_model import LogisticRegression\nfrom scipy.stats import gaussian_kde\nfrom sklearn.decomposition import PCA\nfrom collections import defaultdict\nfrom sklearn.metrics import roc_auc_score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T06:06:28.101824Z","iopub.execute_input":"2025-07-02T06:06:28.102056Z","iopub.status.idle":"2025-07-02T06:06:28.306821Z","shell.execute_reply.started":"2025-07-02T06:06:28.102033Z","shell.execute_reply":"2025-07-02T06:06:28.306293Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# hyperparameters\nmodel_family = 'Gemma2' # options are 'Llama3', 'Llama2', 'Gemma', 'Gemma2' or 'Mistral'\nmodel_size = '2B'\nmodel_type = 'chat' # options are 'chat' or 'base'\nlayer = 19 # layer from which to extract activations\n\n# define datasets used for training\n# the ordering [affirmative_dataset1, negated_dataset1, affirmative_dataset2, negated_dataset2, ...] is required by some functions\ntrain_sets = [\"cities\", \"neg_cities\", \"sp_en_trans\", \"neg_sp_en_trans\", \"inventors\", \"neg_inventors\", \"animal_class\", \"neg_animal_class\", \"element_symb\", \"neg_element_symb\", \"facts\", \"neg_facts\"]\n# get size of each training dataset to include an equal number of statements from each topic in training data\ntrain_set_sizes = dataset_sizes(train_sets) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T06:13:58.102353Z","iopub.execute_input":"2025-07-02T06:13:58.103081Z","iopub.status.idle":"2025-07-02T06:13:58.109528Z","shell.execute_reply.started":"2025-07-02T06:13:58.103055Z","shell.execute_reply":"2025-07-02T06:13:58.108843Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Separation between true and false statements across layers","metadata":{}},{"cell_type":"code","source":"# find the layer with the largest separation between true and false statements\n# you need to have stored the activations in layers 1-26 for all datasets to run this cell\nlayers = np.arange(1, 52, 1)\ndatasets_separation = ['cities', 'neg_cities', 'sp_en_trans', 'neg_sp_en_trans', \\\n                       \"inventors\", \"neg_inventors\", \"animal_class\", \"neg_animal_class\", \"element_symb\"\\\n                       ,\"neg_element_symb\", \"facts\", \"neg_facts\"]\nlist_coords = {}\nfor dataset in datasets_separation:\n    between_class_variances = []\n    within_class_variances = []\n    for layer_nr in layers:\n        dm = DataManager() \n        dm.add_dataset(dataset, model_family, model_size, model_type,\n                        layer_nr, split=None, center=False, device='cpu')\n        acts, labels = dm.data[dataset]\n        # Calculate means for each class\n        false_stmnt_ids = labels == 0\n        true_stmnt_ids = labels == 1\n\n        false_acts = acts[false_stmnt_ids]\n        true_acts = acts[true_stmnt_ids]\n\n        mean_false = false_acts.mean(dim=0)\n        mean_true = true_acts.mean(dim=0)\n\n        # Calculate within-class variance\n        within_class_variance_false = false_acts.var(dim=0).mean()\n        within_class_variance_true = true_acts.var(dim=0).mean()\n        within_class_variances.append((within_class_variance_false + within_class_variance_true).item() / 2)\n\n        # Calculate between-class variance\n        overall_mean = acts.mean(dim=0)\n        between_class_variances.append(((mean_false - overall_mean).pow(2) \n                                        + (mean_true - overall_mean).pow(2)).mean().item() / 2)\n\n    plt.plot(layers, np.array(between_class_variances) / np.array(within_class_variances), label=dataset)\n    list_coords[dataset] = (list(layers), list(np.array(between_class_variances) / np.array(within_class_variances)))\nplt.legend(fontsize=10)\nplt.figure_size(10, 20)\nplt.xticks(np.arange(0,51,1))\nplt.ylabel('Between class variance /\\nwithin-class variance', fontsize=10)\nplt.xlabel('Layer', fontsize=10)\nplt.title('Separation between true and false\\nstatements across layers', fontsize=10)\nplt.grid(True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T06:06:28.364961Z","iopub.execute_input":"2025-07-02T06:06:28.365163Z","iopub.status.idle":"2025-07-02T06:06:34.507836Z","shell.execute_reply.started":"2025-07-02T06:06:28.365148Z","shell.execute_reply":"2025-07-02T06:06:34.507038Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"'''print(list_coords['cities'])\ngrouped_dict = {}\nfor i in range(len(list_coords.keys())):\n    grouped_dict[list_coords[i]] = [grouped]'''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T16:34:47.067882Z","iopub.execute_input":"2025-07-01T16:34:47.068726Z","iopub.status.idle":"2025-07-01T16:34:47.072579Z","shell.execute_reply.started":"2025-07-01T16:34:47.068696Z","shell.execute_reply":"2025-07-01T16:34:47.072029Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Supervised learning of the truth directions and classification accuracies","metadata":{}},{"cell_type":"code","source":"file_path = \"/kaggle/working/Truth_is_Universal/utils.py\"\n\nwith open(file_path, \"r\") as f:\n    for i, line in enumerate(f):\n        if 0 <= i <= 150:\n            try:\n                print(f\"{i+1}: {line.rstrip()}\")\n            except:\n                break","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"layer = 1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T06:39:50.620492Z","iopub.execute_input":"2025-07-02T06:39:50.621014Z","iopub.status.idle":"2025-07-02T06:39:50.624358Z","shell.execute_reply.started":"2025-07-02T06:39:50.620966Z","shell.execute_reply":"2025-07-02T06:39:50.623755Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"nr_runs = 15\nresults = {'t_g': defaultdict(list), 't_p': defaultdict(list), 'd_{LR}': defaultdict(list)}\n\nfor _ in range(nr_runs):\n    for i in range(0, len(train_sets), 2):\n        # leave one dataset out (affirmative + negated)\n        cv_train_sets = [set for j, set in enumerate(train_sets) if j not in (i, i+1)]\n        \n        # Collect training data\n        acts_centered, _, labels, polarities = collect_training_data(cv_train_sets, train_set_sizes, model_family,\n                                                          model_size, model_type, layer)\n        \n        # Fit model\n        t_g, t_p = learn_truth_directions(acts_centered, labels, polarities)\n\n        # fit LR for comparison\n        LR = LogisticRegression(penalty=None, fit_intercept=False)\n        LR.fit(acts_centered.numpy(), labels.numpy())\n        d_lr = torch.from_numpy(LR.coef_[0]).float()\n                \n        # Evaluate on held-out sets, assuming affirmative and negated dataset on the same topic are at index i and i+1\n        for j in range(2):\n            dataset = train_sets[i+j]\n            dm = DataManager()\n            dm.add_dataset(dataset, model_family, model_size, model_type, layer, split=None, center=False, device='cpu')\n            acts, labels = dm.get(dataset)\n\n            auroc = roc_auc_score(labels.numpy(), (acts @ t_g).numpy())\n            results['t_g'][dataset].append(auroc)\n            auroc = roc_auc_score(labels.numpy(), (acts @ t_p).numpy())\n            results['t_p'][dataset].append(auroc)\n            auroc = roc_auc_score(labels.numpy(), (acts @ d_lr).numpy())\n            results['d_{LR}'][dataset].append(auroc)\n\nstat_results = compute_statistics(results)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-07-02T06:46:33.039Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create a custom colormap from red to yellow\ncmap = LinearSegmentedColormap.from_list('red_yellow', [(1, 0, 0), (1, 1, 0)], N=100)\n\n# Create three subplots side-by-side\nfig, (ax1, ax2, ax3) = plt.subplots(figsize=(3.5, 6), ncols=3)\n\nfor ax, key in zip((ax1, ax2, ax3), ('t_g', 't_p', 'd_{LR}')):\n    grid = [[stat_results[key]['mean'][dataset]] for dataset in train_sets]\n    im = ax.imshow(grid, vmin=0, vmax=1, cmap=cmap)\n    \n    ax.set_aspect('auto')\n    ax.set_aspect(0.6)\n    \n    for i, row in enumerate(grid):\n        for j, val in enumerate(row):\n            ax.text(j, i, f'{val:.2f}', ha='center', va='center', fontsize=13)\n    \n    ax.set_yticks(range(len(train_sets)))\n    ax.set_xticks([])\n    ax.set_title(f\"${key}$\", fontsize=14)\n\nax1.set_yticklabels(train_sets, fontsize=13)\nax2.set_yticklabels([])\nax3.set_yticklabels([])\n\n# Adjust the layout to make room for the colorbar\nplt.subplots_adjust(top=0.9, bottom=0.05, left=0.1, right=0.9, wspace=0.4)\n\n# Add colorbar with a specified position\ncbar_ax = fig.add_axes([0.92, 0.15, 0.02, 0.7])  # [left, bottom, width, height]\ncbar = fig.colorbar(im, cax=cbar_ax)\ncbar.ax.tick_params(labelsize=13)\n\nfig.suptitle(\"AUROC\", fontsize=15)\nplt.show()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-07-02T06:46:33.039Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Activation vectors projected onto 2d truth subspace","metadata":{}},{"cell_type":"code","source":"# Compute t_g and t_p using all data\nacts_centered, _, labels, polarities = collect_training_data(train_sets, train_set_sizes, model_family, \n                                                    model_size, model_type, layer)\nt_g, t_p = learn_truth_directions(acts_centered, labels, polarities)\n\n# define helper functions for plotting\ndef collect_affirm_neg_data(train_sets, train_set_sizes, model_family, model_size, model_type, layer):\n    dm_affirm, dm_neg = DataManager(), DataManager()\n    for dataset_name in train_sets:\n        split = min(train_set_sizes.values()) / train_set_sizes[dataset_name]\n        if 'neg_' not in dataset_name:\n            dm_affirm.add_dataset(dataset_name, model_family, model_size, model_type, layer, split=split, center=False, device='cpu')\n        else:\n            dm_neg.add_dataset(dataset_name, model_family, model_size, model_type, layer, split=split, center=False, device='cpu')\n    return dm_affirm.get('train') + dm_neg.get('train')\n\ndef compute_t_affirm(acts_affirm, labels_affirm):\n    LR = LogisticRegression(penalty=None, fit_intercept=True)\n    LR.fit(acts_affirm.numpy(), labels_affirm.numpy())\n    return LR.coef_[0] / np.linalg.norm(LR.coef_[0])\n\ndef compute_orthonormal_vectors(t_g, t_p):\n    t_g_numpy = t_g.numpy()\n    t_p_numpy = t_p.numpy()\n    projection = np.dot(t_p_numpy, t_g_numpy) / np.dot(t_g_numpy, t_g_numpy) * t_g_numpy\n    t_p_orthonormal = (t_p_numpy - projection) / np.linalg.norm(t_p_numpy - projection)\n    t_g_orthonormal = t_g_numpy / np.linalg.norm(t_g_numpy)\n    return t_g_orthonormal, t_p_orthonormal\n\ndef project_activations(acts, t_g, t_p):\n    return t_g @ acts.numpy().T, t_p @ acts.numpy().T\n\ndef plot_vector(ax, vector, t_g_orthonormal, t_p_orthonormal, label, midpoint):\n    # Normalize input vector\n    vector_normalized = vector / np.linalg.norm(vector)\n    \n    # Compute vector_subspace\n    vector_subspace = np.array([(np.dot(t_g_orthonormal, vector_normalized)), \n                                (np.dot(t_p_orthonormal, vector_normalized))])\n\n    vector_subspace = vector_subspace / np.linalg.norm(vector_subspace)\n    \n    # Get current axis limits\n    xlim = ax.get_xlim()\n    ylim = ax.get_ylim()\n    \n    # Compute scale based on axis limits\n    axis_range = max(xlim[1] - xlim[0], ylim[1] - ylim[0])\n    scale = 0.4 * axis_range  # Adjust this factor to change the relative size of the vector\n    \n    # Compute arrow head position\n    arrow_head = np.array(midpoint) + scale * vector_subspace\n    \n    # Compute label offset based on axis limits\n    label_offset = np.array([0.03 * (xlim[1] - xlim[0]), 0.03 * (ylim[1] - ylim[0])])\n    \n    # Adjust label position to avoid overlap with arrow\n    label_position = arrow_head + label_offset * np.sign(vector_subspace)\n    \n    # Plot the vector\n    ax.quiver(*midpoint, *(scale * vector_subspace), \n              color='green', angles='xy', scale_units='xy', scale=1, \n              width=0.03)\n    \n    # Add label\n    ax.annotate(label, xy=label_position, fontsize=21, \n                ha='center', va='center')\n\n# Update the plot_scatter function to use the new plot_vector function\ndef plot_scatter(ax, proj_g, proj_p, labels, proj_g_other, proj_p_other, labels_other,\n                  title, plot_t_a=False, plot_t_g_t_p=False, **kwargs):\n    label_to_color = {0: 'indigo', 1: 'orange'}\n    label_to_marker = {0: 's', 1: '^'}\n\n    for label in [0, 1]:\n        idx = labels.numpy() == label\n        ax.scatter(proj_g[idx], proj_p[idx], c=label_to_color[label], marker=label_to_marker[label], alpha=0.5, s=5)\n        idx_other = labels_other.numpy() == label\n        if title == \"Affirmative & Negated\\nStatements\":\n            ax.scatter(proj_g_other[idx_other], proj_p_other[idx_other], c=label_to_color[label],\n                        marker=label_to_marker[label], alpha=0.5, s=5)\n        else:\n            ax.scatter(proj_g_other[idx_other], proj_p_other[idx_other], c='grey', marker=label_to_marker[label], alpha=0.1, s=5)\n\n    # Compute midpoint based on current axis limits\n    xlim = ax.get_xlim()\n    ylim = ax.get_ylim()\n    midpoint = (0.5 * (xlim[0] + xlim[1]), 0.5 * (ylim[0] + ylim[1]))\n\n    if plot_t_a:\n        plot_vector(ax, kwargs['t_affirm'], kwargs['t_g_orthonormal'], kwargs['t_p_orthonormal'], \"$t_A$\", midpoint)\n    if plot_t_g_t_p:\n        plot_vector(ax, kwargs['t_g'], kwargs['t_g_orthonormal'], kwargs['t_p_orthonormal'], \"$t_G$\", midpoint)\n        plot_vector(ax, kwargs['t_p'], kwargs['t_g_orthonormal'], kwargs['t_p_orthonormal'], \"$t_P$\", midpoint)\n\n    ax.set_title(title, fontsize=19)\n    #ax.set_yticks([])\n    #ax.set_xticks([])\n\n    # Update axis limits after plotting\n    ax.autoscale()\n    ax.set_aspect('equal')\n\ndef add_legend(ax):\n    handles = [plt.scatter([], [], c='indigo', marker='s', label='False'),\n               plt.scatter([], [], c='orange', marker='^', label='True')]\n    ax.legend(handles=handles, fontsize=18)\n\ndef plot_density(ax, acts, labels, t, xlabel):\n    # Convert inputs to NumPy arrays if they're PyTorch tensors\n    if isinstance(acts, torch.Tensor):\n        acts = acts.detach().cpu().numpy()\n    if isinstance(labels, torch.Tensor):\n        labels = labels.detach().cpu().numpy()\n    if isinstance(t, torch.Tensor):\n        t = t.detach().cpu().numpy()\n\n    # Compute projections\n    if t.ndim == 1:\n        proj = t @ acts.T\n    else:\n        proj = t.reshape(1, -1) @ acts.T\n    \n    # Compute axis limits based on projected activations\n    x_min, x_max = np.min(proj), np.max(proj)\n    x_range = x_max - x_min\n    x_padding = 0.1 * x_range  # Add 10% padding on each side\n    xlim = (x_min - x_padding, x_max + x_padding)\n    \n    # Set x-axis limits\n    ax.set_xlim(xlim)\n    \n    # Compute KDE for each label\n    x_grid = np.linspace(xlim[0], xlim[1], 400)\n    for label, color in zip([0, 1], ['indigo', 'orange']):\n        data = proj[labels == label]\n        kde = gaussian_kde(data)\n        density = kde(x_grid)\n        density /= np.trapz(density, x_grid)\n        ax.plot(x_grid, density, color=color)\n\n    # Plot scatter points\n    y_scatter = np.ones(np.shape(proj)) * (-0.05)\n    colors = ['indigo' if label == 0 else 'orange' for label in labels]\n    ax.scatter(proj, y_scatter, c=colors, alpha=0.3, s=10)\n    \n    # Set y-axis limits to accommodate both KDE and scatter points\n    y_max = ax.get_ylim()[1]\n    ax.set_ylim(-0.1, y_max * 1.1)  # Extend y-axis slightly above the maximum KDE value\n\n     # Calculate AUROC\n    auroc = calculate_auroc(acts, labels, t)\n    \n    # Display AUROC in the top left corner\n    ax.text(0.05, 0.95, f'AUROC: {auroc:.2f}', transform=ax.transAxes, \n            verticalalignment='top', fontsize=14, bbox=dict(facecolor='white', alpha=0.7))\n    \n    # Set labels and remove ticks\n    ax.set_ylabel('Frequency', fontsize=19)\n    ax.set_xlabel(xlabel, fontsize=19)\n    ax.set_yticks([])\n    ax.set_xticks([])\n\n    # Add a light grid\n    ax.grid(True, linestyle='--', alpha=0.3)\n\ndef calculate_auroc(acts, labels, t):\n    if isinstance(acts, torch.Tensor):\n        acts = acts.detach().cpu().numpy()\n    if isinstance(labels, torch.Tensor):\n        labels = labels.detach().cpu().numpy()\n    if isinstance(t, torch.Tensor):\n        t = t.detach().cpu().numpy()\n    \n    proj = t @ acts.T\n    auroc = roc_auc_score(labels, proj)\n    return auroc","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T06:38:33.317885Z","iopub.execute_input":"2025-07-02T06:38:33.318092Z","iopub.status.idle":"2025-07-02T06:38:33.493059Z","shell.execute_reply.started":"2025-07-02T06:38:33.318078Z","shell.execute_reply":"2025-07-02T06:38:33.492313Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Figure 1","metadata":{}},{"cell_type":"code","source":"fig = plt.figure(figsize=(11.5, 11))\naxes = [\n    fig.add_axes([0.4, 0.4, 0.26, 0.26]),  # ax1: top center\n    fig.add_axes([0.7, 0.4, 0.26, 0.26]),  # ax2: top right\n    fig.add_axes([0.1, 0.4, 0.26, 0.26]),  # ax3: top left\n    fig.add_axes([0.58, 0.1, 0.25, 0.25]), # ax4: bottom right\n    fig.add_axes([0.23, 0.1, 0.25, 0.25])  # ax5: bottom left\n]\n\n# Collect activations and labels of affirmative and negated statements separately\nacts_affirm, labels_affirm, acts_neg, labels_neg = collect_affirm_neg_data(train_sets, train_set_sizes,\n                                                                            model_family, model_size, model_type, layer)\n\n# Compute t_affirm\nt_affirm = compute_t_affirm(acts_affirm, labels_affirm)\n\n# orthonormalise t_g and t_p\nt_g_orthonormal, t_p_orthonormal = compute_orthonormal_vectors(t_g, t_p)\n\n# Project activations\nproj_g_affirm, proj_p_affirm = project_activations(acts_affirm, t_g_orthonormal, t_p_orthonormal)\nproj_g_neg, proj_p_neg = project_activations(acts_neg, t_g_orthonormal, t_p_orthonormal)\n\n# Plot scatter plots\nplot_scatter(axes[0], proj_g_affirm, proj_p_affirm, labels_affirm,\n              proj_g_neg, proj_p_neg, labels_neg, 'Affirmative Statements', plot_t_a=True,\n                t_affirm=t_affirm, t_g_orthonormal=t_g_orthonormal, t_p_orthonormal=t_p_orthonormal)\nplot_scatter(axes[1], proj_g_neg, proj_p_neg, labels_neg, proj_g_affirm,\n              proj_p_affirm, labels_affirm, 'Negated Statements')\nplot_scatter(axes[2], proj_g_affirm, proj_p_affirm, labels_affirm,\n              proj_g_neg, proj_p_neg, labels_neg, 'Affirmative & Negated\\nStatements', plot_t_g_t_p=True,\n                t_g=t_g, t_p=t_p, t_g_orthonormal=t_g_orthonormal, t_p_orthonormal=t_p_orthonormal)\n\n# Add legend\nadd_legend(axes[2])\n\n# Plot density plots\nacts = torch.cat((acts_affirm, acts_neg), dim=0)\nlabels = torch.cat((labels_affirm, labels_neg))\nplot_density(axes[3], acts, labels, t_affirm, '$a^T t_A$')\nplot_density(axes[4], acts, labels, t_g, '$a^T t_G$')\n\nplt.show()\nprint(f\"layer {layer}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T06:38:33.494937Z","iopub.execute_input":"2025-07-02T06:38:33.495189Z","iopub.status.idle":"2025-07-02T06:38:34.59063Z","shell.execute_reply.started":"2025-07-02T06:38:33.495173Z","shell.execute_reply":"2025-07-02T06:38:34.589883Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Activation vectors projected onto $t_G$ and $t_P$ (reduced version of figure 1)","metadata":{}},{"cell_type":"code","source":"fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12, 6))\nacts_affirm, labels_affirm, acts_neg, labels_neg = collect_affirm_neg_data(train_sets, train_set_sizes, \n                                                                            model_family, model_size, \n                                                                            model_type, layer)\n\nfor i, (acts, labels) in enumerate([(acts_affirm, labels_affirm), (acts_neg, labels_neg)]):\n    prod_g, prod_p = project_activations(acts, t_g, t_p)\n    ax = axes[i]\n    if i==0:\n        ax.set_xlabel('$a_{ij}^T t_G$', fontsize=19)\n        ax.set_ylabel('$a_{ij}^T t_P$', fontsize=19)\n        ax.set_title('Affirmative Statements', fontsize=19)\n    else:\n        ax.set_title('Negated Statements', fontsize=19)\n\n    colors = ['red' if label == 0 else 'blue' for label in labels]\n    ax.scatter(prod_g, prod_p, c=colors, alpha=0.5, s=5)\n\n# Add the legend to the last subplot\nhandles = [plt.scatter([], [], c='red', label='False'),\n            plt.scatter([], [], c='blue', label='True')]\naxes[1].legend(handles=handles, fontsize=19)\n\nfig.suptitle('Projection of activations on $t_G$ and $t_P$', fontsize=19)\nplt.show()\nprint(f\"layer {layer}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T06:38:34.591338Z","iopub.execute_input":"2025-07-02T06:38:34.591536Z","iopub.status.idle":"2025-07-02T06:38:35.180912Z","shell.execute_reply.started":"2025-07-02T06:38:34.591521Z","shell.execute_reply":"2025-07-02T06:38:35.180172Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Projection of other datasets onto $t_G$ and $t_P$ - larger_than and smaller_than are shown as examples","metadata":{}},{"cell_type":"code","source":"fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12, 6))  \n\nacts_affirm, labels_affirm, acts_neg, labels_neg = collect_affirm_neg_data(train_sets, train_set_sizes, model_family,\n                                                                            model_size, model_type, layer)\n# Project activations on t_g and t_p\nproj_g_affirm, proj_p_affirm = project_activations(acts_affirm, t_g, t_p)\nproj_g_neg, proj_p_neg = project_activations(acts_neg, t_g, t_p)\n\n# Define colors and markers for each label\nlabel_to_color = {0: 'indigo', 1: 'orange'}\nlabel_to_marker = {0: 's', 1: '^'}  # s for square, ^ for triangle\n\nfor i, dataset_name in enumerate(['larger_than', 'smaller_than']):\n        ax = axes[i]\n        ax.set_title(dataset_name, fontsize=19)\n        for label in [0,1]:\n                idx = labels_affirm.numpy() == label\n                ax.scatter(proj_g_affirm[idx], proj_p_affirm[idx], c='grey', \n                        marker=label_to_marker[label], alpha=0.3, s=5) \n                idx = labels_neg.numpy() == label\n                ax.scatter(proj_g_neg[idx], proj_p_neg[idx], c='grey', \n                        marker=label_to_marker[label], alpha=0.3, s=5) \n        \n        dm = DataManager()\n        dm.add_dataset(dataset_name, model_family, model_size, model_type, layer, split=None, center=False, device='cpu')\n        acts, labels = dm.data[dataset_name]\n        prod_g, prod_p = project_activations(acts, t_g, t_p)\n        for label in [0,1]:\n                idx = labels.numpy() == label\n                ax.scatter(prod_g[idx], prod_p[idx], c=label_to_color[label], \n                        marker=label_to_marker[label], alpha=0.9, s=15)\n                if i==0:\n                        ax.set_xlabel('$a^T t_G$', fontsize=19)\n                        ax.set_ylabel('$a^T t_P$', fontsize=19)\n\nadd_legend(axes[0])\nfig.suptitle('Projection of activations on $t_G$ and $t_P$', fontsize=19)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T06:38:35.181729Z","iopub.execute_input":"2025-07-02T06:38:35.181924Z","iopub.status.idle":"2025-07-02T06:38:35.968522Z","shell.execute_reply.started":"2025-07-02T06:38:35.181908Z","shell.execute_reply":"2025-07-02T06:38:35.967771Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Dimensionality of Truth\n#### Are there more than two truth dimensions?","metadata":{}},{"cell_type":"markdown","source":"#### Fraction of truth related variance in activations explained by Principal Components","metadata":{}},{"cell_type":"code","source":"# Define the four different statement types and corresponding datasets\nstatement_types = [\"affirmative\", \"affirmative, negated\", \"affirmative, negated, conjunctions\", \"affirmative, affirmative German\",\n                    \"affirmative, affirmative German,\\nnegated, negated German\", \n                   \"affirmative, negated,\\nconjunctions, disjunctions\"]\ndatasets_pca_options = {\n    \"affirmative\": ['cities', 'sp_en_trans', 'inventors', 'animal_class', 'element_symb', 'facts'],\n    \n    \"affirmative, negated\": ['cities', 'sp_en_trans', 'inventors', 'animal_class', 'element_symb', 'facts',\n                             'neg_cities', 'neg_sp_en_trans', 'neg_inventors', 'neg_animal_class', 'neg_element_symb', 'neg_facts'],\n\n    \"affirmative, negated, conjunctions\": ['cities', 'sp_en_trans', 'inventors', 'animal_class', 'element_symb', 'facts',\n                             'neg_cities', 'neg_sp_en_trans', 'neg_inventors', 'neg_animal_class', 'neg_element_symb', 'neg_facts',\n                             'cities_conj', 'sp_en_trans_conj', 'inventors_conj', 'animal_class_conj', 'element_symb_conj', 'facts_conj'],\n    \n    \"affirmative, affirmative German\": ['cities', 'sp_en_trans', 'inventors', 'animal_class', 'element_symb', 'facts',\n                             'cities_de', 'sp_en_trans_de', 'inventors_de', 'animal_class_de', 'element_symb_de', 'facts_de',],\n\n    \"affirmative, affirmative German,\\nnegated, negated German\": ['cities', 'sp_en_trans', 'inventors', 'animal_class', 'element_symb', 'facts',\n                            'cities_de', 'sp_en_trans_de', 'inventors_de', 'animal_class_de', 'element_symb_de', 'facts_de',\n                             'neg_cities', 'neg_sp_en_trans', 'neg_inventors', 'neg_animal_class', 'neg_element_symb', 'neg_facts',\n                             'neg_cities_de', 'neg_sp_en_trans_de', 'neg_inventors_de', 'neg_animal_class_de', 'neg_element_symb_de', 'neg_facts_de'],\n\n    \"affirmative, negated,\\nconjunctions, disjunctions\": ['cities', 'sp_en_trans', 'inventors', 'animal_class', 'element_symb', 'facts',\n                             'neg_cities', 'neg_sp_en_trans', 'neg_inventors', 'neg_animal_class', 'neg_element_symb', 'neg_facts',\n                             'cities_conj', 'sp_en_trans_conj', 'inventors_conj', 'animal_class_conj', 'element_symb_conj', 'facts_conj',\n                             'cities_disj', 'sp_en_trans_disj', 'inventors_disj', 'animal_class_disj', 'element_symb_disj', 'facts_disj']\n}\n\ndef compute_subspace_angle(A, B):\n    # Normalize columns of A and B\n    A = A / torch.linalg.norm(A, dim=0)\n    B = B / torch.linalg.norm(B, dim=0)\n    \n    # Compute SVD of A^T * B\n    U, S, Vt = torch.linalg.svd(A.T @ B)\n    \n    # Compute principal angles\n    angles = torch.arccos(torch.clamp(S, -1, 1))\n    \n    return angles\n\n# Create the 2x2 plot\nfig, axs = plt.subplots(2, 3, figsize=(18, 10))\naxs = axs.flatten()\n\nfor i, statement_type in enumerate(statement_types):\n    datasets_pca = datasets_pca_options[statement_type]\n    directions = []\n\n    for train_set in datasets_pca:\n        dm = DataManager()\n        dm.add_dataset(train_set, model_family, model_size, model_type, layer, split=1.0, center=True, device='cpu')\n        train_acts, train_labels = dm.get('train')\n        true_acts = train_acts[train_labels.to(bool)]\n        false_acts = train_acts[~train_labels.to(bool)]\n        directions.append(torch.mean(true_acts, dim=0))\n        directions.append(torch.mean(false_acts, dim=0))\n\n    mean_acts = np.array([direction.numpy() for direction in directions])\n    pca = PCA(n_components=10)\n    pca.fit(mean_acts)\n    axs[i].scatter(np.arange(1, 11, 1), pca.explained_variance_ratio_, s=90)\n    axs[i].set_title(f'{statement_type}', fontsize=26)\n    if i == 0 or i==3:\n        axs[i].set_ylabel('Explained variance', fontsize=27)\n    if i==3 or i == 4 or i==5:\n        axs[i].set_xlabel('PC index', fontsize=26)\n    axs[i].tick_params(axis='both', which='major', labelsize=20)\n    axs[i].grid(True)\n    if statement_type == \"affirmative, negated\":\n        # Compute subspace angle\n        A = torch.stack([t_g, t_p], dim=1)\n        B = torch.stack([torch.tensor(pca.components_[0, :]), torch.tensor(pca.components_[1, :])], dim=1)\n        angles = compute_subspace_angle(A, B)\n        print(f\"Principal angles between subspaces (in radians): {angles}\")\n        print(f\"Principal angles between subspaces (in degrees): {torch.rad2deg(angles)}\")\n        print(\"Cosine similarity between t_G and first PC: \" + str(torch.tensor(pca.components_[0, :])/torch.linalg.norm(torch.tensor(pca.components_[0, :])) @ t_g/np.linalg.norm(t_g)))\n        print(\"Cosine similarity between t_P and second PC: \" + str(torch.tensor(pca.components_[1, :])/torch.linalg.norm(torch.tensor(pca.components_[1, :])) @ t_p/np.linalg.norm(t_p)))\n\nfig.suptitle('Fraction of variance in centered and averaged\\n activations explained by PCs', fontsize=28)\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T06:38:35.969288Z","iopub.execute_input":"2025-07-02T06:38:35.969525Z","iopub.status.idle":"2025-07-02T06:38:38.450491Z","shell.execute_reply.started":"2025-07-02T06:38:35.969503Z","shell.execute_reply":"2025-07-02T06:38:38.449817Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_sets_subset = [['cities'], ['cities', 'neg_cities'], ['cities', 'neg_cities', 'cities_conj'],\n                     ['cities', 'neg_cities', 'cities_conj', 'cities_disj']]\n\nval_sets_subset = ['cities', 'neg_cities', 'facts', 'neg_facts',\n                   'facts_conj', 'facts_disj']\n\nnum_runs = 10\nproject_options = [None, 't_G_t_P']\n\n# Helper function to create unique keys for training sets\ndef get_train_set_key(train_set):\n    return '_'.join(train_set)\n\n# Initialize dictionaries to store accuracies for each projection option\nall_aurocs_options = {proj: {get_train_set_key(train_set): {val_set: [] for val_set in val_sets_subset} for train_set in train_sets_subset} for proj in project_options}\n\nfor project_out in project_options:\n    all_aurocs = all_aurocs_options[project_out]\n\n    for run in range(num_runs):\n        # Compute t_g and t_p using all data\n        acts_centered, _, labels, polarities = collect_training_data(train_sets, train_set_sizes, model_family, \n                                                            model_size, model_type, layer)\n        t_g, t_p = learn_truth_directions(acts_centered, labels, polarities)\n        # orthonormalize t_g and t_p\n        t_G_orthonormal, t_P_orthonormal = compute_orthonormal_vectors(t_g, t_p)\n\n        for train_set in train_sets_subset:\n            train_set_key = get_train_set_key(train_set)\n            \n            # set up data\n            dm = DataManager()\n            for subset in train_set:\n                dm.add_dataset(subset, model_family, model_size, model_type, layer, split=0.8, center=True, device='cpu')\n            train_acts, train_labels = dm.get('train')\n            if project_out == None:\n                pass\n            elif project_out == 't_G_t_P':\n                train_acts = train_acts - (train_acts @ t_G_orthonormal)[:, None] * t_G_orthonormal - (train_acts @ t_P_orthonormal)[:, None] * t_P_orthonormal\n            polarities = torch.zeros((train_labels.shape)[0])\n            # learn t_G\n            t_g_trained, _ = learn_truth_directions(train_acts, train_labels, polarities)\n            \n            # compute auroc of a^T t_G on validation sets\n            for val_set in val_sets_subset:\n                if val_set in train_set:\n                    acts, labels = dm.get('val')\n                else:\n                    dm.add_dataset(val_set, model_family, model_size, model_type, layer, split=None, center=False, device='cpu')\n                    acts, labels = dm.data[val_set]\n                \n                proj_g = acts @ t_g_trained\n                auroc = roc_auc_score(labels.numpy(), proj_g.numpy())\n                all_aurocs[train_set_key][val_set].append(auroc)\n\n    # Calculate mean and standard deviation for each training-validation set combination\n    mean_aurocs = {train_set: {val_set: np.mean(accs) for val_set, accs in val_sets.items()} for train_set, val_sets in all_aurocs.items()}\n    std_aurocs = {train_set: {val_set: np.std(accs) for val_set, accs in val_sets.items()} for train_set, val_sets in all_aurocs.items()}\n\n    all_aurocs_options[project_out] = {'mean': mean_aurocs, 'std': std_aurocs}\n    print(mean_aurocs, std_aurocs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T06:38:38.451288Z","iopub.execute_input":"2025-07-02T06:38:38.451534Z","iopub.status.idle":"2025-07-02T06:38:55.075344Z","shell.execute_reply.started":"2025-07-02T06:38:38.451516Z","shell.execute_reply":"2025-07-02T06:38:55.074566Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Generalisation accuracies of truth directions trained on different data","metadata":{}},{"cell_type":"code","source":"# Plotting the results\nfig, axes = plt.subplots(figsize=(16, 6.7), nrows=1, ncols=2, sharey=True)\n# Titles for the x and y axes\ntitles_val = ['cities', 'neg_cities', 'facts', 'neg_facts',\n                'facts_conj', 'facts_disj']\ntitles_train = ['cities', '+ neg_cities', '+ cities_conj', '+ cities_disj']\n\n# Create a custom colormap from red to yellow\ncolors = [(1, 0, 0), (1, 1, 0)]  # Red to Yellow\nn_bins = 100  # Discretizes the interpolation into bins\ncmap_name = 'red_yellow'\ncmap = LinearSegmentedColormap.from_list(cmap_name, colors, N=n_bins)\n\nfor idx, project_out in enumerate(project_options):\n    mean_aurocs = all_aurocs_options[project_out]['mean']\n    std_aurocs = all_aurocs_options[project_out]['std']\n    \n    # Prepare the grid for mean accuracies\n    grid = np.zeros((len(train_sets_subset), len(val_sets_subset)))\n\n    # Populate the grid with mean accuracies\n    for i, train_set in enumerate(train_sets_subset):\n        train_set_key = get_train_set_key(train_set)\n        for j, val_set in enumerate(val_sets_subset):\n            grid[i, j] = mean_aurocs[train_set_key][val_set]\n\n    # Plot the grid\n    im = axes[idx].imshow(grid.T, vmin=0, vmax=1, cmap=cmap, aspect='auto')\n\n    # Annotate each cell with the mean accuracy and standard deviation\n    for i in range(len(grid)):\n        for j in range(len(grid[0])):\n            mean_auroc = grid[i][j]\n            std_auroc = std_aurocs[get_train_set_key(train_sets_subset[i])][val_sets_subset[j]]\n            axes[idx].text(i, j, f'{mean_auroc:.2f}', ha='center', va='center', fontsize=16) #Â±{std_auroc:.2f}\n\n\n    # Titles for the x and y axes\n    axes[idx].set_yticks(range(len(val_sets_subset)))\n    axes[idx].set_xticks(range(len(train_sets_subset)))\n    axes[idx].set_yticklabels([val_title for val_title in titles_val], fontsize=17)\n    axes[idx].set_xticklabels([train_title for train_title in titles_train], rotation=45, ha='right', fontsize=17)\n\n    # Set title and labels for the subplot\n    if idx == 0:\n        axes[idx].set_title(f'Projected out: None', fontsize=20)\n    if idx == 1:\n        axes[idx].set_title(f'Projected out: $t_G$ and $t_P$', fontsize=20)\n    if idx == 0:\n        axes[idx].set_ylabel('Test Set', fontsize=20)\n        axes[idx].set_xlabel('Train Set \"cities\"', fontsize=20)\n\n# Add colorbar to the last subplot\ncbar = fig.colorbar(im, ax=axes.ravel().tolist(), shrink=0.8)\ncbar.ax.tick_params(labelsize=16)\nfig.suptitle('AUROC for Projections $a^T t$', fontsize=20, x=0.42)\n\n# Show the plot\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T06:38:55.076245Z","iopub.execute_input":"2025-07-02T06:38:55.076512Z","iopub.status.idle":"2025-07-02T06:38:55.447713Z","shell.execute_reply.started":"2025-07-02T06:38:55.076494Z","shell.execute_reply":"2025-07-02T06:38:55.44695Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Cross-dataset generalization matrix","metadata":{}},{"cell_type":"code","source":"train_sets_subset = [['cities'], ['neg_cities'], ['cities', 'neg_cities'], ['cities_conj'], ['cities_disj']]\n\nval_sets_subset = ['cities', 'neg_cities', 'facts', 'neg_facts',\n                   'facts_conj', 'facts_disj']\n\nnum_runs = 10\n\n# Helper function to create unique keys for training sets\ndef get_train_set_key(train_set):\n    return '_'.join(train_set)\n\n# Initialize dictionaries to store accuracies for each projection option\nall_aurocs = {get_train_set_key(train_set): {val_set: [] for val_set in val_sets_subset} for train_set in train_sets_subset}\nfor run in range(num_runs):\n    for train_set in train_sets_subset:\n        train_set_key = get_train_set_key(train_set)\n        \n        # set up data\n        dm = DataManager()\n        for subset in train_set:\n            dm.add_dataset(subset, model_family, model_size, model_type, layer, split=0.8, center=True, device='cpu')\n        train_acts, train_labels = dm.get('train')\n        polarities = torch.zeros((train_labels.shape)[0])\n        # learn t_G\n        t_g_trained, _ = learn_truth_directions(train_acts, train_labels, polarities)\n        \n        # compute auroc of a^T t_G on validation sets\n        for val_set in val_sets_subset:\n            if val_set in train_set:\n                acts, labels = dm.get('val')\n            else:\n                dm.add_dataset(val_set, model_family, model_size, model_type, layer, split=None, center=False, device='cpu')\n                acts, labels = dm.data[val_set]\n            \n            proj_g = acts @ t_g_trained\n            auroc = roc_auc_score(labels.numpy(), proj_g.numpy())\n            all_aurocs[train_set_key][val_set].append(auroc)\n\n# Calculate mean and standard deviation for each training-validation set combination\nmean_aurocs = {train_set: {val_set: np.mean(accs) for val_set, accs in val_sets.items()} for train_set, val_sets in all_aurocs.items()}\nstd_aurocs = {train_set: {val_set: np.std(accs) for val_set, accs in val_sets.items()} for train_set, val_sets in all_aurocs.items()}\n\nall_aurocs = {'mean': mean_aurocs, 'std': std_aurocs}\nprint(mean_aurocs, std_aurocs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T06:38:55.449534Z","iopub.execute_input":"2025-07-02T06:38:55.449746Z","iopub.status.idle":"2025-07-02T06:39:02.073051Z","shell.execute_reply.started":"2025-07-02T06:38:55.44973Z","shell.execute_reply":"2025-07-02T06:39:02.072363Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plotting the results\nfig, ax = plt.subplots(figsize=(7, 5.5), nrows=1, ncols=1, sharey=True)\n# Titles for the x and y axes\ntitles_val = ['cities', 'neg_cities', 'facts', 'neg_facts',\n                'facts_conj', 'facts_disj']\ntitles_train = ['cities', 'neg_cities', 'cities+neg_cities', 'cities_conj', 'cities_disj']\n\n# Create a custom colormap from red to yellow\ncolors = [(1, 0, 0), (1, 1, 0)]  # Red to Yellow\nn_bins = 100  # Discretizes the interpolation into bins\ncmap_name = 'red_yellow'\ncmap = LinearSegmentedColormap.from_list(cmap_name, colors, N=n_bins)\n\nmean_aurocs = all_aurocs['mean']\nstd_aurocs = all_aurocs['std']\n\n# Prepare the grid for mean accuracies\ngrid = np.zeros((len(train_sets_subset), len(val_sets_subset)))\n\n# Populate the grid with mean accuracies\nfor i, train_set in enumerate(train_sets_subset):\n    train_set_key = get_train_set_key(train_set)\n    for j, val_set in enumerate(val_sets_subset):\n        grid[i, j] = mean_aurocs[train_set_key][val_set]\n\n# Plot the grid\nim = ax.imshow(grid.T, vmin=0, vmax=1, cmap=cmap, aspect='auto')\n\n# Annotate each cell with the mean accuracy and standard deviation\nfor i in range(len(grid)):\n    for j in range(len(grid[0])):\n        mean_auroc = grid[i][j]\n        std_auroc = std_aurocs[get_train_set_key(train_sets_subset[i])][val_sets_subset[j]]\n        ax.text(i, j, f'{mean_auroc:.2f}', ha='center', va='center', fontsize=16) #Â±{std_auroc:.2f}\n\n\n    # Titles for the x and y axes\n    ax.set_yticks(range(len(val_sets_subset)))\n    ax.set_xticks(range(len(train_sets_subset)))\n    ax.set_yticklabels([val_title for val_title in titles_val], fontsize=17)\n    ax.set_xticklabels([train_title for train_title in titles_train], rotation=45, ha='right', fontsize=17)\n\n    ax.set_ylabel('Test Set', fontsize=20)\n    ax.set_xlabel('Train Set', fontsize=20)\n\n# Add colorbar to the last subplot\ncbar = fig.colorbar(im, shrink=0.8)\ncbar.ax.tick_params(labelsize=16)\nfig.suptitle('AUROC for Projections $a^T t$', fontsize=20, x=0.42)\n\n# Show the plot\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T06:39:02.073789Z","iopub.execute_input":"2025-07-02T06:39:02.07411Z","iopub.status.idle":"2025-07-02T06:39:02.320708Z","shell.execute_reply.started":"2025-07-02T06:39:02.074086Z","shell.execute_reply":"2025-07-02T06:39:02.320026Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data = np.zeros((100,2))\n# zeroth feature is perfectly predictive of label\ndata[0:50,0] = 1.0\n# first feature is correlated with the correct label but not perfectly, 20 correct, 5 incorrect\ndata[0:20, 1] = 1.0\ndata[50:55, 1] = 1.0\nlabels = np.concatenate((np.ones(50), np.zeros(50)))\n# which method can disentangle feature 0 from feature 1?\n# mass mean\nd_mm = np.mean(data[labels == 1.0], axis=0) - np.mean(data[labels==0.0], axis=0)\nprint(d_mm)\n# LR\nLR = LogisticRegression(penalty=None, fit_intercept = True)\nLR.fit(data, labels)\nd_LR = LR.coef_\nprint(LR.intercept_)\nprint(d_LR) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T06:39:02.32156Z","iopub.execute_input":"2025-07-02T06:39:02.32187Z","iopub.status.idle":"2025-07-02T06:39:02.335162Z","shell.execute_reply.started":"2025-07-02T06:39:02.321853Z","shell.execute_reply":"2025-07-02T06:39:02.334514Z"}},"outputs":[],"execution_count":null}]}