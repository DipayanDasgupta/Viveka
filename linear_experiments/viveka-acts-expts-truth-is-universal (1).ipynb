{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.11"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12336928,"sourceType":"datasetVersion","datasetId":7777053},{"sourceId":12340085,"sourceType":"datasetVersion","datasetId":7779271}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!git clone https://github.com/vsriramv/Truth_is_Universal.git","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2025-07-11T04:00:54.884542Z","iopub.execute_input":"2025-07-11T04:00:54.884715Z","iopub.status.idle":"2025-07-11T04:00:56.305638Z","shell.execute_reply.started":"2025-07-11T04:00:54.884698Z","shell.execute_reply":"2025-07-11T04:00:56.304670Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Cloning into 'Truth_is_Universal'...\nremote: Enumerating objects: 182, done.\u001b[K\nremote: Counting objects: 100% (66/66), done.\u001b[K\nremote: Compressing objects: 100% (49/49), done.\u001b[K\nremote: Total 182 (delta 40), reused 17 (delta 17), pack-reused 116 (from 1)\u001b[K\nReceiving objects: 100% (182/182), 7.59 MiB | 23.47 MiB/s, done.\nResolving deltas: 100% (74/74), done.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import torch\nfrom transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\nfrom huggingface_hub import login\n\nHF_TOKEN = \"<add yours>\"\n\nlogin(token=HF_TOKEN)\n\nmodel_name = \"google/gemma-2-2b-it\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_name, token=HF_TOKEN)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=torch.bfloat16,\n    device_map=\"auto\",\n    token=HF_TOKEN\n)\n\ndef format_prompt(user_message: str) -> str:\n    return f\"<start_of_turn>user\\n{user_message}<end_of_turn>\\n<start_of_turn>model\\n\"\n\npipe = pipeline(\n    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer\n)","metadata":{"execution":{"iopub.status.busy":"2025-07-11T04:04:01.734410Z","iopub.execute_input":"2025-07-11T04:04:01.735356Z","iopub.status.idle":"2025-07-11T04:05:06.354578Z","shell.execute_reply.started":"2025-07-11T04:04:01.735316Z","shell.execute_reply":"2025-07-11T04:05:06.353988Z"},"trusted":true},"outputs":[{"name":"stderr","text":"2025-07-11 04:04:12.749254: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1752206652.983938      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1752206653.051574      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/47.0k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bd015fddb0bf4d4e9fc322f6a43c713e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5898e6c09ae74c9ba5681c60ea5db99e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e4579f97afc443b79bbe59ffa556517a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c91144cb149641aaa99e55a48f570c84"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/838 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a2cda8296e104afabaca83d318c1f2e3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/24.2k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5f6a6c036db54f14a4743d22dc26d141"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c8249bd5364140038d18937567d34e9d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/4.99G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f66d1c1fa9f949bba024e1d44d99111a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/241M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f539745a88ff4a95a0de9a53542ff31b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9bc6f7becea543dc9c6d5165cae12fc5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/187 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0e998c3a1d964c298b4fc1c7288129fe"}},"metadata":{}},{"name":"stderr","text":"Device set to use cuda:0\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"prompt = format_prompt(\"statement: Nothing is everything. True or False? Answer:\")\noutputs = pipe(prompt, max_new_tokens=256, do_sample=True, temperature=0.7\n              )\nresponse = outputs[0][\"generated_text\"].split(\"<start_of_turn>model\\n\")[-1].strip()\nprint(response)","metadata":{"execution":{"iopub.status.busy":"2025-07-11T04:05:16.866543Z","iopub.execute_input":"2025-07-11T04:05:16.867123Z","iopub.status.idle":"2025-07-11T04:05:31.301040Z","shell.execute_reply.started":"2025-07-11T04:05:16.867099Z","shell.execute_reply":"2025-07-11T04:05:31.300423Z"},"trusted":true},"outputs":[{"name":"stdout","text":"The statement \"Nothing is everything\" is **philosophically complex and open to debate**. Here's why:\n\n**Arguments for \"True\":**\n\n* **Existentialism:**  Some existentialist philosophers believe existence precedes essence, meaning we are born into a state of \"nothingness\" and define ourselves through our actions and choices.\n* **Void and Ultimate Reality:** Certain spiritual or religious traditions see \"nothing\" as the ultimate reality, a source from which everything emanates. In Eastern philosophies, for example, \"void\" (shunyata) is a fundamental concept.\n* **Potentialism:**  This philosophical perspective suggests that everything has the potential for existence.  Therefore, \"nothing\" could be seen as the ultimate potential.\n\n**Arguments against \"True\":**\n\n* **The Nature of \"Nothing\":** \"Nothing\" is a concept we grapple with. It's difficult to define, and even if we could, it wouldn't necessarily imply everything.\n* **Subjective Reality:** What we perceive as \"nothing\" might be someone else's \"something\" based on their cultural, emotional, and personal understanding. \n* **The Problem of Definition:**  To prove \"nothing is everything\" we'd need\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"!pip install -q huggingface_hub git-lfs\n\nfrom huggingface_hub import snapshot_download\n\nsnapshot_download(\n    repo_id=\"google/gemma-2-2b-it\",\n    local_dir=\"./gemma-2-2b-it\",\n    token=\"<add yours>\",  \n    local_dir_use_symlinks=False\n)","metadata":{"execution":{"iopub.status.busy":"2025-07-11T04:10:55.592851Z","iopub.execute_input":"2025-07-11T04:10:55.593553Z","iopub.status.idle":"2025-07-11T04:11:05.907515Z","shell.execute_reply.started":"2025-07-11T04:10:55.593524Z","shell.execute_reply":"2025-07-11T04:11:05.906544Z"},"trusted":true},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:980: UserWarning: `local_dir_use_symlinks` parameter is deprecated and will be ignored. The process to download files to a local folder has been updated and do not rely on symlinks anymore. You only need to pass a destination folder as`local_dir`.\nFor more details, check out https://huggingface.co/docs/huggingface_hub/main/en/guides/download#download-files-to-local-folder.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Fetching 11 files:   0%|          | 0/11 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3e5480df17e940fe9dee09868b86b46c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":".gitattributes:   0%|          | 0.00/1.57k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2b326b777ded4f3a9170e9a4f873e103"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/29.1k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b637b9dfb47f42d4b98cc22463d6ac2f"}},"metadata":{}},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"'/kaggle/working/gemma-2-2b-it'"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"%cd /kaggle/working/Truth_is_Universal","metadata":{"execution":{"iopub.status.busy":"2025-07-11T04:11:09.463912Z","iopub.execute_input":"2025-07-11T04:11:09.464537Z","iopub.status.idle":"2025-07-11T04:11:09.472478Z","shell.execute_reply.started":"2025-07-11T04:11:09.464504Z","shell.execute_reply":"2025-07-11T04:11:09.471660Z"},"trusted":true},"outputs":[{"name":"stdout","text":"/kaggle/working/Truth_is_Universal\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"with open(\"/kaggle/working/Truth_is_Universal/config.ini\",\"w\") as f:\n    f.write(\"\"\"[Gemma2]\nweights_directory = /kaggle/working/\n2B_chat_subdir = gemma-2-2b-it\"\"\")","metadata":{"execution":{"iopub.status.busy":"2025-07-11T04:11:12.396427Z","iopub.execute_input":"2025-07-11T04:11:12.397275Z","iopub.status.idle":"2025-07-11T04:11:14.769571Z","shell.execute_reply.started":"2025-07-11T04:11:12.397246Z","shell.execute_reply":"2025-07-11T04:11:14.768420Z"},"trusted":true},"outputs":[],"execution_count":6},{"cell_type":"code","source":"%cat config.ini","metadata":{"execution":{"iopub.status.busy":"2025-07-11T04:11:14.770898Z","iopub.execute_input":"2025-07-11T04:11:14.771219Z","iopub.status.idle":"2025-07-11T04:11:15.954384Z","shell.execute_reply.started":"2025-07-11T04:11:14.771191Z","shell.execute_reply":"2025-07-11T04:11:15.953355Z"},"trusted":true},"outputs":[{"name":"stdout","text":"[Gemma2]\nweights_directory = /kaggle/working/\n2B_chat_subdir = gemma-2-2b-it","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"#!grep -A 5 \"class Hook\" /kaggle/working/Truth_is_Universal/utils.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T10:42:06.425771Z","iopub.execute_input":"2025-07-02T10:42:06.426077Z","iopub.status.idle":"2025-07-02T10:42:06.588171Z","shell.execute_reply.started":"2025-07-02T10:42:06.426049Z","shell.execute_reply":"2025-07-02T10:42:06.587228Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%cd /kaggle/working/Truth_is_Universal","metadata":{"execution":{"iopub.status.busy":"2025-07-11T04:11:19.010566Z","iopub.execute_input":"2025-07-11T04:11:19.011494Z","iopub.status.idle":"2025-07-11T04:11:19.016356Z","shell.execute_reply.started":"2025-07-11T04:11:19.011455Z","shell.execute_reply":"2025-07-11T04:11:19.015638Z"},"trusted":true},"outputs":[{"name":"stdout","text":"/kaggle/working/Truth_is_Universal\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"#!python generate_attn_acts.py --model_family Gemma2 --model_size 2B --model_type chat --layers -1 --datasets all_topic_specific --device cuda:0\n","metadata":{"execution":{"iopub.status.busy":"2025-07-03T06:32:07.466144Z","iopub.execute_input":"2025-07-03T06:32:07.466515Z","iopub.status.idle":"2025-07-03T06:34:24.944037Z","shell.execute_reply.started":"2025-07-03T06:32:07.46649Z","shell.execute_reply":"2025-07-03T06:34:24.94302Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#!zip -r /kaggle/working/activations_52_layer_new_prompt.zip /kaggle/working/Truth_is_Universal/acts","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%cp -r /kaggle/input/activations-zip/kaggle/working/Truth_is_Universal/acts /kaggle/working/Truth_is_Universal/acts","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T04:11:23.016346Z","iopub.execute_input":"2025-07-11T04:11:23.016626Z","iopub.status.idle":"2025-07-11T04:12:58.969582Z","shell.execute_reply.started":"2025-07-11T04:11:23.016606Z","shell.execute_reply":"2025-07-11T04:12:58.968740Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"#%cp -r /kaggle/input/activations-new-pf/kaggle/working/Truth_is_Universal/acts /kaggle/working/Truth_is_Universal/acts","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T17:39:50.323531Z","iopub.execute_input":"2025-07-01T17:39:50.324133Z","iopub.status.idle":"2025-07-01T17:42:42.590496Z","shell.execute_reply.started":"2025-07-01T17:39:50.32411Z","shell.execute_reply":"2025-07-01T17:42:42.589652Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#%rm -rf /kaggle/working/Truth_is_Universal/acts","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T11:37:10.757299Z","iopub.execute_input":"2025-07-01T11:37:10.758004Z","iopub.status.idle":"2025-07-01T11:37:12.199113Z","shell.execute_reply.started":"2025-07-01T11:37:10.757975Z","shell.execute_reply":"2025-07-01T11:37:12.198048Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"'''import os\n\n# Paste your actual username and key here:\nos.environ[\"KAGGLE_USERNAME\"] = \"sharankeshavs\"\nos.environ[\"KAGGLE_KEY\"] = \"22e318c6eb8d6363c54d6a874d089340\"''' #please dont steal this or the hf key:(","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T06:52:57.115798Z","iopub.execute_input":"2025-07-01T06:52:57.116689Z","iopub.status.idle":"2025-07-01T06:52:57.121099Z","shell.execute_reply.started":"2025-07-01T06:52:57.116658Z","shell.execute_reply":"2025-07-01T06:52:57.120121Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#!kaggle kernels output sharankeshavs/viveka-truth-is-universal-trial2/ -p /kaggle/working/","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-27T10:07:12.92902Z","iopub.execute_input":"2025-06-27T10:07:12.929935Z","iopub.status.idle":"2025-06-27T10:07:50.946418Z","shell.execute_reply.started":"2025-06-27T10:07:12.929898Z","shell.execute_reply":"2025-06-27T10:07:50.945547Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":" #%ls","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-27T18:59:20.585301Z","iopub.execute_input":"2025-06-27T18:59:20.586113Z","iopub.status.idle":"2025-06-27T18:59:20.706441Z","shell.execute_reply.started":"2025-06-27T18:59:20.586079Z","shell.execute_reply":"2025-06-27T18:59:20.705306Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"statement = \"The city of Sulaymaniyah is in Turkey.\"\nprompt    = f\"{statement} Is this statement T/F?\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T04:17:36.427540Z","iopub.execute_input":"2025-07-11T04:17:36.428295Z","iopub.status.idle":"2025-07-11T04:17:36.431989Z","shell.execute_reply.started":"2025-07-11T04:17:36.428263Z","shell.execute_reply":"2025-07-11T04:17:36.431333Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"from IPython.display import Markdown, display","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T04:17:38.228713Z","iopub.execute_input":"2025-07-11T04:17:38.229226Z","iopub.status.idle":"2025-07-11T04:17:38.232779Z","shell.execute_reply.started":"2025-07-11T04:17:38.229199Z","shell.execute_reply":"2025-07-11T04:17:38.232142Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"import torch\nimport pandas as pd\n\n# ── ASSUME model & tokenizer are already defined and on the correct device ─────\ndevice = next(model.parameters()).device\n\n# ── PARAMETERS ─────────────────────────────────────────────────────────────────\n#statement = \"The city of Krasnodar is not in Russia.\"\n#statement = \"What is donald trump's grandson's birthday?\"\nprompt = (\n    \"<start_of_turn>user\\n\"\n    f\"{statement}\\n\"\n    \"<end_of_turn>\\n\"\n    \"<start_of_turn>model\\n\"\n)\nmax_steps = 50  # maximum tokens to generate\n\n# ── TOKENIZE PROMPT ─────────────────────────────────────────────────────────────\nenc = tokenizer(prompt, return_tensors=\"pt\").to(device)\ninput_ids = enc.input_ids\nattention = enc.attention_mask\n\n# ── PREPARE UNEMBED + OPTIONAL BIAS ─────────────────────────────────────────────\nunembed = model.lm_head.weight.data.cpu()  # (vocab_size, d_model)\nbias_vec = (\n    model.lm_head.bias.data.cpu()\n    if model.lm_head.bias is not None\n    else torch.zeros(unembed.size(0))\n)\n\n# ── DETERMINE LAYER COUNT ───────────────────────────────────────────────────────\nnum_layers = (\n    len(model.config.hidden_sizes) + 1\n    if hasattr(model.config, \"hidden_sizes\")\n    else model.config.num_hidden_layers + 1\n)\nlayer_headers = [f\"L{L}\" for L in range(num_layers)]\n\n# ── GENERATION + RECORDING ─────────────────────────────────────────────────────\nrecords = []\ngenerated_tokens = []\n\nfor step in range(max_steps):\n    out = model(\n        input_ids=input_ids,\n        attention_mask=attention,\n        output_hidden_states=True,\n        use_cache=False\n    )\n\n    # Get greedy token prediction at each layer\n    layer_tokens = []\n    for h in out.hidden_states:\n        h_last = h[0, -1, :].cpu()  # (d_model,)\n        logits = (h_last @ unembed.T) + bias_vec  # (vocab_size,)\n        top_id = logits.argmax().item()\n        layer_tokens.append(tokenizer.decode([top_id]))\n\n    # Model's actual greedy output\n    full_logits = out.logits[0, -1, :].cpu()\n    next_id = full_logits.argmax().item()\n    next_tok = tokenizer.decode([next_id])\n\n    # Record step\n    row = {\"step\": step, \"token\": next_tok}\n    row.update({layer_headers[i]: layer_tokens[i] for i in range(num_layers)})\n    records.append(row)\n\n    # Accumulate generated tokens\n    generated_tokens.append(next_tok)\n\n    # Stop if EOS token is generated\n    if next_id == tokenizer.eos_token_id:\n        break\n\n    # Update input for next step\n    new_id = torch.tensor([[next_id]], device=device)\n    input_ids = torch.cat([input_ids, new_id], dim=1)\n    attention = torch.cat([attention, torch.ones_like(new_id)], dim=1)\n\n# ── FINAL OUTPUT ────────────────────────────────────────────────────────────────\ndf = pd.DataFrame(records)\nfull_answer = \"\".join(generated_tokens).strip()\n\n# Output\nprint(df)\nprint(\"\\nFull generated answer:\\n\", Markdown(full_answer))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T04:17:59.810260Z","iopub.execute_input":"2025-07-11T04:17:59.810986Z","iopub.status.idle":"2025-07-11T04:18:54.888841Z","shell.execute_reply.started":"2025-07-11T04:17:59.810960Z","shell.execute_reply":"2025-07-11T04:18:54.888045Z"}},"outputs":[{"name":"stdout","text":"    step          token          L0          L1          L2       L3       L4  \\\n0      0           That          \\n          \\n       <bos>    <bos>    <bos>   \n1      1      statement        That        That       <bos>     That    <bos>   \n2      2             is   statement   statement   statement    <bos>    <bos>   \n3      3      incorrect          is          is       <bos>       is    <bos>   \n4      4              .   incorrect   incorrect       <bos>    <bos>    <bos>   \n5      5                          .           .       <bos>    <bos>    <bos>   \n6      6           \\n\\n                                                 <bos>   \n7      7             Su        \\n\\n        \\n\\n       <bos>    <bos>    <bos>   \n8      8            lay          Su          Su          Su       Su    <bos>   \n9      9           mani         lay         lay         lay      lay      lay   \n10    10            yah        mani        mani        mani     mani     mani   \n11    11             is         yah         yah         yah      yah    <bos>   \n12    12              a          is          is       <bos>    <bos>    <bos>   \n13    13           city           a           a       <bos>        a    <bos>   \n14    14             in        city        city       <bos>    <bos>    <bos>   \n15    15             **          in          in       <bos>       in    <bos>   \n16    16           Iraq          **          **       <bos>    <bos>    <bos>   \n17    17            **,      myſelf        Iraq        Iraq     Iraq     Iraq   \n18    18            not         **,       <bos>       <bos>    <bos>    <bos>   \n19    19         Turkey         not         not       <bos>      not    <bos>   \n20    20              .      Turkey      Turkey      Turkey   Turkey   Turkey   \n21    21                          .           .       <bos>    <bos>    <bos>   \n22    22             \\n                                                 <bos>   \n23    23  <end_of_turn>          \\n          \\n       <bos>    <bos>    <bos>   \n24    24          <eos>      myſelf      myſelf       <bos>   myſelf   itſelf   \n\n       L5          L6             L7  ...          L17            L18  \\\n0   <bos>       <bos>          <bos>  ...      purpoſe      incorrect   \n1    That       <bos>           That  ...    incorrect      incorrect   \n2   <bos>       <bos>      statement  ...       itſelf      incorrect   \n3      is       <bos>             is  ...    incorrect      incorrect   \n4   <bos>   incorrect      incorrect  ...     pleaſure    religieuses   \n5   <bos>       <bos>              .  ...      purpoſe       suivants   \n6     the       <bos>                 ...      purpoſe           \\n\\n   \n7   <bos>       <bos>           \\n\\n  ...      purpoſe       suivants   \n8   <bos>       <bos>             Su  ...     pleaſure        Majefty   \n9     lay       <bos>  RenderAtEndOf  ...     pleaſure         sorrow   \n10  <bos>       <bos>            Efq  ...      Majefty        Majefty   \n11  <bos>       <bos>            Efq  ...          Efq        belongs   \n12  <bos>       <bos>             is  ...     actually        located   \n13  <bos>       <bos>              a  ...         ^(@)           ^(@)   \n14  <bos>       <bos>           city  ...          Efq        located   \n15  <bos>       <bos>             in  ...          Efq     sanitaires   \n16  <bos>       <bos>       pleaſure  ...      purpoſe        purpoſe   \n17  <bos>        Iraq         itſelf  ...      Majefty        Majefty   \n18  <bos>       <bos>          <bos>  ...        yaitu   specifically   \n19    not       <bos>            not  ...       drawal    necessarily   \n20  <bos>       <bos>         Turkey  ...   sanitaires              .   \n21  <bos>       <bos>          <bos>  ...         \\n\\n           \\n\\n   \n22    the       <bos>                 ...         \\n\\n           \\n\\n   \n23  <bos>       <bos>          <bos>  ...         \\n\\n           \\n\\n   \n24  <bos>       <bos>       pleaſure  ...     pleaſure          <eos>   \n\n              L19            L20         L21         L22         L23  \\\n0       incorrect      incorrect   incorrect        That        that   \n1       incorrect      incorrect   incorrect   statement          is   \n2       incorrect      incorrect          is          is          is   \n3       incorrect      incorrect   incorrect   incorrect   incorrect   \n4               .              .           .           .           .   \n5            \\n\\n           \\n\\n        \\n\\n        \\n\\n        \\n\\n   \n6            \\n\\n           \\n\\n        \\n\\n        \\n\\n        \\n\\n   \n7             The            The         The         The         The   \n8         Majefty      maternity   passenger   maternity      family   \n9          sorrow        urinary      health        city        city   \n10          houſe       vicinity        city        city        city   \n11             is             is          is          is          is   \n12       actually        located    actually     located           a   \n13           city           city        city        city        city   \n14        located        located     located     located          in   \n15            the            the         the         the         the   \n16       pleaſure       northern    northern    northern    northern   \n17        Majefty        instead           .           ,           ,   \n18   specifically   specifically         not         not         not   \n19             in             in          in          in          in   \n20              .              .           .           .           .   \n21           \\n\\n           \\n\\n        \\n\\n          it               \n22           \\n\\n           \\n\\n        \\n\\n          \\n          \\n   \n23          <eos>           \\n\\n       <eos>          \\n          \\n   \n24          <eos>          <eos>       <eos>       <eos>       <eos>   \n\n           L24         L25            L26  \n0         that        This           That  \n1           is           '      statement  \n2           is          is             is  \n3    incorrect   incorrect      incorrect  \n4            .           .              .  \n5         \\n\\n                             \n6         \\n\\n        \\n\\n           \\n\\n  \n7          The         The             Su  \n8          lay         lay            lay  \n9         city           -           mani  \n10        city           ,            yah  \n11          is          is             is  \n12           a           a              a  \n13        city        city           city  \n14          in          in             in  \n15         the         the             **  \n16    northern         the           Iraq  \n17           ,           ,            **,  \n18         not         not            not  \n19          in          in         Turkey  \n20           .           .              .  \n21                                         \n22          \\n          \\n             \\n  \n23                          <end_of_turn>  \n24       <eos>       <eos>          <eos>  \n\n[25 rows x 29 columns]\n\nFull generated answer:\n <IPython.core.display.Markdown object>\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"print(f\"Full Answer  : \\n{full_answer}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T04:19:30.550121Z","iopub.execute_input":"2025-07-11T04:19:30.550432Z","iopub.status.idle":"2025-07-11T04:19:30.554483Z","shell.execute_reply.started":"2025-07-11T04:19:30.550408Z","shell.execute_reply":"2025-07-11T04:19:30.553811Z"}},"outputs":[{"name":"stdout","text":"Full Answer  : \nThat statement is incorrect. \n\nSulaymaniyah is a city in **Iraq**, not Turkey. \n<end_of_turn><eos>\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"#pd.set_option(\"display.max_rows\",None)\npd.set_option(\"display.max_columns\",None)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T04:20:02.900346Z","iopub.execute_input":"2025-07-11T04:20:02.901140Z","iopub.status.idle":"2025-07-11T04:20:02.904614Z","shell.execute_reply.started":"2025-07-11T04:20:02.901075Z","shell.execute_reply":"2025-07-11T04:20:02.904042Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T04:20:04.780643Z","iopub.execute_input":"2025-07-11T04:20:04.781436Z","iopub.status.idle":"2025-07-11T04:20:04.803208Z","shell.execute_reply.started":"2025-07-11T04:20:04.781409Z","shell.execute_reply":"2025-07-11T04:20:04.802393Z"}},"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"    step          token          L0          L1          L2       L3       L4  \\\n0      0           That          \\n          \\n       <bos>    <bos>    <bos>   \n1      1      statement        That        That       <bos>     That    <bos>   \n2      2             is   statement   statement   statement    <bos>    <bos>   \n3      3      incorrect          is          is       <bos>       is    <bos>   \n4      4              .   incorrect   incorrect       <bos>    <bos>    <bos>   \n5      5                          .           .       <bos>    <bos>    <bos>   \n6      6           \\n\\n                                                 <bos>   \n7      7             Su        \\n\\n        \\n\\n       <bos>    <bos>    <bos>   \n8      8            lay          Su          Su          Su       Su    <bos>   \n9      9           mani         lay         lay         lay      lay      lay   \n10    10            yah        mani        mani        mani     mani     mani   \n11    11             is         yah         yah         yah      yah    <bos>   \n12    12              a          is          is       <bos>    <bos>    <bos>   \n13    13           city           a           a       <bos>        a    <bos>   \n14    14             in        city        city       <bos>    <bos>    <bos>   \n15    15             **          in          in       <bos>       in    <bos>   \n16    16           Iraq          **          **       <bos>    <bos>    <bos>   \n17    17            **,      myſelf        Iraq        Iraq     Iraq     Iraq   \n18    18            not         **,       <bos>       <bos>    <bos>    <bos>   \n19    19         Turkey         not         not       <bos>      not    <bos>   \n20    20              .      Turkey      Turkey      Turkey   Turkey   Turkey   \n21    21                          .           .       <bos>    <bos>    <bos>   \n22    22             \\n                                                 <bos>   \n23    23  <end_of_turn>          \\n          \\n       <bos>    <bos>    <bos>   \n24    24          <eos>      myſelf      myſelf       <bos>   myſelf   itſelf   \n\n       L5          L6             L7          L8            L9            L10  \\\n0   <bos>       <bos>          <bos>        \\n\\n          \\n\\n           \\n\\n   \n1    That       <bos>           That        That          That           That   \n2   <bos>       <bos>      statement   statement        itſelf         itſelf   \n3      is       <bos>             is          is            is             is   \n4   <bos>   incorrect      incorrect   incorrect     incorrect      incorrect   \n5   <bos>       <bos>              .           .   religieuses          Theſe   \n6     the       <bos>                                            automatiques   \n7   <bos>       <bos>           \\n\\n        \\n\\n          \\n\\n           \\n\\n   \n8   <bos>       <bos>             Su    pleaſure         Jefus          Jefus   \n9     lay       <bos>  RenderAtEndOf       يتيمه         يتيمه          Jefus   \n10  <bos>       <bos>            Efq         Efq      pleaſure       pleaſure   \n11  <bos>       <bos>            Efq         Efq           Efq            Efq   \n12  <bos>       <bos>             is          is            is             is   \n13  <bos>       <bos>              a           a             a              a   \n14  <bos>       <bos>           city        city          city           city   \n15  <bos>       <bos>             in          in            in             in   \n16  <bos>       <bos>       pleaſure    pleaſure      pleaſure       pleaſure   \n17  <bos>        Iraq         itſelf         Efq           Efq            Efq   \n18  <bos>       <bos>          <bos>           ⓧ      pleaſure       pleaſure   \n19    not       <bos>            not         not       تضيفلها        تضيفلها   \n20  <bos>       <bos>         Turkey    pleaſure      Monfieur       Monfieur   \n21  <bos>       <bos>          <bos>           .             .       Monfieur   \n22    the       <bos>                                                   blest   \n23  <bos>       <bos>          <bos>          \\n            \\n             \\n   \n24  <bos>       <bos>       pleaſure     purpoſe       purpoſe        purpoſe   \n\n          L11         L12        L13         L14         L15         L16  \\\n0    sauvages    pleaſure          ⓧ           ⓧ           ⓧ    pleaſure   \n1       Theſe       Theſe       ^(@)        That        That     purpoſe   \n2         Efq      myſelf     myſelf    pleaſure     purpoſe     purpoſe   \n3          is          is       very        very        very   incorrect   \n4    Monfieur   incorrect   Monfieur   incorrect   incorrect    pleaſure   \n5       Theſe       Theſe   pleaſure     purpoſe     purpoſe     purpoſe   \n6       Jefus       Jefus      Jefus       Jefus       Jefus       Jefus   \n7        \\n\\n       Theſe     reaſon      reaſon      reaſon     purpoſe   \n8       Jefus       Jefus    purpoſe     purpoſe     purpoſe    pleaſure   \n9       Jefus       Jefus        Efq         Efq    pleaſure    pleaſure   \n10   pleaſure         Efq   pleaſure         Efq         Efq         Efq   \n11        Efq         Efq        Efq         Efq         Efq     Majefty   \n12         is        ^(@)          ⓧ           ⓧ        ^(@)        ^(@)   \n13       ^(@)        ^(@)       ^(@)        ^(@)        ^(@)        ^(@)   \n14       city         Efq       city           ⓧ         Efq         Efq   \n15      بوابة        ^(@)       ^(@)        ^(@)         Efq         Efq   \n16   pleaſure    pleaſure   pleaſure    pleaſure      myſelf      myſelf   \n17        Efq         Efq    Majefty     Majefty     Majefty     Majefty   \n18   pleaſure    pleaſure   pleaſure    pleaſure       poffe        raiſ   \n19    تضيفلها        ^(@)       ^(@)        ^(@)        ^(@)        ^(@)   \n20      Jefus     Majefty   pleaſure     Majefty       Jefus    pleaſure   \n21   Monfieur     purpoſe   pleaſure     purpoſe      myſelf     purpoſe   \n22      Jefus       Jefus      Jefus       Theſe        \\n\\n        \\n\\n   \n23         \\n          \\n       \\n\\n        \\n\\n        \\n\\n        \\n\\n   \n24    purpoſe     purpoſe   pleaſure    pleaſure    pleaſure    pleaſure   \n\n            L17            L18            L19            L20         L21  \\\n0       purpoſe      incorrect      incorrect      incorrect   incorrect   \n1     incorrect      incorrect      incorrect      incorrect   incorrect   \n2        itſelf      incorrect      incorrect      incorrect          is   \n3     incorrect      incorrect      incorrect      incorrect   incorrect   \n4      pleaſure    religieuses              .              .           .   \n5       purpoſe       suivants           \\n\\n           \\n\\n        \\n\\n   \n6       purpoſe           \\n\\n           \\n\\n           \\n\\n        \\n\\n   \n7       purpoſe       suivants            The            The         The   \n8      pleaſure        Majefty        Majefty      maternity   passenger   \n9      pleaſure         sorrow         sorrow        urinary      health   \n10      Majefty        Majefty          houſe       vicinity        city   \n11          Efq        belongs             is             is          is   \n12     actually        located       actually        located    actually   \n13         ^(@)           ^(@)           city           city        city   \n14          Efq        located        located        located     located   \n15          Efq     sanitaires            the            the         the   \n16      purpoſe        purpoſe       pleaſure       northern    northern   \n17      Majefty        Majefty        Majefty        instead           .   \n18        yaitu   specifically   specifically   specifically         not   \n19       drawal    necessarily             in             in          in   \n20   sanitaires              .              .              .           .   \n21         \\n\\n           \\n\\n           \\n\\n           \\n\\n        \\n\\n   \n22         \\n\\n           \\n\\n           \\n\\n           \\n\\n        \\n\\n   \n23         \\n\\n           \\n\\n          <eos>           \\n\\n       <eos>   \n24     pleaſure          <eos>          <eos>          <eos>       <eos>   \n\n           L22         L23         L24         L25            L26  \n0         That        that        that        This           That  \n1    statement          is          is           '      statement  \n2           is          is          is          is             is  \n3    incorrect   incorrect   incorrect   incorrect      incorrect  \n4            .           .           .           .              .  \n5         \\n\\n        \\n\\n        \\n\\n                             \n6         \\n\\n        \\n\\n        \\n\\n        \\n\\n           \\n\\n  \n7          The         The         The         The             Su  \n8    maternity      family         lay         lay            lay  \n9         city        city        city           -           mani  \n10        city        city        city           ,            yah  \n11          is          is          is          is             is  \n12     located           a           a           a              a  \n13        city        city        city        city           city  \n14     located          in          in          in             in  \n15         the         the         the         the             **  \n16    northern    northern    northern         the           Iraq  \n17           ,           ,           ,           ,            **,  \n18         not         not         not         not            not  \n19          in          in          in          in         Turkey  \n20           .           .           .           .              .  \n21          it                                                     \n22          \\n          \\n          \\n          \\n             \\n  \n23          \\n          \\n                          <end_of_turn>  \n24       <eos>       <eos>       <eos>       <eos>          <eos>  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>step</th>\n      <th>token</th>\n      <th>L0</th>\n      <th>L1</th>\n      <th>L2</th>\n      <th>L3</th>\n      <th>L4</th>\n      <th>L5</th>\n      <th>L6</th>\n      <th>L7</th>\n      <th>L8</th>\n      <th>L9</th>\n      <th>L10</th>\n      <th>L11</th>\n      <th>L12</th>\n      <th>L13</th>\n      <th>L14</th>\n      <th>L15</th>\n      <th>L16</th>\n      <th>L17</th>\n      <th>L18</th>\n      <th>L19</th>\n      <th>L20</th>\n      <th>L21</th>\n      <th>L22</th>\n      <th>L23</th>\n      <th>L24</th>\n      <th>L25</th>\n      <th>L26</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>That</td>\n      <td>\\n</td>\n      <td>\\n</td>\n      <td>&lt;bos&gt;</td>\n      <td>&lt;bos&gt;</td>\n      <td>&lt;bos&gt;</td>\n      <td>&lt;bos&gt;</td>\n      <td>&lt;bos&gt;</td>\n      <td>&lt;bos&gt;</td>\n      <td>\\n\\n</td>\n      <td>\\n\\n</td>\n      <td>\\n\\n</td>\n      <td>sauvages</td>\n      <td>pleaſure</td>\n      <td>ⓧ</td>\n      <td>ⓧ</td>\n      <td>ⓧ</td>\n      <td>pleaſure</td>\n      <td>purpoſe</td>\n      <td>incorrect</td>\n      <td>incorrect</td>\n      <td>incorrect</td>\n      <td>incorrect</td>\n      <td>That</td>\n      <td>that</td>\n      <td>that</td>\n      <td>This</td>\n      <td>That</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>statement</td>\n      <td>That</td>\n      <td>That</td>\n      <td>&lt;bos&gt;</td>\n      <td>That</td>\n      <td>&lt;bos&gt;</td>\n      <td>That</td>\n      <td>&lt;bos&gt;</td>\n      <td>That</td>\n      <td>That</td>\n      <td>That</td>\n      <td>That</td>\n      <td>Theſe</td>\n      <td>Theſe</td>\n      <td>^(@)</td>\n      <td>That</td>\n      <td>That</td>\n      <td>purpoſe</td>\n      <td>incorrect</td>\n      <td>incorrect</td>\n      <td>incorrect</td>\n      <td>incorrect</td>\n      <td>incorrect</td>\n      <td>statement</td>\n      <td>is</td>\n      <td>is</td>\n      <td>'</td>\n      <td>statement</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>is</td>\n      <td>statement</td>\n      <td>statement</td>\n      <td>statement</td>\n      <td>&lt;bos&gt;</td>\n      <td>&lt;bos&gt;</td>\n      <td>&lt;bos&gt;</td>\n      <td>&lt;bos&gt;</td>\n      <td>statement</td>\n      <td>statement</td>\n      <td>itſelf</td>\n      <td>itſelf</td>\n      <td>Efq</td>\n      <td>myſelf</td>\n      <td>myſelf</td>\n      <td>pleaſure</td>\n      <td>purpoſe</td>\n      <td>purpoſe</td>\n      <td>itſelf</td>\n      <td>incorrect</td>\n      <td>incorrect</td>\n      <td>incorrect</td>\n      <td>is</td>\n      <td>is</td>\n      <td>is</td>\n      <td>is</td>\n      <td>is</td>\n      <td>is</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>incorrect</td>\n      <td>is</td>\n      <td>is</td>\n      <td>&lt;bos&gt;</td>\n      <td>is</td>\n      <td>&lt;bos&gt;</td>\n      <td>is</td>\n      <td>&lt;bos&gt;</td>\n      <td>is</td>\n      <td>is</td>\n      <td>is</td>\n      <td>is</td>\n      <td>is</td>\n      <td>is</td>\n      <td>very</td>\n      <td>very</td>\n      <td>very</td>\n      <td>incorrect</td>\n      <td>incorrect</td>\n      <td>incorrect</td>\n      <td>incorrect</td>\n      <td>incorrect</td>\n      <td>incorrect</td>\n      <td>incorrect</td>\n      <td>incorrect</td>\n      <td>incorrect</td>\n      <td>incorrect</td>\n      <td>incorrect</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>.</td>\n      <td>incorrect</td>\n      <td>incorrect</td>\n      <td>&lt;bos&gt;</td>\n      <td>&lt;bos&gt;</td>\n      <td>&lt;bos&gt;</td>\n      <td>&lt;bos&gt;</td>\n      <td>incorrect</td>\n      <td>incorrect</td>\n      <td>incorrect</td>\n      <td>incorrect</td>\n      <td>incorrect</td>\n      <td>Monfieur</td>\n      <td>incorrect</td>\n      <td>Monfieur</td>\n      <td>incorrect</td>\n      <td>incorrect</td>\n      <td>pleaſure</td>\n      <td>pleaſure</td>\n      <td>religieuses</td>\n      <td>.</td>\n      <td>.</td>\n      <td>.</td>\n      <td>.</td>\n      <td>.</td>\n      <td>.</td>\n      <td>.</td>\n      <td>.</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>5</td>\n      <td></td>\n      <td>.</td>\n      <td>.</td>\n      <td>&lt;bos&gt;</td>\n      <td>&lt;bos&gt;</td>\n      <td>&lt;bos&gt;</td>\n      <td>&lt;bos&gt;</td>\n      <td>&lt;bos&gt;</td>\n      <td>.</td>\n      <td>.</td>\n      <td>religieuses</td>\n      <td>Theſe</td>\n      <td>Theſe</td>\n      <td>Theſe</td>\n      <td>pleaſure</td>\n      <td>purpoſe</td>\n      <td>purpoſe</td>\n      <td>purpoſe</td>\n      <td>purpoſe</td>\n      <td>suivants</td>\n      <td>\\n\\n</td>\n      <td>\\n\\n</td>\n      <td>\\n\\n</td>\n      <td>\\n\\n</td>\n      <td>\\n\\n</td>\n      <td>\\n\\n</td>\n      <td></td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>6</td>\n      <td>\\n\\n</td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td>&lt;bos&gt;</td>\n      <td>the</td>\n      <td>&lt;bos&gt;</td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td>automatiques</td>\n      <td>Jefus</td>\n      <td>Jefus</td>\n      <td>Jefus</td>\n      <td>Jefus</td>\n      <td>Jefus</td>\n      <td>Jefus</td>\n      <td>purpoſe</td>\n      <td>\\n\\n</td>\n      <td>\\n\\n</td>\n      <td>\\n\\n</td>\n      <td>\\n\\n</td>\n      <td>\\n\\n</td>\n      <td>\\n\\n</td>\n      <td>\\n\\n</td>\n      <td>\\n\\n</td>\n      <td>\\n\\n</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>7</td>\n      <td>Su</td>\n      <td>\\n\\n</td>\n      <td>\\n\\n</td>\n      <td>&lt;bos&gt;</td>\n      <td>&lt;bos&gt;</td>\n      <td>&lt;bos&gt;</td>\n      <td>&lt;bos&gt;</td>\n      <td>&lt;bos&gt;</td>\n      <td>\\n\\n</td>\n      <td>\\n\\n</td>\n      <td>\\n\\n</td>\n      <td>\\n\\n</td>\n      <td>\\n\\n</td>\n      <td>Theſe</td>\n      <td>reaſon</td>\n      <td>reaſon</td>\n      <td>reaſon</td>\n      <td>purpoſe</td>\n      <td>purpoſe</td>\n      <td>suivants</td>\n      <td>The</td>\n      <td>The</td>\n      <td>The</td>\n      <td>The</td>\n      <td>The</td>\n      <td>The</td>\n      <td>The</td>\n      <td>Su</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>8</td>\n      <td>lay</td>\n      <td>Su</td>\n      <td>Su</td>\n      <td>Su</td>\n      <td>Su</td>\n      <td>&lt;bos&gt;</td>\n      <td>&lt;bos&gt;</td>\n      <td>&lt;bos&gt;</td>\n      <td>Su</td>\n      <td>pleaſure</td>\n      <td>Jefus</td>\n      <td>Jefus</td>\n      <td>Jefus</td>\n      <td>Jefus</td>\n      <td>purpoſe</td>\n      <td>purpoſe</td>\n      <td>purpoſe</td>\n      <td>pleaſure</td>\n      <td>pleaſure</td>\n      <td>Majefty</td>\n      <td>Majefty</td>\n      <td>maternity</td>\n      <td>passenger</td>\n      <td>maternity</td>\n      <td>family</td>\n      <td>lay</td>\n      <td>lay</td>\n      <td>lay</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>9</td>\n      <td>mani</td>\n      <td>lay</td>\n      <td>lay</td>\n      <td>lay</td>\n      <td>lay</td>\n      <td>lay</td>\n      <td>lay</td>\n      <td>&lt;bos&gt;</td>\n      <td>RenderAtEndOf</td>\n      <td>يتيمه</td>\n      <td>يتيمه</td>\n      <td>Jefus</td>\n      <td>Jefus</td>\n      <td>Jefus</td>\n      <td>Efq</td>\n      <td>Efq</td>\n      <td>pleaſure</td>\n      <td>pleaſure</td>\n      <td>pleaſure</td>\n      <td>sorrow</td>\n      <td>sorrow</td>\n      <td>urinary</td>\n      <td>health</td>\n      <td>city</td>\n      <td>city</td>\n      <td>city</td>\n      <td>-</td>\n      <td>mani</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>10</td>\n      <td>yah</td>\n      <td>mani</td>\n      <td>mani</td>\n      <td>mani</td>\n      <td>mani</td>\n      <td>mani</td>\n      <td>&lt;bos&gt;</td>\n      <td>&lt;bos&gt;</td>\n      <td>Efq</td>\n      <td>Efq</td>\n      <td>pleaſure</td>\n      <td>pleaſure</td>\n      <td>pleaſure</td>\n      <td>Efq</td>\n      <td>pleaſure</td>\n      <td>Efq</td>\n      <td>Efq</td>\n      <td>Efq</td>\n      <td>Majefty</td>\n      <td>Majefty</td>\n      <td>houſe</td>\n      <td>vicinity</td>\n      <td>city</td>\n      <td>city</td>\n      <td>city</td>\n      <td>city</td>\n      <td>,</td>\n      <td>yah</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>11</td>\n      <td>is</td>\n      <td>yah</td>\n      <td>yah</td>\n      <td>yah</td>\n      <td>yah</td>\n      <td>&lt;bos&gt;</td>\n      <td>&lt;bos&gt;</td>\n      <td>&lt;bos&gt;</td>\n      <td>Efq</td>\n      <td>Efq</td>\n      <td>Efq</td>\n      <td>Efq</td>\n      <td>Efq</td>\n      <td>Efq</td>\n      <td>Efq</td>\n      <td>Efq</td>\n      <td>Efq</td>\n      <td>Majefty</td>\n      <td>Efq</td>\n      <td>belongs</td>\n      <td>is</td>\n      <td>is</td>\n      <td>is</td>\n      <td>is</td>\n      <td>is</td>\n      <td>is</td>\n      <td>is</td>\n      <td>is</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>12</td>\n      <td>a</td>\n      <td>is</td>\n      <td>is</td>\n      <td>&lt;bos&gt;</td>\n      <td>&lt;bos&gt;</td>\n      <td>&lt;bos&gt;</td>\n      <td>&lt;bos&gt;</td>\n      <td>&lt;bos&gt;</td>\n      <td>is</td>\n      <td>is</td>\n      <td>is</td>\n      <td>is</td>\n      <td>is</td>\n      <td>^(@)</td>\n      <td>ⓧ</td>\n      <td>ⓧ</td>\n      <td>^(@)</td>\n      <td>^(@)</td>\n      <td>actually</td>\n      <td>located</td>\n      <td>actually</td>\n      <td>located</td>\n      <td>actually</td>\n      <td>located</td>\n      <td>a</td>\n      <td>a</td>\n      <td>a</td>\n      <td>a</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>13</td>\n      <td>city</td>\n      <td>a</td>\n      <td>a</td>\n      <td>&lt;bos&gt;</td>\n      <td>a</td>\n      <td>&lt;bos&gt;</td>\n      <td>&lt;bos&gt;</td>\n      <td>&lt;bos&gt;</td>\n      <td>a</td>\n      <td>a</td>\n      <td>a</td>\n      <td>a</td>\n      <td>^(@)</td>\n      <td>^(@)</td>\n      <td>^(@)</td>\n      <td>^(@)</td>\n      <td>^(@)</td>\n      <td>^(@)</td>\n      <td>^(@)</td>\n      <td>^(@)</td>\n      <td>city</td>\n      <td>city</td>\n      <td>city</td>\n      <td>city</td>\n      <td>city</td>\n      <td>city</td>\n      <td>city</td>\n      <td>city</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>14</td>\n      <td>in</td>\n      <td>city</td>\n      <td>city</td>\n      <td>&lt;bos&gt;</td>\n      <td>&lt;bos&gt;</td>\n      <td>&lt;bos&gt;</td>\n      <td>&lt;bos&gt;</td>\n      <td>&lt;bos&gt;</td>\n      <td>city</td>\n      <td>city</td>\n      <td>city</td>\n      <td>city</td>\n      <td>city</td>\n      <td>Efq</td>\n      <td>city</td>\n      <td>ⓧ</td>\n      <td>Efq</td>\n      <td>Efq</td>\n      <td>Efq</td>\n      <td>located</td>\n      <td>located</td>\n      <td>located</td>\n      <td>located</td>\n      <td>located</td>\n      <td>in</td>\n      <td>in</td>\n      <td>in</td>\n      <td>in</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>15</td>\n      <td>**</td>\n      <td>in</td>\n      <td>in</td>\n      <td>&lt;bos&gt;</td>\n      <td>in</td>\n      <td>&lt;bos&gt;</td>\n      <td>&lt;bos&gt;</td>\n      <td>&lt;bos&gt;</td>\n      <td>in</td>\n      <td>in</td>\n      <td>in</td>\n      <td>in</td>\n      <td>بوابة</td>\n      <td>^(@)</td>\n      <td>^(@)</td>\n      <td>^(@)</td>\n      <td>Efq</td>\n      <td>Efq</td>\n      <td>Efq</td>\n      <td>sanitaires</td>\n      <td>the</td>\n      <td>the</td>\n      <td>the</td>\n      <td>the</td>\n      <td>the</td>\n      <td>the</td>\n      <td>the</td>\n      <td>**</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>16</td>\n      <td>Iraq</td>\n      <td>**</td>\n      <td>**</td>\n      <td>&lt;bos&gt;</td>\n      <td>&lt;bos&gt;</td>\n      <td>&lt;bos&gt;</td>\n      <td>&lt;bos&gt;</td>\n      <td>&lt;bos&gt;</td>\n      <td>pleaſure</td>\n      <td>pleaſure</td>\n      <td>pleaſure</td>\n      <td>pleaſure</td>\n      <td>pleaſure</td>\n      <td>pleaſure</td>\n      <td>pleaſure</td>\n      <td>pleaſure</td>\n      <td>myſelf</td>\n      <td>myſelf</td>\n      <td>purpoſe</td>\n      <td>purpoſe</td>\n      <td>pleaſure</td>\n      <td>northern</td>\n      <td>northern</td>\n      <td>northern</td>\n      <td>northern</td>\n      <td>northern</td>\n      <td>the</td>\n      <td>Iraq</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>17</td>\n      <td>**,</td>\n      <td>myſelf</td>\n      <td>Iraq</td>\n      <td>Iraq</td>\n      <td>Iraq</td>\n      <td>Iraq</td>\n      <td>&lt;bos&gt;</td>\n      <td>Iraq</td>\n      <td>itſelf</td>\n      <td>Efq</td>\n      <td>Efq</td>\n      <td>Efq</td>\n      <td>Efq</td>\n      <td>Efq</td>\n      <td>Majefty</td>\n      <td>Majefty</td>\n      <td>Majefty</td>\n      <td>Majefty</td>\n      <td>Majefty</td>\n      <td>Majefty</td>\n      <td>Majefty</td>\n      <td>instead</td>\n      <td>.</td>\n      <td>,</td>\n      <td>,</td>\n      <td>,</td>\n      <td>,</td>\n      <td>**,</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>18</td>\n      <td>not</td>\n      <td>**,</td>\n      <td>&lt;bos&gt;</td>\n      <td>&lt;bos&gt;</td>\n      <td>&lt;bos&gt;</td>\n      <td>&lt;bos&gt;</td>\n      <td>&lt;bos&gt;</td>\n      <td>&lt;bos&gt;</td>\n      <td>&lt;bos&gt;</td>\n      <td>ⓧ</td>\n      <td>pleaſure</td>\n      <td>pleaſure</td>\n      <td>pleaſure</td>\n      <td>pleaſure</td>\n      <td>pleaſure</td>\n      <td>pleaſure</td>\n      <td>poffe</td>\n      <td>raiſ</td>\n      <td>yaitu</td>\n      <td>specifically</td>\n      <td>specifically</td>\n      <td>specifically</td>\n      <td>not</td>\n      <td>not</td>\n      <td>not</td>\n      <td>not</td>\n      <td>not</td>\n      <td>not</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>19</td>\n      <td>Turkey</td>\n      <td>not</td>\n      <td>not</td>\n      <td>&lt;bos&gt;</td>\n      <td>not</td>\n      <td>&lt;bos&gt;</td>\n      <td>not</td>\n      <td>&lt;bos&gt;</td>\n      <td>not</td>\n      <td>not</td>\n      <td>تضيفلها</td>\n      <td>تضيفلها</td>\n      <td>تضيفلها</td>\n      <td>^(@)</td>\n      <td>^(@)</td>\n      <td>^(@)</td>\n      <td>^(@)</td>\n      <td>^(@)</td>\n      <td>drawal</td>\n      <td>necessarily</td>\n      <td>in</td>\n      <td>in</td>\n      <td>in</td>\n      <td>in</td>\n      <td>in</td>\n      <td>in</td>\n      <td>in</td>\n      <td>Turkey</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>20</td>\n      <td>.</td>\n      <td>Turkey</td>\n      <td>Turkey</td>\n      <td>Turkey</td>\n      <td>Turkey</td>\n      <td>Turkey</td>\n      <td>&lt;bos&gt;</td>\n      <td>&lt;bos&gt;</td>\n      <td>Turkey</td>\n      <td>pleaſure</td>\n      <td>Monfieur</td>\n      <td>Monfieur</td>\n      <td>Jefus</td>\n      <td>Majefty</td>\n      <td>pleaſure</td>\n      <td>Majefty</td>\n      <td>Jefus</td>\n      <td>pleaſure</td>\n      <td>sanitaires</td>\n      <td>.</td>\n      <td>.</td>\n      <td>.</td>\n      <td>.</td>\n      <td>.</td>\n      <td>.</td>\n      <td>.</td>\n      <td>.</td>\n      <td>.</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>21</td>\n      <td></td>\n      <td>.</td>\n      <td>.</td>\n      <td>&lt;bos&gt;</td>\n      <td>&lt;bos&gt;</td>\n      <td>&lt;bos&gt;</td>\n      <td>&lt;bos&gt;</td>\n      <td>&lt;bos&gt;</td>\n      <td>&lt;bos&gt;</td>\n      <td>.</td>\n      <td>.</td>\n      <td>Monfieur</td>\n      <td>Monfieur</td>\n      <td>purpoſe</td>\n      <td>pleaſure</td>\n      <td>purpoſe</td>\n      <td>myſelf</td>\n      <td>purpoſe</td>\n      <td>\\n\\n</td>\n      <td>\\n\\n</td>\n      <td>\\n\\n</td>\n      <td>\\n\\n</td>\n      <td>\\n\\n</td>\n      <td>it</td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>22</td>\n      <td>\\n</td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td>&lt;bos&gt;</td>\n      <td>the</td>\n      <td>&lt;bos&gt;</td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td>blest</td>\n      <td>Jefus</td>\n      <td>Jefus</td>\n      <td>Jefus</td>\n      <td>Theſe</td>\n      <td>\\n\\n</td>\n      <td>\\n\\n</td>\n      <td>\\n\\n</td>\n      <td>\\n\\n</td>\n      <td>\\n\\n</td>\n      <td>\\n\\n</td>\n      <td>\\n\\n</td>\n      <td>\\n</td>\n      <td>\\n</td>\n      <td>\\n</td>\n      <td>\\n</td>\n      <td>\\n</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>23</td>\n      <td>&lt;end_of_turn&gt;</td>\n      <td>\\n</td>\n      <td>\\n</td>\n      <td>&lt;bos&gt;</td>\n      <td>&lt;bos&gt;</td>\n      <td>&lt;bos&gt;</td>\n      <td>&lt;bos&gt;</td>\n      <td>&lt;bos&gt;</td>\n      <td>&lt;bos&gt;</td>\n      <td>\\n</td>\n      <td>\\n</td>\n      <td>\\n</td>\n      <td>\\n</td>\n      <td>\\n</td>\n      <td>\\n\\n</td>\n      <td>\\n\\n</td>\n      <td>\\n\\n</td>\n      <td>\\n\\n</td>\n      <td>\\n\\n</td>\n      <td>\\n\\n</td>\n      <td>&lt;eos&gt;</td>\n      <td>\\n\\n</td>\n      <td>&lt;eos&gt;</td>\n      <td>\\n</td>\n      <td>\\n</td>\n      <td></td>\n      <td></td>\n      <td>&lt;end_of_turn&gt;</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>24</td>\n      <td>&lt;eos&gt;</td>\n      <td>myſelf</td>\n      <td>myſelf</td>\n      <td>&lt;bos&gt;</td>\n      <td>myſelf</td>\n      <td>itſelf</td>\n      <td>&lt;bos&gt;</td>\n      <td>&lt;bos&gt;</td>\n      <td>pleaſure</td>\n      <td>purpoſe</td>\n      <td>purpoſe</td>\n      <td>purpoſe</td>\n      <td>purpoſe</td>\n      <td>purpoſe</td>\n      <td>pleaſure</td>\n      <td>pleaſure</td>\n      <td>pleaſure</td>\n      <td>pleaſure</td>\n      <td>pleaſure</td>\n      <td>&lt;eos&gt;</td>\n      <td>&lt;eos&gt;</td>\n      <td>&lt;eos&gt;</td>\n      <td>&lt;eos&gt;</td>\n      <td>&lt;eos&gt;</td>\n      <td>&lt;eos&gt;</td>\n      <td>&lt;eos&gt;</td>\n      <td>&lt;eos&gt;</td>\n      <td>&lt;eos&gt;</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"df.iloc[0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T04:21:27.419463Z","iopub.execute_input":"2025-07-11T04:21:27.419757Z","iopub.status.idle":"2025-07-11T04:21:27.426185Z","shell.execute_reply.started":"2025-07-11T04:21:27.419736Z","shell.execute_reply":"2025-07-11T04:21:27.425493Z"}},"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"step              0\ntoken          That\nL0               \\n\nL1               \\n\nL2            <bos>\nL3            <bos>\nL4            <bos>\nL5            <bos>\nL6            <bos>\nL7            <bos>\nL8             \\n\\n\nL9             \\n\\n\nL10            \\n\\n\nL11        sauvages\nL12        pleaſure\nL13               ⓧ\nL14               ⓧ\nL15               ⓧ\nL16        pleaſure\nL17         purpoſe\nL18       incorrect\nL19       incorrect\nL20       incorrect\nL21       incorrect\nL22            That\nL23            that\nL24            that\nL25            This\nL26            That\nName: 0, dtype: object"},"metadata":{}}],"execution_count":17},{"cell_type":"code","source":"%cd /kaggle/working/Truth_is_Universal/","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T04:34:15.565805Z","iopub.execute_input":"2025-07-11T04:34:15.566500Z","iopub.status.idle":"2025-07-11T04:34:15.570708Z","shell.execute_reply.started":"2025-07-11T04:34:15.566472Z","shell.execute_reply":"2025-07-11T04:34:15.570150Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working/Truth_is_Universal\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"import os\nimport torch as t\ndataset = \"cities\"\nactivation_dir = f\"acts/Gemma2/2B/chat/{dataset}\"\nexample_index = 425 #putting 426 gives me 427th ex, or 428th row\nbatch_size = 25\n\nbatch_start = (example_index // batch_size) * batch_size\noffset_in_batch = example_index % batch_size\n\nlayer_vectors = []\n\nfor layer_idx in range(26):\n    file_path = os.path.join(activation_dir, f\"layer_{layer_idx}_{batch_start}.pt\")\n    if not os.path.exists(file_path):\n        print(f\"Missing file: {file_path}\")\n        continue\n    \n    acts = t.load(file_path)\n    vector = acts[offset_in_batch]  # shape: [hidden_dim]\n    layer_vectors.append(vector.cpu().numpy())  # convert to numpy if needed\n\nprint(f\"Collected {len(layer_vectors)} layer vectors for example {example_index}\")\ndf_data = pd.read_csv(f\"/kaggle/working/Truth_is_Universal/datasets/{dataset}.csv\")\nprint(f\"Example : cities/{example_index}/{df_data.iloc[example_index]}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T04:38:33.405066Z","iopub.execute_input":"2025-07-11T04:38:33.405384Z","iopub.status.idle":"2025-07-11T04:38:33.435843Z","shell.execute_reply.started":"2025-07-11T04:38:33.405361Z","shell.execute_reply":"2025-07-11T04:38:33.435131Z"}},"outputs":[{"name":"stdout","text":"Collected 26 layer vectors for example 425\nExample : cities/425/statement          The city of Sulaymaniyah is in Turkey.\nlabel                                                   0\ncity                                         Sulaymaniyah\ncountry                                            Turkey\ncorrect_country                                      Iraq\nName: 425, dtype: object\n","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom math import pi\nlayer_vectors = np.array(layer_vectors)\nprint(layer_vectors.shape)\n\nn = 26\ndef get_similarity_heatmap(layer_vectors):\n    normed = layer_vectors / np.linalg.norm(layer_vectors, axis=1, keepdims=True)\n    sim_matrix = normed @ normed.T  \n    sim_matrix = np.clip(sim_matrix, -1.0, 1.0)\n    heatmap_values = np.arccos(sim_matrix)*1/pi\n    return heatmap_values\n\nsns.heatmap(get_similarity_heatmap(layer_vectors), cmap='viridis')\nplt.title(\"1/pi arc cos( cosine similarity ( x_i, x_j))\")\nplt.xlabel(\"Layer j\")\nplt.ylabel(\"Layer i\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T05:00:03.662009Z","iopub.execute_input":"2025-07-11T05:00:03.662586Z","iopub.status.idle":"2025-07-11T05:00:04.105749Z","shell.execute_reply.started":"2025-07-11T05:00:03.662562Z","shell.execute_reply":"2025-07-11T05:00:04.104988Z"}},"outputs":[{"name":"stdout","text":"(26, 2304)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 2 Axes>","image/png":"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\n"},"metadata":{}}],"execution_count":52},{"cell_type":"code","source":"print(tokenizer(\" True\",  add_special_tokens=False).input_ids)\nprint(tokenizer(\"False\", add_special_tokens=False).input_ids)\n\n# 2) Convert back to tokens to confirm:\nfor tid in tokenizer(\" True\", add_special_tokens=False).input_ids:\n    print(tid, tokenizer.convert_ids_to_tokens(tid))\nfor tid in tokenizer(\"False\", add_special_tokens=False).input_ids:\n    print(tid, tokenizer.convert_ids_to_tokens(tid))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-07T02:19:01.068784Z","iopub.execute_input":"2025-07-07T02:19:01.069293Z","iopub.status.idle":"2025-07-07T02:19:01.07557Z","shell.execute_reply.started":"2025-07-07T02:19:01.069268Z","shell.execute_reply":"2025-07-07T02:19:01.074851Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch \nimport numpy as np\nfrom utils import DataManager, dataset_sizes, collect_training_data, compute_statistics\nfrom probes import learn_truth_directions\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import LinearSegmentedColormap\nfrom sklearn.linear_model import LogisticRegression\nfrom scipy.stats import gaussian_kde\nfrom sklearn.decomposition import PCA\nfrom collections import defaultdict\nfrom sklearn.metrics import roc_auc_score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T06:06:28.101824Z","iopub.execute_input":"2025-07-02T06:06:28.102056Z","iopub.status.idle":"2025-07-02T06:06:28.306821Z","shell.execute_reply.started":"2025-07-02T06:06:28.102033Z","shell.execute_reply":"2025-07-02T06:06:28.306293Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# hyperparameters\nmodel_family = 'Gemma2' # options are 'Llama3', 'Llama2', 'Gemma', 'Gemma2' or 'Mistral'\nmodel_size = '2B'\nmodel_type = 'chat' # options are 'chat' or 'base'\nlayer = 19 # layer from which to extract activations\n\n# define datasets used for training\n# the ordering [affirmative_dataset1, negated_dataset1, affirmative_dataset2, negated_dataset2, ...] is required by some functions\ntrain_sets = [\"cities\", \"neg_cities\", \"sp_en_trans\", \"neg_sp_en_trans\", \"inventors\", \"neg_inventors\", \"animal_class\", \"neg_animal_class\", \"element_symb\", \"neg_element_symb\", \"facts\", \"neg_facts\"]\n# get size of each training dataset to include an equal number of statements from each topic in training data\ntrain_set_sizes = dataset_sizes(train_sets) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T06:13:58.102353Z","iopub.execute_input":"2025-07-02T06:13:58.103081Z","iopub.status.idle":"2025-07-02T06:13:58.109528Z","shell.execute_reply.started":"2025-07-02T06:13:58.103055Z","shell.execute_reply":"2025-07-02T06:13:58.108843Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Separation between true and false statements across layers","metadata":{}},{"cell_type":"code","source":"# find the layer with the largest separation between true and false statements\n# you need to have stored the activations in layers 1-26 for all datasets to run this cell\nlayers = np.arange(1, 52, 1)\ndatasets_separation = ['cities', 'neg_cities', 'sp_en_trans', 'neg_sp_en_trans', \\\n                       \"inventors\", \"neg_inventors\", \"animal_class\", \"neg_animal_class\", \"element_symb\"\\\n                       ,\"neg_element_symb\", \"facts\", \"neg_facts\"]\nlist_coords = {}\nfor dataset in datasets_separation:\n    between_class_variances = []\n    within_class_variances = []\n    for layer_nr in layers:\n        dm = DataManager() \n        dm.add_dataset(dataset, model_family, model_size, model_type,\n                        layer_nr, split=None, center=False, device='cpu')\n        acts, labels = dm.data[dataset]\n        # Calculate means for each class\n        false_stmnt_ids = labels == 0\n        true_stmnt_ids = labels == 1\n\n        false_acts = acts[false_stmnt_ids]\n        true_acts = acts[true_stmnt_ids]\n\n        mean_false = false_acts.mean(dim=0)\n        mean_true = true_acts.mean(dim=0)\n\n        # Calculate within-class variance\n        within_class_variance_false = false_acts.var(dim=0).mean()\n        within_class_variance_true = true_acts.var(dim=0).mean()\n        within_class_variances.append((within_class_variance_false + within_class_variance_true).item() / 2)\n\n        # Calculate between-class variance\n        overall_mean = acts.mean(dim=0)\n        between_class_variances.append(((mean_false - overall_mean).pow(2) \n                                        + (mean_true - overall_mean).pow(2)).mean().item() / 2)\n\n    plt.plot(layers, np.array(between_class_variances) / np.array(within_class_variances), label=dataset)\n    list_coords[dataset] = (list(layers), list(np.array(between_class_variances) / np.array(within_class_variances)))\nplt.legend(fontsize=10)\nplt.figure_size(10, 20)\nplt.xticks(np.arange(0,51,1))\nplt.ylabel('Between class variance /\\nwithin-class variance', fontsize=10)\nplt.xlabel('Layer', fontsize=10)\nplt.title('Separation between true and false\\nstatements across layers', fontsize=10)\nplt.grid(True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T06:06:28.364961Z","iopub.execute_input":"2025-07-02T06:06:28.365163Z","iopub.status.idle":"2025-07-02T06:06:34.507836Z","shell.execute_reply.started":"2025-07-02T06:06:28.365148Z","shell.execute_reply":"2025-07-02T06:06:34.507038Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"'''print(list_coords['cities'])\ngrouped_dict = {}\nfor i in range(len(list_coords.keys())):\n    grouped_dict[list_coords[i]] = [grouped]'''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T16:34:47.067882Z","iopub.execute_input":"2025-07-01T16:34:47.068726Z","iopub.status.idle":"2025-07-01T16:34:47.072579Z","shell.execute_reply.started":"2025-07-01T16:34:47.068696Z","shell.execute_reply":"2025-07-01T16:34:47.072029Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Supervised learning of the truth directions and classification accuracies","metadata":{}},{"cell_type":"code","source":"file_path = \"/kaggle/working/Truth_is_Universal/utils.py\"\n\nwith open(file_path, \"r\") as f:\n    for i, line in enumerate(f):\n        if 0 <= i <= 150:\n            try:\n                print(f\"{i+1}: {line.rstrip()}\")\n            except:\n                break","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"layer = 1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T06:39:50.620492Z","iopub.execute_input":"2025-07-02T06:39:50.621014Z","iopub.status.idle":"2025-07-02T06:39:50.624358Z","shell.execute_reply.started":"2025-07-02T06:39:50.620966Z","shell.execute_reply":"2025-07-02T06:39:50.623755Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"nr_runs = 15\nresults = {'t_g': defaultdict(list), 't_p': defaultdict(list), 'd_{LR}': defaultdict(list)}\n\nfor _ in range(nr_runs):\n    for i in range(0, len(train_sets), 2):\n        # leave one dataset out (affirmative + negated)\n        cv_train_sets = [set for j, set in enumerate(train_sets) if j not in (i, i+1)]\n        \n        # Collect training data\n        acts_centered, _, labels, polarities = collect_training_data(cv_train_sets, train_set_sizes, model_family,\n                                                          model_size, model_type, layer)\n        \n        # Fit model\n        t_g, t_p = learn_truth_directions(acts_centered, labels, polarities)\n\n        # fit LR for comparison\n        LR = LogisticRegression(penalty=None, fit_intercept=False)\n        LR.fit(acts_centered.numpy(), labels.numpy())\n        d_lr = torch.from_numpy(LR.coef_[0]).float()\n                \n        # Evaluate on held-out sets, assuming affirmative and negated dataset on the same topic are at index i and i+1\n        for j in range(2):\n            dataset = train_sets[i+j]\n            dm = DataManager()\n            dm.add_dataset(dataset, model_family, model_size, model_type, layer, split=None, center=False, device='cpu')\n            acts, labels = dm.get(dataset)\n\n            auroc = roc_auc_score(labels.numpy(), (acts @ t_g).numpy())\n            results['t_g'][dataset].append(auroc)\n            auroc = roc_auc_score(labels.numpy(), (acts @ t_p).numpy())\n            results['t_p'][dataset].append(auroc)\n            auroc = roc_auc_score(labels.numpy(), (acts @ d_lr).numpy())\n            results['d_{LR}'][dataset].append(auroc)\n\nstat_results = compute_statistics(results)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-07-02T06:46:33.039Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create a custom colormap from red to yellow\ncmap = LinearSegmentedColormap.from_list('red_yellow', [(1, 0, 0), (1, 1, 0)], N=100)\n\n# Create three subplots side-by-side\nfig, (ax1, ax2, ax3) = plt.subplots(figsize=(3.5, 6), ncols=3)\n\nfor ax, key in zip((ax1, ax2, ax3), ('t_g', 't_p', 'd_{LR}')):\n    grid = [[stat_results[key]['mean'][dataset]] for dataset in train_sets]\n    im = ax.imshow(grid, vmin=0, vmax=1, cmap=cmap)\n    \n    ax.set_aspect('auto')\n    ax.set_aspect(0.6)\n    \n    for i, row in enumerate(grid):\n        for j, val in enumerate(row):\n            ax.text(j, i, f'{val:.2f}', ha='center', va='center', fontsize=13)\n    \n    ax.set_yticks(range(len(train_sets)))\n    ax.set_xticks([])\n    ax.set_title(f\"${key}$\", fontsize=14)\n\nax1.set_yticklabels(train_sets, fontsize=13)\nax2.set_yticklabels([])\nax3.set_yticklabels([])\n\n# Adjust the layout to make room for the colorbar\nplt.subplots_adjust(top=0.9, bottom=0.05, left=0.1, right=0.9, wspace=0.4)\n\n# Add colorbar with a specified position\ncbar_ax = fig.add_axes([0.92, 0.15, 0.02, 0.7])  # [left, bottom, width, height]\ncbar = fig.colorbar(im, cax=cbar_ax)\ncbar.ax.tick_params(labelsize=13)\n\nfig.suptitle(\"AUROC\", fontsize=15)\nplt.show()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-07-02T06:46:33.039Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Activation vectors projected onto 2d truth subspace","metadata":{}},{"cell_type":"code","source":"# Compute t_g and t_p using all data\nacts_centered, _, labels, polarities = collect_training_data(train_sets, train_set_sizes, model_family, \n                                                    model_size, model_type, layer)\nt_g, t_p = learn_truth_directions(acts_centered, labels, polarities)\n\n# define helper functions for plotting\ndef collect_affirm_neg_data(train_sets, train_set_sizes, model_family, model_size, model_type, layer):\n    dm_affirm, dm_neg = DataManager(), DataManager()\n    for dataset_name in train_sets:\n        split = min(train_set_sizes.values()) / train_set_sizes[dataset_name]\n        if 'neg_' not in dataset_name:\n            dm_affirm.add_dataset(dataset_name, model_family, model_size, model_type, layer, split=split, center=False, device='cpu')\n        else:\n            dm_neg.add_dataset(dataset_name, model_family, model_size, model_type, layer, split=split, center=False, device='cpu')\n    return dm_affirm.get('train') + dm_neg.get('train')\n\ndef compute_t_affirm(acts_affirm, labels_affirm):\n    LR = LogisticRegression(penalty=None, fit_intercept=True)\n    LR.fit(acts_affirm.numpy(), labels_affirm.numpy())\n    return LR.coef_[0] / np.linalg.norm(LR.coef_[0])\n\ndef compute_orthonormal_vectors(t_g, t_p):\n    t_g_numpy = t_g.numpy()\n    t_p_numpy = t_p.numpy()\n    projection = np.dot(t_p_numpy, t_g_numpy) / np.dot(t_g_numpy, t_g_numpy) * t_g_numpy\n    t_p_orthonormal = (t_p_numpy - projection) / np.linalg.norm(t_p_numpy - projection)\n    t_g_orthonormal = t_g_numpy / np.linalg.norm(t_g_numpy)\n    return t_g_orthonormal, t_p_orthonormal\n\ndef project_activations(acts, t_g, t_p):\n    return t_g @ acts.numpy().T, t_p @ acts.numpy().T\n\ndef plot_vector(ax, vector, t_g_orthonormal, t_p_orthonormal, label, midpoint):\n    # Normalize input vector\n    vector_normalized = vector / np.linalg.norm(vector)\n    \n    # Compute vector_subspace\n    vector_subspace = np.array([(np.dot(t_g_orthonormal, vector_normalized)), \n                                (np.dot(t_p_orthonormal, vector_normalized))])\n\n    vector_subspace = vector_subspace / np.linalg.norm(vector_subspace)\n    \n    # Get current axis limits\n    xlim = ax.get_xlim()\n    ylim = ax.get_ylim()\n    \n    # Compute scale based on axis limits\n    axis_range = max(xlim[1] - xlim[0], ylim[1] - ylim[0])\n    scale = 0.4 * axis_range  # Adjust this factor to change the relative size of the vector\n    \n    # Compute arrow head position\n    arrow_head = np.array(midpoint) + scale * vector_subspace\n    \n    # Compute label offset based on axis limits\n    label_offset = np.array([0.03 * (xlim[1] - xlim[0]), 0.03 * (ylim[1] - ylim[0])])\n    \n    # Adjust label position to avoid overlap with arrow\n    label_position = arrow_head + label_offset * np.sign(vector_subspace)\n    \n    # Plot the vector\n    ax.quiver(*midpoint, *(scale * vector_subspace), \n              color='green', angles='xy', scale_units='xy', scale=1, \n              width=0.03)\n    \n    # Add label\n    ax.annotate(label, xy=label_position, fontsize=21, \n                ha='center', va='center')\n\n# Update the plot_scatter function to use the new plot_vector function\ndef plot_scatter(ax, proj_g, proj_p, labels, proj_g_other, proj_p_other, labels_other,\n                  title, plot_t_a=False, plot_t_g_t_p=False, **kwargs):\n    label_to_color = {0: 'indigo', 1: 'orange'}\n    label_to_marker = {0: 's', 1: '^'}\n\n    for label in [0, 1]:\n        idx = labels.numpy() == label\n        ax.scatter(proj_g[idx], proj_p[idx], c=label_to_color[label], marker=label_to_marker[label], alpha=0.5, s=5)\n        idx_other = labels_other.numpy() == label\n        if title == \"Affirmative & Negated\\nStatements\":\n            ax.scatter(proj_g_other[idx_other], proj_p_other[idx_other], c=label_to_color[label],\n                        marker=label_to_marker[label], alpha=0.5, s=5)\n        else:\n            ax.scatter(proj_g_other[idx_other], proj_p_other[idx_other], c='grey', marker=label_to_marker[label], alpha=0.1, s=5)\n\n    # Compute midpoint based on current axis limits\n    xlim = ax.get_xlim()\n    ylim = ax.get_ylim()\n    midpoint = (0.5 * (xlim[0] + xlim[1]), 0.5 * (ylim[0] + ylim[1]))\n\n    if plot_t_a:\n        plot_vector(ax, kwargs['t_affirm'], kwargs['t_g_orthonormal'], kwargs['t_p_orthonormal'], \"$t_A$\", midpoint)\n    if plot_t_g_t_p:\n        plot_vector(ax, kwargs['t_g'], kwargs['t_g_orthonormal'], kwargs['t_p_orthonormal'], \"$t_G$\", midpoint)\n        plot_vector(ax, kwargs['t_p'], kwargs['t_g_orthonormal'], kwargs['t_p_orthonormal'], \"$t_P$\", midpoint)\n\n    ax.set_title(title, fontsize=19)\n    #ax.set_yticks([])\n    #ax.set_xticks([])\n\n    # Update axis limits after plotting\n    ax.autoscale()\n    ax.set_aspect('equal')\n\ndef add_legend(ax):\n    handles = [plt.scatter([], [], c='indigo', marker='s', label='False'),\n               plt.scatter([], [], c='orange', marker='^', label='True')]\n    ax.legend(handles=handles, fontsize=18)\n\ndef plot_density(ax, acts, labels, t, xlabel):\n    # Convert inputs to NumPy arrays if they're PyTorch tensors\n    if isinstance(acts, torch.Tensor):\n        acts = acts.detach().cpu().numpy()\n    if isinstance(labels, torch.Tensor):\n        labels = labels.detach().cpu().numpy()\n    if isinstance(t, torch.Tensor):\n        t = t.detach().cpu().numpy()\n\n    # Compute projections\n    if t.ndim == 1:\n        proj = t @ acts.T\n    else:\n        proj = t.reshape(1, -1) @ acts.T\n    \n    # Compute axis limits based on projected activations\n    x_min, x_max = np.min(proj), np.max(proj)\n    x_range = x_max - x_min\n    x_padding = 0.1 * x_range  # Add 10% padding on each side\n    xlim = (x_min - x_padding, x_max + x_padding)\n    \n    # Set x-axis limits\n    ax.set_xlim(xlim)\n    \n    # Compute KDE for each label\n    x_grid = np.linspace(xlim[0], xlim[1], 400)\n    for label, color in zip([0, 1], ['indigo', 'orange']):\n        data = proj[labels == label]\n        kde = gaussian_kde(data)\n        density = kde(x_grid)\n        density /= np.trapz(density, x_grid)\n        ax.plot(x_grid, density, color=color)\n\n    # Plot scatter points\n    y_scatter = np.ones(np.shape(proj)) * (-0.05)\n    colors = ['indigo' if label == 0 else 'orange' for label in labels]\n    ax.scatter(proj, y_scatter, c=colors, alpha=0.3, s=10)\n    \n    # Set y-axis limits to accommodate both KDE and scatter points\n    y_max = ax.get_ylim()[1]\n    ax.set_ylim(-0.1, y_max * 1.1)  # Extend y-axis slightly above the maximum KDE value\n\n     # Calculate AUROC\n    auroc = calculate_auroc(acts, labels, t)\n    \n    # Display AUROC in the top left corner\n    ax.text(0.05, 0.95, f'AUROC: {auroc:.2f}', transform=ax.transAxes, \n            verticalalignment='top', fontsize=14, bbox=dict(facecolor='white', alpha=0.7))\n    \n    # Set labels and remove ticks\n    ax.set_ylabel('Frequency', fontsize=19)\n    ax.set_xlabel(xlabel, fontsize=19)\n    ax.set_yticks([])\n    ax.set_xticks([])\n\n    # Add a light grid\n    ax.grid(True, linestyle='--', alpha=0.3)\n\ndef calculate_auroc(acts, labels, t):\n    if isinstance(acts, torch.Tensor):\n        acts = acts.detach().cpu().numpy()\n    if isinstance(labels, torch.Tensor):\n        labels = labels.detach().cpu().numpy()\n    if isinstance(t, torch.Tensor):\n        t = t.detach().cpu().numpy()\n    \n    proj = t @ acts.T\n    auroc = roc_auc_score(labels, proj)\n    return auroc","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T06:38:33.317885Z","iopub.execute_input":"2025-07-02T06:38:33.318092Z","iopub.status.idle":"2025-07-02T06:38:33.493059Z","shell.execute_reply.started":"2025-07-02T06:38:33.318078Z","shell.execute_reply":"2025-07-02T06:38:33.492313Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Figure 1","metadata":{}},{"cell_type":"code","source":"fig = plt.figure(figsize=(11.5, 11))\naxes = [\n    fig.add_axes([0.4, 0.4, 0.26, 0.26]),  # ax1: top center\n    fig.add_axes([0.7, 0.4, 0.26, 0.26]),  # ax2: top right\n    fig.add_axes([0.1, 0.4, 0.26, 0.26]),  # ax3: top left\n    fig.add_axes([0.58, 0.1, 0.25, 0.25]), # ax4: bottom right\n    fig.add_axes([0.23, 0.1, 0.25, 0.25])  # ax5: bottom left\n]\n\n# Collect activations and labels of affirmative and negated statements separately\nacts_affirm, labels_affirm, acts_neg, labels_neg = collect_affirm_neg_data(train_sets, train_set_sizes,\n                                                                            model_family, model_size, model_type, layer)\n\n# Compute t_affirm\nt_affirm = compute_t_affirm(acts_affirm, labels_affirm)\n\n# orthonormalise t_g and t_p\nt_g_orthonormal, t_p_orthonormal = compute_orthonormal_vectors(t_g, t_p)\n\n# Project activations\nproj_g_affirm, proj_p_affirm = project_activations(acts_affirm, t_g_orthonormal, t_p_orthonormal)\nproj_g_neg, proj_p_neg = project_activations(acts_neg, t_g_orthonormal, t_p_orthonormal)\n\n# Plot scatter plots\nplot_scatter(axes[0], proj_g_affirm, proj_p_affirm, labels_affirm,\n              proj_g_neg, proj_p_neg, labels_neg, 'Affirmative Statements', plot_t_a=True,\n                t_affirm=t_affirm, t_g_orthonormal=t_g_orthonormal, t_p_orthonormal=t_p_orthonormal)\nplot_scatter(axes[1], proj_g_neg, proj_p_neg, labels_neg, proj_g_affirm,\n              proj_p_affirm, labels_affirm, 'Negated Statements')\nplot_scatter(axes[2], proj_g_affirm, proj_p_affirm, labels_affirm,\n              proj_g_neg, proj_p_neg, labels_neg, 'Affirmative & Negated\\nStatements', plot_t_g_t_p=True,\n                t_g=t_g, t_p=t_p, t_g_orthonormal=t_g_orthonormal, t_p_orthonormal=t_p_orthonormal)\n\n# Add legend\nadd_legend(axes[2])\n\n# Plot density plots\nacts = torch.cat((acts_affirm, acts_neg), dim=0)\nlabels = torch.cat((labels_affirm, labels_neg))\nplot_density(axes[3], acts, labels, t_affirm, '$a^T t_A$')\nplot_density(axes[4], acts, labels, t_g, '$a^T t_G$')\n\nplt.show()\nprint(f\"layer {layer}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T06:38:33.494937Z","iopub.execute_input":"2025-07-02T06:38:33.495189Z","iopub.status.idle":"2025-07-02T06:38:34.59063Z","shell.execute_reply.started":"2025-07-02T06:38:33.495173Z","shell.execute_reply":"2025-07-02T06:38:34.589883Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Activation vectors projected onto $t_G$ and $t_P$ (reduced version of figure 1)","metadata":{}},{"cell_type":"code","source":"fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12, 6))\nacts_affirm, labels_affirm, acts_neg, labels_neg = collect_affirm_neg_data(train_sets, train_set_sizes, \n                                                                            model_family, model_size, \n                                                                            model_type, layer)\n\nfor i, (acts, labels) in enumerate([(acts_affirm, labels_affirm), (acts_neg, labels_neg)]):\n    prod_g, prod_p = project_activations(acts, t_g, t_p)\n    ax = axes[i]\n    if i==0:\n        ax.set_xlabel('$a_{ij}^T t_G$', fontsize=19)\n        ax.set_ylabel('$a_{ij}^T t_P$', fontsize=19)\n        ax.set_title('Affirmative Statements', fontsize=19)\n    else:\n        ax.set_title('Negated Statements', fontsize=19)\n\n    colors = ['red' if label == 0 else 'blue' for label in labels]\n    ax.scatter(prod_g, prod_p, c=colors, alpha=0.5, s=5)\n\n# Add the legend to the last subplot\nhandles = [plt.scatter([], [], c='red', label='False'),\n            plt.scatter([], [], c='blue', label='True')]\naxes[1].legend(handles=handles, fontsize=19)\n\nfig.suptitle('Projection of activations on $t_G$ and $t_P$', fontsize=19)\nplt.show()\nprint(f\"layer {layer}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T06:38:34.591338Z","iopub.execute_input":"2025-07-02T06:38:34.591536Z","iopub.status.idle":"2025-07-02T06:38:35.180912Z","shell.execute_reply.started":"2025-07-02T06:38:34.591521Z","shell.execute_reply":"2025-07-02T06:38:35.180172Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Projection of other datasets onto $t_G$ and $t_P$ - larger_than and smaller_than are shown as examples","metadata":{}},{"cell_type":"code","source":"fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12, 6))  \n\nacts_affirm, labels_affirm, acts_neg, labels_neg = collect_affirm_neg_data(train_sets, train_set_sizes, model_family,\n                                                                            model_size, model_type, layer)\n# Project activations on t_g and t_p\nproj_g_affirm, proj_p_affirm = project_activations(acts_affirm, t_g, t_p)\nproj_g_neg, proj_p_neg = project_activations(acts_neg, t_g, t_p)\n\n# Define colors and markers for each label\nlabel_to_color = {0: 'indigo', 1: 'orange'}\nlabel_to_marker = {0: 's', 1: '^'}  # s for square, ^ for triangle\n\nfor i, dataset_name in enumerate(['larger_than', 'smaller_than']):\n        ax = axes[i]\n        ax.set_title(dataset_name, fontsize=19)\n        for label in [0,1]:\n                idx = labels_affirm.numpy() == label\n                ax.scatter(proj_g_affirm[idx], proj_p_affirm[idx], c='grey', \n                        marker=label_to_marker[label], alpha=0.3, s=5) \n                idx = labels_neg.numpy() == label\n                ax.scatter(proj_g_neg[idx], proj_p_neg[idx], c='grey', \n                        marker=label_to_marker[label], alpha=0.3, s=5) \n        \n        dm = DataManager()\n        dm.add_dataset(dataset_name, model_family, model_size, model_type, layer, split=None, center=False, device='cpu')\n        acts, labels = dm.data[dataset_name]\n        prod_g, prod_p = project_activations(acts, t_g, t_p)\n        for label in [0,1]:\n                idx = labels.numpy() == label\n                ax.scatter(prod_g[idx], prod_p[idx], c=label_to_color[label], \n                        marker=label_to_marker[label], alpha=0.9, s=15)\n                if i==0:\n                        ax.set_xlabel('$a^T t_G$', fontsize=19)\n                        ax.set_ylabel('$a^T t_P$', fontsize=19)\n\nadd_legend(axes[0])\nfig.suptitle('Projection of activations on $t_G$ and $t_P$', fontsize=19)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T06:38:35.181729Z","iopub.execute_input":"2025-07-02T06:38:35.181924Z","iopub.status.idle":"2025-07-02T06:38:35.968522Z","shell.execute_reply.started":"2025-07-02T06:38:35.181908Z","shell.execute_reply":"2025-07-02T06:38:35.967771Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Dimensionality of Truth\n#### Are there more than two truth dimensions?","metadata":{}},{"cell_type":"markdown","source":"#### Fraction of truth related variance in activations explained by Principal Components","metadata":{}},{"cell_type":"code","source":"# Define the four different statement types and corresponding datasets\nstatement_types = [\"affirmative\", \"affirmative, negated\", \"affirmative, negated, conjunctions\", \"affirmative, affirmative German\",\n                    \"affirmative, affirmative German,\\nnegated, negated German\", \n                   \"affirmative, negated,\\nconjunctions, disjunctions\"]\ndatasets_pca_options = {\n    \"affirmative\": ['cities', 'sp_en_trans', 'inventors', 'animal_class', 'element_symb', 'facts'],\n    \n    \"affirmative, negated\": ['cities', 'sp_en_trans', 'inventors', 'animal_class', 'element_symb', 'facts',\n                             'neg_cities', 'neg_sp_en_trans', 'neg_inventors', 'neg_animal_class', 'neg_element_symb', 'neg_facts'],\n\n    \"affirmative, negated, conjunctions\": ['cities', 'sp_en_trans', 'inventors', 'animal_class', 'element_symb', 'facts',\n                             'neg_cities', 'neg_sp_en_trans', 'neg_inventors', 'neg_animal_class', 'neg_element_symb', 'neg_facts',\n                             'cities_conj', 'sp_en_trans_conj', 'inventors_conj', 'animal_class_conj', 'element_symb_conj', 'facts_conj'],\n    \n    \"affirmative, affirmative German\": ['cities', 'sp_en_trans', 'inventors', 'animal_class', 'element_symb', 'facts',\n                             'cities_de', 'sp_en_trans_de', 'inventors_de', 'animal_class_de', 'element_symb_de', 'facts_de',],\n\n    \"affirmative, affirmative German,\\nnegated, negated German\": ['cities', 'sp_en_trans', 'inventors', 'animal_class', 'element_symb', 'facts',\n                            'cities_de', 'sp_en_trans_de', 'inventors_de', 'animal_class_de', 'element_symb_de', 'facts_de',\n                             'neg_cities', 'neg_sp_en_trans', 'neg_inventors', 'neg_animal_class', 'neg_element_symb', 'neg_facts',\n                             'neg_cities_de', 'neg_sp_en_trans_de', 'neg_inventors_de', 'neg_animal_class_de', 'neg_element_symb_de', 'neg_facts_de'],\n\n    \"affirmative, negated,\\nconjunctions, disjunctions\": ['cities', 'sp_en_trans', 'inventors', 'animal_class', 'element_symb', 'facts',\n                             'neg_cities', 'neg_sp_en_trans', 'neg_inventors', 'neg_animal_class', 'neg_element_symb', 'neg_facts',\n                             'cities_conj', 'sp_en_trans_conj', 'inventors_conj', 'animal_class_conj', 'element_symb_conj', 'facts_conj',\n                             'cities_disj', 'sp_en_trans_disj', 'inventors_disj', 'animal_class_disj', 'element_symb_disj', 'facts_disj']\n}\n\ndef compute_subspace_angle(A, B):\n    # Normalize columns of A and B\n    A = A / torch.linalg.norm(A, dim=0)\n    B = B / torch.linalg.norm(B, dim=0)\n    \n    # Compute SVD of A^T * B\n    U, S, Vt = torch.linalg.svd(A.T @ B)\n    \n    # Compute principal angles\n    angles = torch.arccos(torch.clamp(S, -1, 1))\n    \n    return angles\n\n# Create the 2x2 plot\nfig, axs = plt.subplots(2, 3, figsize=(18, 10))\naxs = axs.flatten()\n\nfor i, statement_type in enumerate(statement_types):\n    datasets_pca = datasets_pca_options[statement_type]\n    directions = []\n\n    for train_set in datasets_pca:\n        dm = DataManager()\n        dm.add_dataset(train_set, model_family, model_size, model_type, layer, split=1.0, center=True, device='cpu')\n        train_acts, train_labels = dm.get('train')\n        true_acts = train_acts[train_labels.to(bool)]\n        false_acts = train_acts[~train_labels.to(bool)]\n        directions.append(torch.mean(true_acts, dim=0))\n        directions.append(torch.mean(false_acts, dim=0))\n\n    mean_acts = np.array([direction.numpy() for direction in directions])\n    pca = PCA(n_components=10)\n    pca.fit(mean_acts)\n    axs[i].scatter(np.arange(1, 11, 1), pca.explained_variance_ratio_, s=90)\n    axs[i].set_title(f'{statement_type}', fontsize=26)\n    if i == 0 or i==3:\n        axs[i].set_ylabel('Explained variance', fontsize=27)\n    if i==3 or i == 4 or i==5:\n        axs[i].set_xlabel('PC index', fontsize=26)\n    axs[i].tick_params(axis='both', which='major', labelsize=20)\n    axs[i].grid(True)\n    if statement_type == \"affirmative, negated\":\n        # Compute subspace angle\n        A = torch.stack([t_g, t_p], dim=1)\n        B = torch.stack([torch.tensor(pca.components_[0, :]), torch.tensor(pca.components_[1, :])], dim=1)\n        angles = compute_subspace_angle(A, B)\n        print(f\"Principal angles between subspaces (in radians): {angles}\")\n        print(f\"Principal angles between subspaces (in degrees): {torch.rad2deg(angles)}\")\n        print(\"Cosine similarity between t_G and first PC: \" + str(torch.tensor(pca.components_[0, :])/torch.linalg.norm(torch.tensor(pca.components_[0, :])) @ t_g/np.linalg.norm(t_g)))\n        print(\"Cosine similarity between t_P and second PC: \" + str(torch.tensor(pca.components_[1, :])/torch.linalg.norm(torch.tensor(pca.components_[1, :])) @ t_p/np.linalg.norm(t_p)))\n\nfig.suptitle('Fraction of variance in centered and averaged\\n activations explained by PCs', fontsize=28)\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T06:38:35.969288Z","iopub.execute_input":"2025-07-02T06:38:35.969525Z","iopub.status.idle":"2025-07-02T06:38:38.450491Z","shell.execute_reply.started":"2025-07-02T06:38:35.969503Z","shell.execute_reply":"2025-07-02T06:38:38.449817Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_sets_subset = [['cities'], ['cities', 'neg_cities'], ['cities', 'neg_cities', 'cities_conj'],\n                     ['cities', 'neg_cities', 'cities_conj', 'cities_disj']]\n\nval_sets_subset = ['cities', 'neg_cities', 'facts', 'neg_facts',\n                   'facts_conj', 'facts_disj']\n\nnum_runs = 10\nproject_options = [None, 't_G_t_P']\n\n# Helper function to create unique keys for training sets\ndef get_train_set_key(train_set):\n    return '_'.join(train_set)\n\n# Initialize dictionaries to store accuracies for each projection option\nall_aurocs_options = {proj: {get_train_set_key(train_set): {val_set: [] for val_set in val_sets_subset} for train_set in train_sets_subset} for proj in project_options}\n\nfor project_out in project_options:\n    all_aurocs = all_aurocs_options[project_out]\n\n    for run in range(num_runs):\n        # Compute t_g and t_p using all data\n        acts_centered, _, labels, polarities = collect_training_data(train_sets, train_set_sizes, model_family, \n                                                            model_size, model_type, layer)\n        t_g, t_p = learn_truth_directions(acts_centered, labels, polarities)\n        # orthonormalize t_g and t_p\n        t_G_orthonormal, t_P_orthonormal = compute_orthonormal_vectors(t_g, t_p)\n\n        for train_set in train_sets_subset:\n            train_set_key = get_train_set_key(train_set)\n            \n            # set up data\n            dm = DataManager()\n            for subset in train_set:\n                dm.add_dataset(subset, model_family, model_size, model_type, layer, split=0.8, center=True, device='cpu')\n            train_acts, train_labels = dm.get('train')\n            if project_out == None:\n                pass\n            elif project_out == 't_G_t_P':\n                train_acts = train_acts - (train_acts @ t_G_orthonormal)[:, None] * t_G_orthonormal - (train_acts @ t_P_orthonormal)[:, None] * t_P_orthonormal\n            polarities = torch.zeros((train_labels.shape)[0])\n            # learn t_G\n            t_g_trained, _ = learn_truth_directions(train_acts, train_labels, polarities)\n            \n            # compute auroc of a^T t_G on validation sets\n            for val_set in val_sets_subset:\n                if val_set in train_set:\n                    acts, labels = dm.get('val')\n                else:\n                    dm.add_dataset(val_set, model_family, model_size, model_type, layer, split=None, center=False, device='cpu')\n                    acts, labels = dm.data[val_set]\n                \n                proj_g = acts @ t_g_trained\n                auroc = roc_auc_score(labels.numpy(), proj_g.numpy())\n                all_aurocs[train_set_key][val_set].append(auroc)\n\n    # Calculate mean and standard deviation for each training-validation set combination\n    mean_aurocs = {train_set: {val_set: np.mean(accs) for val_set, accs in val_sets.items()} for train_set, val_sets in all_aurocs.items()}\n    std_aurocs = {train_set: {val_set: np.std(accs) for val_set, accs in val_sets.items()} for train_set, val_sets in all_aurocs.items()}\n\n    all_aurocs_options[project_out] = {'mean': mean_aurocs, 'std': std_aurocs}\n    print(mean_aurocs, std_aurocs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T06:38:38.451288Z","iopub.execute_input":"2025-07-02T06:38:38.451534Z","iopub.status.idle":"2025-07-02T06:38:55.075344Z","shell.execute_reply.started":"2025-07-02T06:38:38.451516Z","shell.execute_reply":"2025-07-02T06:38:55.074566Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Generalisation accuracies of truth directions trained on different data","metadata":{}},{"cell_type":"code","source":"# Plotting the results\nfig, axes = plt.subplots(figsize=(16, 6.7), nrows=1, ncols=2, sharey=True)\n# Titles for the x and y axes\ntitles_val = ['cities', 'neg_cities', 'facts', 'neg_facts',\n                'facts_conj', 'facts_disj']\ntitles_train = ['cities', '+ neg_cities', '+ cities_conj', '+ cities_disj']\n\n# Create a custom colormap from red to yellow\ncolors = [(1, 0, 0), (1, 1, 0)]  # Red to Yellow\nn_bins = 100  # Discretizes the interpolation into bins\ncmap_name = 'red_yellow'\ncmap = LinearSegmentedColormap.from_list(cmap_name, colors, N=n_bins)\n\nfor idx, project_out in enumerate(project_options):\n    mean_aurocs = all_aurocs_options[project_out]['mean']\n    std_aurocs = all_aurocs_options[project_out]['std']\n    \n    # Prepare the grid for mean accuracies\n    grid = np.zeros((len(train_sets_subset), len(val_sets_subset)))\n\n    # Populate the grid with mean accuracies\n    for i, train_set in enumerate(train_sets_subset):\n        train_set_key = get_train_set_key(train_set)\n        for j, val_set in enumerate(val_sets_subset):\n            grid[i, j] = mean_aurocs[train_set_key][val_set]\n\n    # Plot the grid\n    im = axes[idx].imshow(grid.T, vmin=0, vmax=1, cmap=cmap, aspect='auto')\n\n    # Annotate each cell with the mean accuracy and standard deviation\n    for i in range(len(grid)):\n        for j in range(len(grid[0])):\n            mean_auroc = grid[i][j]\n            std_auroc = std_aurocs[get_train_set_key(train_sets_subset[i])][val_sets_subset[j]]\n            axes[idx].text(i, j, f'{mean_auroc:.2f}', ha='center', va='center', fontsize=16) #±{std_auroc:.2f}\n\n\n    # Titles for the x and y axes\n    axes[idx].set_yticks(range(len(val_sets_subset)))\n    axes[idx].set_xticks(range(len(train_sets_subset)))\n    axes[idx].set_yticklabels([val_title for val_title in titles_val], fontsize=17)\n    axes[idx].set_xticklabels([train_title for train_title in titles_train], rotation=45, ha='right', fontsize=17)\n\n    # Set title and labels for the subplot\n    if idx == 0:\n        axes[idx].set_title(f'Projected out: None', fontsize=20)\n    if idx == 1:\n        axes[idx].set_title(f'Projected out: $t_G$ and $t_P$', fontsize=20)\n    if idx == 0:\n        axes[idx].set_ylabel('Test Set', fontsize=20)\n        axes[idx].set_xlabel('Train Set \"cities\"', fontsize=20)\n\n# Add colorbar to the last subplot\ncbar = fig.colorbar(im, ax=axes.ravel().tolist(), shrink=0.8)\ncbar.ax.tick_params(labelsize=16)\nfig.suptitle('AUROC for Projections $a^T t$', fontsize=20, x=0.42)\n\n# Show the plot\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T06:38:55.076245Z","iopub.execute_input":"2025-07-02T06:38:55.076512Z","iopub.status.idle":"2025-07-02T06:38:55.447713Z","shell.execute_reply.started":"2025-07-02T06:38:55.076494Z","shell.execute_reply":"2025-07-02T06:38:55.44695Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Cross-dataset generalization matrix","metadata":{}},{"cell_type":"code","source":"train_sets_subset = [['cities'], ['neg_cities'], ['cities', 'neg_cities'], ['cities_conj'], ['cities_disj']]\n\nval_sets_subset = ['cities', 'neg_cities', 'facts', 'neg_facts',\n                   'facts_conj', 'facts_disj']\n\nnum_runs = 10\n\n# Helper function to create unique keys for training sets\ndef get_train_set_key(train_set):\n    return '_'.join(train_set)\n\n# Initialize dictionaries to store accuracies for each projection option\nall_aurocs = {get_train_set_key(train_set): {val_set: [] for val_set in val_sets_subset} for train_set in train_sets_subset}\nfor run in range(num_runs):\n    for train_set in train_sets_subset:\n        train_set_key = get_train_set_key(train_set)\n        \n        # set up data\n        dm = DataManager()\n        for subset in train_set:\n            dm.add_dataset(subset, model_family, model_size, model_type, layer, split=0.8, center=True, device='cpu')\n        train_acts, train_labels = dm.get('train')\n        polarities = torch.zeros((train_labels.shape)[0])\n        # learn t_G\n        t_g_trained, _ = learn_truth_directions(train_acts, train_labels, polarities)\n        \n        # compute auroc of a^T t_G on validation sets\n        for val_set in val_sets_subset:\n            if val_set in train_set:\n                acts, labels = dm.get('val')\n            else:\n                dm.add_dataset(val_set, model_family, model_size, model_type, layer, split=None, center=False, device='cpu')\n                acts, labels = dm.data[val_set]\n            \n            proj_g = acts @ t_g_trained\n            auroc = roc_auc_score(labels.numpy(), proj_g.numpy())\n            all_aurocs[train_set_key][val_set].append(auroc)\n\n# Calculate mean and standard deviation for each training-validation set combination\nmean_aurocs = {train_set: {val_set: np.mean(accs) for val_set, accs in val_sets.items()} for train_set, val_sets in all_aurocs.items()}\nstd_aurocs = {train_set: {val_set: np.std(accs) for val_set, accs in val_sets.items()} for train_set, val_sets in all_aurocs.items()}\n\nall_aurocs = {'mean': mean_aurocs, 'std': std_aurocs}\nprint(mean_aurocs, std_aurocs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T06:38:55.449534Z","iopub.execute_input":"2025-07-02T06:38:55.449746Z","iopub.status.idle":"2025-07-02T06:39:02.073051Z","shell.execute_reply.started":"2025-07-02T06:38:55.44973Z","shell.execute_reply":"2025-07-02T06:39:02.072363Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plotting the results\nfig, ax = plt.subplots(figsize=(7, 5.5), nrows=1, ncols=1, sharey=True)\n# Titles for the x and y axes\ntitles_val = ['cities', 'neg_cities', 'facts', 'neg_facts',\n                'facts_conj', 'facts_disj']\ntitles_train = ['cities', 'neg_cities', 'cities+neg_cities', 'cities_conj', 'cities_disj']\n\n# Create a custom colormap from red to yellow\ncolors = [(1, 0, 0), (1, 1, 0)]  # Red to Yellow\nn_bins = 100  # Discretizes the interpolation into bins\ncmap_name = 'red_yellow'\ncmap = LinearSegmentedColormap.from_list(cmap_name, colors, N=n_bins)\n\nmean_aurocs = all_aurocs['mean']\nstd_aurocs = all_aurocs['std']\n\n# Prepare the grid for mean accuracies\ngrid = np.zeros((len(train_sets_subset), len(val_sets_subset)))\n\n# Populate the grid with mean accuracies\nfor i, train_set in enumerate(train_sets_subset):\n    train_set_key = get_train_set_key(train_set)\n    for j, val_set in enumerate(val_sets_subset):\n        grid[i, j] = mean_aurocs[train_set_key][val_set]\n\n# Plot the grid\nim = ax.imshow(grid.T, vmin=0, vmax=1, cmap=cmap, aspect='auto')\n\n# Annotate each cell with the mean accuracy and standard deviation\nfor i in range(len(grid)):\n    for j in range(len(grid[0])):\n        mean_auroc = grid[i][j]\n        std_auroc = std_aurocs[get_train_set_key(train_sets_subset[i])][val_sets_subset[j]]\n        ax.text(i, j, f'{mean_auroc:.2f}', ha='center', va='center', fontsize=16) #±{std_auroc:.2f}\n\n\n    # Titles for the x and y axes\n    ax.set_yticks(range(len(val_sets_subset)))\n    ax.set_xticks(range(len(train_sets_subset)))\n    ax.set_yticklabels([val_title for val_title in titles_val], fontsize=17)\n    ax.set_xticklabels([train_title for train_title in titles_train], rotation=45, ha='right', fontsize=17)\n\n    ax.set_ylabel('Test Set', fontsize=20)\n    ax.set_xlabel('Train Set', fontsize=20)\n\n# Add colorbar to the last subplot\ncbar = fig.colorbar(im, shrink=0.8)\ncbar.ax.tick_params(labelsize=16)\nfig.suptitle('AUROC for Projections $a^T t$', fontsize=20, x=0.42)\n\n# Show the plot\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T06:39:02.073789Z","iopub.execute_input":"2025-07-02T06:39:02.07411Z","iopub.status.idle":"2025-07-02T06:39:02.320708Z","shell.execute_reply.started":"2025-07-02T06:39:02.074086Z","shell.execute_reply":"2025-07-02T06:39:02.320026Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data = np.zeros((100,2))\n# zeroth feature is perfectly predictive of label\ndata[0:50,0] = 1.0\n# first feature is correlated with the correct label but not perfectly, 20 correct, 5 incorrect\ndata[0:20, 1] = 1.0\ndata[50:55, 1] = 1.0\nlabels = np.concatenate((np.ones(50), np.zeros(50)))\n# which method can disentangle feature 0 from feature 1?\n# mass mean\nd_mm = np.mean(data[labels == 1.0], axis=0) - np.mean(data[labels==0.0], axis=0)\nprint(d_mm)\n# LR\nLR = LogisticRegression(penalty=None, fit_intercept = True)\nLR.fit(data, labels)\nd_LR = LR.coef_\nprint(LR.intercept_)\nprint(d_LR) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T06:39:02.32156Z","iopub.execute_input":"2025-07-02T06:39:02.32187Z","iopub.status.idle":"2025-07-02T06:39:02.335162Z","shell.execute_reply.started":"2025-07-02T06:39:02.321853Z","shell.execute_reply":"2025-07-02T06:39:02.334514Z"}},"outputs":[],"execution_count":null}]}