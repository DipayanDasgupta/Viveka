{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d9382dc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded a tensor directly.\n",
      "resid: shape=torch.Size([44, 2304]), dtype=torch.float32\n",
      "  mean=0.0723, std=6.7727, min=-186.0000, max=190.0000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the activations\n",
    "file_path = \"C:/Users/arshm/Documents/Code/Viveka/linear_experiment_2_NN_Probing/acts_output/google__gemma-2-2b-it/triviaqa-subsampled/layer_22_0.pt\"\n",
    "data = torch.load(file_path, map_location=\"cpu\")\n",
    "\n",
    "# Helper: Pretty print tensor stats\n",
    "def print_tensor_stats(name, tensor):\n",
    "    print(f\"{name}: shape={tensor.shape}, dtype={tensor.dtype}\")\n",
    "    print(f\"  mean={tensor.mean().item():.4f}, std={tensor.std().item():.4f}, min={tensor.min().item():.4f}, max={tensor.max().item():.4f}\")\n",
    "\n",
    "# Case 1: Tensor\n",
    "if isinstance(data, torch.Tensor):\n",
    "    print(\"Loaded a tensor directly.\")\n",
    "    print_tensor_stats(\"resid\", data)\n",
    "\n",
    "# Case 2: Dict of tensors\n",
    "elif isinstance(data, dict):\n",
    "    print(f\"Loaded a dict with {len(data)} keys.\")\n",
    "    for k, v in data.items():\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            print_tensor_stats(k, v)\n",
    "        else:\n",
    "            print(f\"{k}: Not a tensor ({type(v)})\")\n",
    "\n",
    "# Optional: Visualize one layer\n",
    "    # Example: visualize norm across sequence for one layer\n",
    "    layer_key = list(data.keys())[0]  # pick a layer\n",
    "    tensor = data[layer_key]  # shape: (batch, seq, d_model)\n",
    "    \n",
    "    if tensor.ndim == 3:\n",
    "        # Mean norm across sequence positions\n",
    "        mean_norms = tensor.norm(dim=-1).mean(dim=0)  # shape: (seq_len,)\n",
    "        plt.plot(mean_norms.numpy())\n",
    "        plt.title(f\"Mean Residual Stream Norms across Sequence [{layer_key}]\")\n",
    "        plt.xlabel(\"Sequence Position\")\n",
    "        plt.ylabel(\"Mean L2 Norm\")\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "else:\n",
    "    print(f\"Unknown format: {type(data)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02740ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch as t\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "activation_dir = \"/kaggle/working/Truth_is_Universal/acts/Gemma2/2B/chat/cities\"\n",
    "num_layers = 26\n",
    "\n",
    "# Structure: layer_outputs[layer_idx] = [vector_0, vector_1, ..., vector_N]\n",
    "layer_outputs = [[] for _ in range(num_layers)]\n",
    "\n",
    "for fname in sorted(os.listdir(activation_dir)):\n",
    "    if not fname.endswith(\".pt\"):\n",
    "        continue\n",
    "    # Filename format: layer_{layer_idx}_{batch_start}.pt\n",
    "    parts = fname.replace(\".pt\", \"\").split(\"_\")\n",
    "    if len(parts) != 3:\n",
    "        continue\n",
    "    try:\n",
    "        layer_idx = int(parts[1])\n",
    "    except ValueError:\n",
    "        continue\n",
    "    if layer_idx >= num_layers:\n",
    "        continue\n",
    "\n",
    "    file_path = os.path.join(activation_dir, fname)\n",
    "    acts = t.load(file_path)  # shape: [batch_size, hidden_dim]\n",
    "    acts_np = acts.cpu().numpy()\n",
    "\n",
    "    layer_outputs[layer_idx].append(acts_np) \n",
    "# Now stack batches for each layer\n",
    "final_layerwise_vectors = [np.concatenate(batches, axis=0) for batches in layer_outputs]\n",
    "# final_layerwise_vectors[i].shape = [num_examples, hidden_dim]\n",
    "\n",
    "print(\"Collected activations for:\")\n",
    "for i, arr in enumerate(final_layerwise_vectors):\n",
    "    print(f\"  Layer {i}: {arr.shape}\")\n",
    "\n",
    "\n",
    "import os\n",
    "import torch as t\n",
    "dataset = \"cities\"\n",
    "activation_dir = f\"acts/Gemma2/2B/chat/{dataset}\"\n",
    "example_index = 425 #putting 426 gives me 427th ex, or 428th row\n",
    "batch_size = 25\n",
    "\n",
    "batch_start = (example_index // batch_size) * batch_size\n",
    "offset_in_batch = example_index % batch_size\n",
    "\n",
    "layer_vectors = []\n",
    "\n",
    "for layer_idx in range(26):\n",
    "    file_path = os.path.join(activation_dir, f\"layer_{layer_idx}_{batch_start}.pt\")\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"Missing file: {file_path}\")\n",
    "        continue\n",
    "    \n",
    "    acts = t.load(file_path)\n",
    "    vector = acts[offset_in_batch]  # shape: [hidden_dim]\n",
    "    layer_vectors.append(vector.cpu().numpy())\n",
    "\n",
    "print(f\"Collected {len(layer_vectors)} layer vectors for example {example_index}\")\n",
    "df_data = pd.read_csv(f\"/kaggle/working/Truth_is_Universal/datasets/{dataset}.csv\")\n",
    "print(f\"Example : cities/{example_index}/{df_data.iloc[example_index]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
